{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b26910-1a58-444d-9120-37fff69d4bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98860e2-18b2-4f9e-b5c6-45af5c48c182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load_dqx_checks.py\n",
    "\n",
    "# === Cell 1: Install DQX Package (if your cluster image doesn't already include it) ===\n",
    "%pip install databricks-labs-dqx\n",
    "\n",
    "# === Cell 2: Restart Python to pick up libs (Databricks convention) ===\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "# === Cell 3: Parameters (adjust paths/mode here) ===\n",
    "OUTPUT_CONFIG_PATH = \"resources/dqx_output_config.yaml\"\n",
    "RULES_DIR          = \"resources/dqx_checks_config\"   # trailing slash optional\n",
    "TIME_ZONE          = \"America/Chicago\"               # for audit timestamps\n",
    "MODE               = \"WRITE\"                         # one of: VALIDATE, DRY_RUN, WRITE\n",
    "\n",
    "# === Cell 4: Imports & helpers ===\n",
    "import os, json, re, hashlib, yaml\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from delta.tables import DeltaTable\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "# your utils (kept as-is per your environment)\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "\n",
    "# --- Section header for readable job logs ---\n",
    "def section(title: str) -> None:\n",
    "    bar = \"═\" * 70\n",
    "    print(f\"\\n{bar}\\n║ {title.ljust(68)}║\\n{bar}\")\n",
    "\n",
    "# --- DQX rule table schema (adds logic_hash; hash_id == logic_hash) ---\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"hash_id\",       T.StringType(), False),  # canonical content hash (same as logic_hash)\n",
    "    T.StructField(\"logic_hash\",    T.StringType(), False),  # explicit column for clarity/future\n",
    "    T.StructField(\"table_name\",    T.StringType(), False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",          T.StringType(), False),\n",
    "    T.StructField(\"criticality\",   T.StringType(), False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",         T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\",T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\",  T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # ops fields\n",
    "    T.StructField(\"yaml_path\",     T.StringType(), False),\n",
    "    T.StructField(\"active\",        T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\",    T.StringType(), False),\n",
    "    T.StructField(\"created_at\",    T.StringType(), False),  # ISO string; cast on write\n",
    "    T.StructField(\"updated_by\",    T.StringType(), True),\n",
    "    T.StructField(\"updated_at\",    T.StringType(), True),\n",
    "])\n",
    "\n",
    "# --- Canonical hashing utilities ---\n",
    "_WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def _norm_ws(s: str) -> str:\n",
    "    return _WHITESPACE_RE.sub(\" \", s.strip())\n",
    "\n",
    "def _sorted_list(v) -> List[str]:\n",
    "    return sorted((str(x) for x in v))\n",
    "\n",
    "def _normalize_arguments(func: str, args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Order-insensitive, stringified, semantics-preserving normalization for check.arguments.\"\"\"\n",
    "    if not args:\n",
    "        return {}\n",
    "    func = (func or \"\").lower()\n",
    "    out: Dict[str, Any] = {}\n",
    "    for k, v in args.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if k in {\"columns\", \"allowed\"} and isinstance(v, (list, tuple)):\n",
    "            out[k] = _sorted_list(v)\n",
    "        elif k == \"column\":\n",
    "            out[k] = str(v)\n",
    "        elif k in {\"min_limit\", \"max_limit\"}:\n",
    "            out[k] = str(v)\n",
    "        elif k == \"regex\":\n",
    "            out[k] = str(v)\n",
    "        elif k == \"expression\" and isinstance(v, str):\n",
    "            out[k] = _norm_ws(v)\n",
    "        else:\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                out[k] = [str(x) for x in v]\n",
    "            elif isinstance(v, dict):\n",
    "                out[k] = {kk: str(vv) for kk, vv in sorted(v.items())}\n",
    "            else:\n",
    "                out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def canonical_rule_payload(rule: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Minimal, stable payload for hashing identity.\n",
    "    NOTE: We keep for_each_column as a normalized list to match your existing behavior.\n",
    "    \"\"\"\n",
    "    table_name = str(rule[\"table_name\"]).lower()\n",
    "    check = rule[\"check\"]\n",
    "    func = str(check[\"function\"]).lower()\n",
    "    args = _normalize_arguments(func, check.get(\"arguments\", {}) or {})\n",
    "    fec = check.get(\"for_each_column\")\n",
    "    fec_norm = _sorted_list(fec) if isinstance(fec, (list, tuple)) else None\n",
    "    filt = rule.get(\"filter\")\n",
    "    filt = _norm_ws(filt) if isinstance(filt, str) else None\n",
    "\n",
    "    # Exclude: name, run_config_name, user_metadata, criticality\n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"check\": {\"function\": func, \"arguments\": args, \"for_each_column\": fec_norm},\n",
    "        \"filter\": filt,\n",
    "    }\n",
    "\n",
    "def compute_logic_hash(rule: Dict[str, Any]) -> str:\n",
    "    payload = canonical_rule_payload(rule)\n",
    "    data = json.dumps(payload, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "    return hashlib.sha256(data.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Convert arbitrary JSON-serializable values to map<string,string> for DQX.\"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "# --- Validation & processing functions (kept, with minor additions) ---\n",
    "def parse_output_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, \"r\") as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "    if \"dqx_config_table_name\" not in config or \"run_config_name\" not in config:\n",
    "        raise ValueError(\"Config must include 'dqx_config_table_name' and 'run_config_name'.\")\n",
    "    return config\n",
    "\n",
    "def validate_rules_file(rules, file_base: str, file_path: str):\n",
    "    problems = []\n",
    "    seen_names = set()\n",
    "    table_names = {r.get(\"table_name\") for r in rules if isinstance(r, dict)}\n",
    "    if len(table_names) != 1:\n",
    "        problems.append(f\"Inconsistent table_name values in {file_path}: {table_names}\")\n",
    "    expected_table = file_base\n",
    "    try:\n",
    "        tn = list(table_names)[0]\n",
    "        if tn.split(\".\")[-1] != expected_table:\n",
    "            problems.append(f\"Table name in rules ({tn}) does not match filename ({expected_table}) in {file_path}\")\n",
    "    except Exception:\n",
    "        problems.append(f\"No valid table_name found in {file_path}\")\n",
    "    for rule in rules:\n",
    "        name = rule.get(\"name\")\n",
    "        if not name:\n",
    "            problems.append(f\"Missing rule name in {file_path}\")\n",
    "        if name in seen_names:\n",
    "            problems.append(f\"Duplicate rule name '{name}' in {file_path}\")\n",
    "        seen_names.add(name)\n",
    "    if problems:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {problems}\")\n",
    "\n",
    "def validate_rule_fields(rule, file_path: str):\n",
    "    problems = []\n",
    "    required_fields = [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "    for field in required_fields:\n",
    "        if not rule.get(field):\n",
    "            problems.append(f\"Missing required field '{field}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        problems.append(f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"criticality\") not in {\"error\", \"warn\", \"warning\"}:\n",
    "        problems.append(f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        problems.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if problems:\n",
    "        raise ValueError(f\"Rule-level validation failed: {problems}\")\n",
    "\n",
    "def validate_with_dqx(rules, file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if status.has_errors:\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "def process_yaml_file(path: str, output_config: Dict[str, Any], time_zone: str = \"UTC\"):\n",
    "    \"\"\"Read one YAML file, validate, and flatten into rows for the table.\"\"\"\n",
    "    file_base = os.path.splitext(os.path.basename(path))[0]\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = yaml.safe_load(fh)\n",
    "    if isinstance(docs, dict):\n",
    "        docs = [docs]\n",
    "\n",
    "    validate_rules_file(docs, file_base, path)\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat_rules = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path)\n",
    "\n",
    "        # Canonical logic hash (independent of name/run_config/criticality)\n",
    "        logic_hash = compute_logic_hash(rule)\n",
    "\n",
    "        check_dict = rule[\"check\"]\n",
    "        function = check_dict.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\")\n",
    "\n",
    "        for_each = check_dict.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\")\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\")\n",
    "\n",
    "        arguments = check_dict.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map<string,string> (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "        else:\n",
    "            user_metadata = {}\n",
    "\n",
    "        # Inject rule_id into user_metadata so the runner can pick it up directly\n",
    "        user_metadata.setdefault(\"rule_id\", logic_hash)\n",
    "\n",
    "        flat_rules.append({\n",
    "            \"hash_id\":       logic_hash,           # CHANGED: same as logic_hash\n",
    "            \"logic_hash\":    logic_hash,           # NEW\n",
    "            \"table_name\":    rule[\"table_name\"],\n",
    "            \"name\":          rule[\"name\"],\n",
    "            \"criticality\":   rule[\"criticality\"],\n",
    "            \"check\": {\n",
    "                \"function\":        function,\n",
    "                \"for_each_column\": for_each if for_each else None,\n",
    "                \"arguments\":       arguments if arguments else None,\n",
    "            },\n",
    "            \"filter\":         rule.get(\"filter\"),\n",
    "            \"run_config_name\":rule[\"run_config_name\"],\n",
    "            \"user_metadata\":  user_metadata if user_metadata else None,\n",
    "            \"yaml_path\":      path,\n",
    "            \"active\":         rule.get(\"active\", True),\n",
    "            \"created_by\":     \"AdminUser\",\n",
    "            \"created_at\":     now,\n",
    "            \"updated_by\":     None,\n",
    "            \"updated_at\":     None,\n",
    "        })\n",
    "\n",
    "    # Semantic validation by DQX engine\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat_rules\n",
    "\n",
    "def ensure_delta_table(spark: SparkSession, delta_table_name: str):\n",
    "    if not spark.catalog.tableExists(delta_table_name):\n",
    "        print(f\"Creating new Delta table at {delta_table_name}\")\n",
    "        empty_df = spark.createDataFrame([], TABLE_SCHEMA)\n",
    "        empty_df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "    else:\n",
    "        print(f\"Delta table already exists at {delta_table_name}\")\n",
    "\n",
    "def upsert_rules_into_delta(spark: SparkSession, rules, delta_table_name: str):\n",
    "    if not rules:\n",
    "        print(\"No rules to write, skipping upsert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nWriting rules to Delta table '{delta_table_name}'...\")\n",
    "    print(f\"Number of rules to write: {len(rules)}\")\n",
    "\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            \"target.yaml_path = source.yaml_path AND target.table_name = source.table_name AND target.name = source.name\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    except Exception:\n",
    "        print(\"Delta merge failed (likely first write). Writing full table.\")\n",
    "        df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "    print(f\"Successfully wrote {df.count()} rules to '{delta_table_name}'.\")\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules):\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    print(\"\\n==== Dry Run: Rules DataFrame to be uploaded ====\")\n",
    "    df.show(truncate=False, n=50)\n",
    "    print(f\"Total rules: {df.count()}\")\n",
    "    return df\n",
    "\n",
    "def validate_all_rules(rules_dir: str, output_config: Dict[str, Any], fail_fast: bool = True):\n",
    "    errors = []\n",
    "    print(f\"Starting validation for all YAML rule files in '{rules_dir}'\")\n",
    "    for fname in sorted(os.listdir(rules_dir)):\n",
    "        if not fname.endswith((\".yaml\", \".yml\")) or fname.startswith(\".\"):\n",
    "            continue\n",
    "        full_path = os.path.join(rules_dir, fname)\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            file_base = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            with open(full_path, \"r\") as fh:\n",
    "                docs = yaml.safe_load(fh)\n",
    "            if isinstance(docs, dict):\n",
    "                docs = [docs]\n",
    "            validate_rules_file(docs, file_base, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "    \n",
    "# === Cell 5: Thin Orchestrator (holds state; logic stays in pure functions) ===\n",
    "class RunMode(Enum):\n",
    "    VALIDATE = auto()\n",
    "    DRY_RUN  = auto()\n",
    "    WRITE    = auto()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LoaderConfig:\n",
    "    output_config_path: str\n",
    "    rules_dir: str\n",
    "    time_zone: str = \"America/Chicago\"\n",
    "\n",
    "class DQXRuleLoader:\n",
    "    \"\"\"Tiny facade for clear phases & logging; recomputes rules on every run.\"\"\"\n",
    "    def __init__(self, spark: SparkSession, cfg: LoaderConfig):\n",
    "        self.spark = spark\n",
    "        self.cfg = cfg\n",
    "\n",
    "        section(\"PARSING OUTPUT CONFIG\")\n",
    "        self.output_config = parse_output_config(cfg.output_config_path)\n",
    "        self.delta_table   = self.output_config[\"dqx_config_table_name\"]\n",
    "\n",
    "    def _load_all_rules(self) -> List[Dict[str, Any]]:\n",
    "        section(\"LOADING + FLATTENING RULES FROM YAML\")\n",
    "        rules: List[Dict[str, Any]] = []\n",
    "        rules_dir = self.cfg.rules_dir.rstrip(\"/\")\n",
    "\n",
    "        for fname in sorted(os.listdir(rules_dir)):\n",
    "            if not fname.endswith((\".yaml\", \".yml\")) or fname.startswith(\".\"):\n",
    "                continue\n",
    "            path = os.path.join(rules_dir, fname)\n",
    "            rules.extend(process_yaml_file(path, self.output_config, time_zone=self.cfg.time_zone))\n",
    "        print(f\"Collected {len(rules)} rule rows from YAML.\")\n",
    "        return rules\n",
    "\n",
    "    def run(self, mode: RunMode) -> None:\n",
    "        section(\"ENVIRONMENT\")\n",
    "        print_notebook_env(self.spark, local_timezone=self.cfg.time_zone)\n",
    "\n",
    "        rules = self._load_all_rules()\n",
    "\n",
    "        if mode is RunMode.VALIDATE:\n",
    "            section(\"VALIDATE-ONLY\")\n",
    "            validate_all_rules(self.cfg.rules_dir, self.output_config)\n",
    "            return\n",
    "\n",
    "        if mode is RunMode.DRY_RUN:\n",
    "            section(\"DRY RUN (SHOW DATAFRAME)\")\n",
    "            print_rules_df(self.spark, rules)\n",
    "            return\n",
    "\n",
    "        section(\"WRITE (UPSERT INTO DELTA)\")\n",
    "        ensure_delta_table(self.spark, self.delta_table)\n",
    "        upsert_rules_into_delta(self.spark, rules, self.delta_table)\n",
    "        print(f\"Finished writing rules to '{self.delta_table}'.\")\n",
    "        \n",
    "        \n",
    "# === Cell 6: Entrypoint ===\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "loader = DQXRuleLoader(\n",
    "    spark,\n",
    "    LoaderConfig(\n",
    "        output_config_path=OUTPUT_CONFIG_PATH,\n",
    "        rules_dir=RULES_DIR,\n",
    "        time_zone=TIME_ZONE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mode = {\n",
    "    \"VALIDATE\": RunMode.VALIDATE,\n",
    "    \"DRY_RUN\":  RunMode.DRY_RUN,\n",
    "    \"WRITE\":    RunMode.WRITE,\n",
    "}.get(MODE.upper(), RunMode.WRITE)\n",
    "\n",
    "loader.run(mode)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
