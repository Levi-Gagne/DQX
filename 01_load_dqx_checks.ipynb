{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apply DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5042f1e5-f5d4-4dda-a0b8-3e9130961893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flow"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START: update_dqx_rules_table\n",
    "|\n",
    "|-- 1. Check if Delta config table exists:\n",
    "|     |-- If table does NOT exist:\n",
    "|     |     |-- Create empty Delta table using TABLE_SCHEMA.\n",
    "|     |\n",
    "|     |-- If table DOES exist:\n",
    "|           |-- Proceed to next step.\n",
    "|\n",
    "|-- 2. For each YAML file in rules_dir:\n",
    "|     |-- Parse file (YAML load).\n",
    "|     |-- FILE-LEVEL validation (all rules in file target same table; no dup rule names; filename matches table name).\n",
    "|     |-- For each rule:\n",
    "|           |-- RULE-LEVEL validation (required fields, format, criticality, etc).\n",
    "|     |-- DQX syntax validation (DQEngine.validate_checks).\n",
    "|     |-- For each rule:\n",
    "|           |-- Extract run_config_name, look up valid_target_table and quarantine_target_table from output config.\n",
    "|           |-- Flatten and collect rule as dict (with hash_id, audit fields, etc).\n",
    "|\n",
    "|-- 3. Combine all flattened rules into one list.\n",
    "|\n",
    "|-- 4. Upsert all rules into Delta table:\n",
    "|     |-- If entry exists (yaml_path, table_name, name): UPDATE all fields except created_at/created_by.\n",
    "|     |-- If entry missing: INSERT, set created_at=now(UTC), created_by='admin', updated_at/updated_by=None.\n",
    "|\n",
    "|-- END: update_dqx_rules_table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1e26e-12cc-44e8-ae5a-e7b1fa4d5049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1154f87-29b1-4cb5-8a2f-ff0737aa9cda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec628222-f3a5-4c6c-be75-03735af77e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import yaml\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "\n",
    "\n",
    "# --- Unified schema, with DQX columns sandwiched ---\n",
    "# DQX core columns must match the docs:\n",
    "# name (STRING), criticality (STRING), check (STRUCT), filter (STRING),\n",
    "# run_config_name (STRING), user_metadata (MAP<STRING,STRING>)\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"hash_id\", T.StringType(), False),\n",
    "    T.StructField(\"table_name\", T.StringType(), False),\n",
    "\n",
    "    # DQX fields begin here\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"criticality\", T.StringType(), False),\n",
    "    T.StructField(\n",
    "        \"check\",\n",
    "        T.StructType([\n",
    "            T.StructField(\"function\", T.StringType(), False),\n",
    "            T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "            T.StructField(\"arguments\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "        ]),\n",
    "        False,\n",
    "    ),\n",
    "    T.StructField(\"filter\", T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\", T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # your ops fields\n",
    "    T.StructField(\"yaml_path\", T.StringType(), False),\n",
    "    T.StructField(\"active\", T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\", T.StringType(), False),\n",
    "    T.StructField(\"created_at\", T.StringType(), False),  # stored as ISO string; we may cast on write\n",
    "    T.StructField(\"updated_by\", T.StringType(), True),\n",
    "    T.StructField(\"updated_at\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "def compute_hash(rule_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"Stable hash over the identifying fields of a rule.\"\"\"\n",
    "    relevant = {\n",
    "        k: rule_dict[k]\n",
    "        for k in [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "        if k in rule_dict\n",
    "    }\n",
    "    return hashlib.md5(json.dumps(relevant, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert a dict of arbitrary JSON-serializable values to map<string,string>\n",
    "    required by DQX (lists/dicts -> JSON, bool -> 'true'/'false', else str()).\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_yaml_file(path: str, output_config: Dict[str, Any], time_zone: str = \"UTC\"):\n",
    "    \"\"\"Read one YAML file, validate, and flatten into rows for the table.\"\"\"\n",
    "    file_base = os.path.splitext(os.path.basename(path))[0]\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = yaml.safe_load(fh)\n",
    "    if isinstance(docs, dict):\n",
    "        docs = [docs]\n",
    "\n",
    "    validate_rules_file(docs, file_base, path)\n",
    "\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat_rules = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path)\n",
    "\n",
    "        h = compute_hash(rule)\n",
    "        check_dict = rule[\"check\"]\n",
    "\n",
    "        # Strong typing for DQX struct:\n",
    "        function = check_dict.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\")\n",
    "\n",
    "        for_each = check_dict.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\")\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\")\n",
    "\n",
    "        arguments = check_dict.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)  # enforce map<string,string>\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map<string,string> (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        flat_rules.append({\n",
    "            \"hash_id\": h,\n",
    "            \"table_name\": rule[\"table_name\"],\n",
    "\n",
    "            \"name\": rule[\"name\"],\n",
    "            \"criticality\": rule[\"criticality\"],\n",
    "            \"check\": {\n",
    "                \"function\": function,\n",
    "                \"for_each_column\": for_each if for_each else None,\n",
    "                \"arguments\": arguments if arguments else None,\n",
    "            },\n",
    "            \"filter\": rule.get(\"filter\"),\n",
    "            \"run_config_name\": rule[\"run_config_name\"],\n",
    "            \"user_metadata\": user_metadata if user_metadata else None,\n",
    "\n",
    "            \"yaml_path\": path,\n",
    "            \"active\": rule.get(\"active\", True),\n",
    "            \"created_by\": \"AdminUser\",\n",
    "            \"created_at\": now,\n",
    "            \"updated_by\": None,\n",
    "            \"updated_at\": None,\n",
    "        })\n",
    "\n",
    "    # Validate with DQX engine (semantic)\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat_rules\n",
    "\n",
    "\n",
    "def parse_output_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, \"r\") as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "    # Expect the new keys\n",
    "    required = [\"dqx_checks_config_table_name\", \"dqx_yaml_checks\", \"run_config_name\"]\n",
    "    missing = [k for k in required if k not in config]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Config missing required keys: {missing}\")\n",
    "    return config\n",
    "\n",
    "\n",
    "def validate_rules_file(rules, file_base: str, file_path: str):\n",
    "    problems = []\n",
    "    seen_names = set()\n",
    "    table_names = {r.get(\"table_name\") for r in rules if isinstance(r, dict)}\n",
    "    if len(table_names) != 1:\n",
    "        problems.append(f\"Inconsistent table_name values in {file_path}: {table_names}\")\n",
    "    expected_table = file_base\n",
    "    try:\n",
    "        tn = list(table_names)[0]\n",
    "        if tn.split(\".\")[-1] != expected_table:\n",
    "            problems.append(\n",
    "                f\"Table name in rules ({tn}) does not match filename ({expected_table}) in {file_path}\"\n",
    "            )\n",
    "    except Exception:\n",
    "        problems.append(f\"No valid table_name found in {file_path}\")\n",
    "\n",
    "    for rule in rules:\n",
    "        name = rule.get(\"name\")\n",
    "        if not name:\n",
    "            problems.append(f\"Missing rule name in {file_path}\")\n",
    "        if name in seen_names:\n",
    "            problems.append(f\"Duplicate rule name '{name}' in {file_path}\")\n",
    "        seen_names.add(name)\n",
    "\n",
    "    if problems:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {problems}\")\n",
    "\n",
    "\n",
    "def validate_rule_fields(rule, file_path: str):\n",
    "    problems = []\n",
    "    required_fields = [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "    for field in required_fields:\n",
    "        if not rule.get(field):\n",
    "            problems.append(\n",
    "                f\"Missing required field '{field}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "            )\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        problems.append(\n",
    "            f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if rule.get(\"criticality\") not in {\"error\", \"warn\", \"warning\"}:\n",
    "        problems.append(\n",
    "            f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        problems.append(\n",
    "            f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if problems:\n",
    "        raise ValueError(f\"Rule-level validation failed: {problems}\")\n",
    "\n",
    "\n",
    "def validate_with_dqx(rules, file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if status.has_errors:\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "\n",
    "def ensure_delta_table(spark: SparkSession, delta_table_name: str):\n",
    "    if not spark.catalog.tableExists(delta_table_name):\n",
    "        print(f\"Creating new Delta table at {delta_table_name}\")\n",
    "        empty_df = spark.createDataFrame([], TABLE_SCHEMA)\n",
    "        empty_df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "    else:\n",
    "        print(f\"Delta table already exists at {delta_table_name}\")\n",
    "\n",
    "\n",
    "def upsert_rules_into_delta(spark: SparkSession, rules, delta_table_name: str):\n",
    "    if not rules:\n",
    "        print(\"No rules to write, skipping upsert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nWriting rules to Delta table '{delta_table_name}'...\")\n",
    "    print(f\"Number of rules to write: {len(rules)}\")\n",
    "\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "\n",
    "    # Cast audit timestamps to actual TIMESTAMP in the sink\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            \"target.yaml_path = source.yaml_path AND target.table_name = source.table_name AND target.name = source.name\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    except Exception:\n",
    "        print(\"Delta merge failed (likely first write). Writing full table.\")\n",
    "        df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "    print(f\"Successfully wrote {df.count()} rules to '{delta_table_name}'.\")\n",
    "\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules):\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    print(\"\\n==== Dry Run: Rules DataFrame to be uploaded ====\")\n",
    "    df.show(truncate=False, n=50)\n",
    "    print(f\"Total rules: {df.count()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_all_rules(rules_dir: str, output_config: Dict[str, Any], fail_fast: bool = True):\n",
    "    errors = []\n",
    "    print(f\"Starting validation for all YAML rule files in '{rules_dir}'\")\n",
    "    for fname in os.listdir(rules_dir):\n",
    "        if not fname.endswith((\".yaml\", \".yml\")):\n",
    "            continue\n",
    "        full_path = os.path.join(rules_dir, fname)\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            file_base = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            with open(full_path, \"r\") as fh:\n",
    "                docs = yaml.safe_load(fh)\n",
    "            if isinstance(docs, dict):\n",
    "                docs = [docs]\n",
    "            validate_rules_file(docs, file_base, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def main(\n",
    "    output_config_path: str = \"resources/dqx_config.yaml\",\n",
    "    rules_dir: Optional[str] = None,\n",
    "    time_zone: str = \"America/Chicago\",\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False\n",
    "):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    print_notebook_env(spark, local_timezone=time_zone)\n",
    "\n",
    "    output_config = parse_output_config(output_config_path)\n",
    "\n",
    "    # pick up rules_dir from config if not provided explicitly\n",
    "    rules_dir = rules_dir or output_config[\"dqx_yaml_checks\"]\n",
    "\n",
    "    delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "\n",
    "    all_rules = []\n",
    "    for fname in os.listdir(rules_dir):\n",
    "        if not fname.endswith((\".yaml\", \".yml\")):\n",
    "            continue\n",
    "        full_path = os.path.join(rules_dir, fname)\n",
    "        file_rules = process_yaml_file(full_path, output_config, time_zone=time_zone)\n",
    "        all_rules.extend(file_rules)\n",
    "\n",
    "    if validate_only:\n",
    "        print(\"\\nValidation only: not writing any rules.\")\n",
    "        validate_all_rules(rules_dir, output_config)\n",
    "        return\n",
    "\n",
    "    if dry_run:\n",
    "        print_rules_df(spark, all_rules)\n",
    "        return\n",
    "\n",
    "    ensure_delta_table(spark, delta_table_name)\n",
    "    upsert_rules_into_delta(spark, all_rules, delta_table_name)\n",
    "    print(f\"Finished writing rules to '{delta_table_name}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main(dry_run=True)\n",
    "    # main(validate_only=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b338fb5-c7a5-4f74-9a91-3e8d426351b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7514497-0027-45c6-9756-050f9e690576",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "New Rule Hashing Method"
    }
   },
   "outputs": [],
   "source": [
    "# load_dqx_checks.py\n",
    "\n",
    "# === Cell 1: Install DQX Package (if your cluster image doesn't already include it) ===\n",
    "%pip install databricks-labs-dqx\n",
    "\n",
    "# === Cell 2: Restart Python to pick up libs (Databricks convention) ===\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "# === Cell 3: Parameters (adjust paths/mode here) ===\n",
    "OUTPUT_CONFIG_PATH = \"resources/dqx_output_config.yaml\"\n",
    "RULES_DIR          = \"resources/dqx_checks_config\"   # trailing slash optional\n",
    "TIME_ZONE          = \"America/Chicago\"               # for audit timestamps\n",
    "MODE               = \"WRITE\"                         # one of: VALIDATE, DRY_RUN, WRITE\n",
    "\n",
    "# === Cell 4: Imports & helpers ===\n",
    "import os, json, re, hashlib, yaml\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from delta.tables import DeltaTable\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "# your utils (kept as-is per your environment)\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "\n",
    "# --- Section header for readable job logs ---\n",
    "def section(title: str) -> None:\n",
    "    bar = \"═\" * 70\n",
    "    print(f\"\\n{bar}\\n║ {title.ljust(68)}║\\n{bar}\")\n",
    "\n",
    "# --- DQX rule table schema (adds logic_hash; hash_id == logic_hash) ---\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"hash_id\",       T.StringType(), False),  # canonical content hash (same as logic_hash)\n",
    "    T.StructField(\"logic_hash\",    T.StringType(), False),  # explicit column for clarity/future\n",
    "    T.StructField(\"table_name\",    T.StringType(), False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",          T.StringType(), False),\n",
    "    T.StructField(\"criticality\",   T.StringType(), False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",         T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\",T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\",  T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # ops fields\n",
    "    T.StructField(\"yaml_path\",     T.StringType(), False),\n",
    "    T.StructField(\"active\",        T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\",    T.StringType(), False),\n",
    "    T.StructField(\"created_at\",    T.StringType(), False),  # ISO string; cast on write\n",
    "    T.StructField(\"updated_by\",    T.StringType(), True),\n",
    "    T.StructField(\"updated_at\",    T.StringType(), True),\n",
    "])\n",
    "\n",
    "# --- Canonical hashing utilities ---\n",
    "_WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def _norm_ws(s: str) -> str:\n",
    "    return _WHITESPACE_RE.sub(\" \", s.strip())\n",
    "\n",
    "def _sorted_list(v) -> List[str]:\n",
    "    return sorted((str(x) for x in v))\n",
    "\n",
    "def _normalize_arguments(func: str, args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Order-insensitive, stringified, semantics-preserving normalization for check.arguments.\"\"\"\n",
    "    if not args:\n",
    "        return {}\n",
    "    func = (func or \"\").lower()\n",
    "    out: Dict[str, Any] = {}\n",
    "    for k, v in args.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if k in {\"columns\", \"allowed\"} and isinstance(v, (list, tuple)):\n",
    "            out[k] = _sorted_list(v)\n",
    "        elif k == \"column\":\n",
    "            out[k] = str(v)\n",
    "        elif k in {\"min_limit\", \"max_limit\"}:\n",
    "            out[k] = str(v)\n",
    "        elif k == \"regex\":\n",
    "            out[k] = str(v)\n",
    "        elif k == \"expression\" and isinstance(v, str):\n",
    "            out[k] = _norm_ws(v)\n",
    "        else:\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                out[k] = [str(x) for x in v]\n",
    "            elif isinstance(v, dict):\n",
    "                out[k] = {kk: str(vv) for kk, vv in sorted(v.items())}\n",
    "            else:\n",
    "                out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def canonical_rule_payload(rule: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Minimal, stable payload for hashing identity.\n",
    "    NOTE: We keep for_each_column as a normalized list to match your existing behavior.\n",
    "    \"\"\"\n",
    "    table_name = str(rule[\"table_name\"]).lower()\n",
    "    check = rule[\"check\"]\n",
    "    func = str(check[\"function\"]).lower()\n",
    "    args = _normalize_arguments(func, check.get(\"arguments\", {}) or {})\n",
    "    fec = check.get(\"for_each_column\")\n",
    "    fec_norm = _sorted_list(fec) if isinstance(fec, (list, tuple)) else None\n",
    "    filt = rule.get(\"filter\")\n",
    "    filt = _norm_ws(filt) if isinstance(filt, str) else None\n",
    "\n",
    "    # Exclude: name, run_config_name, user_metadata, criticality\n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"check\": {\"function\": func, \"arguments\": args, \"for_each_column\": fec_norm},\n",
    "        \"filter\": filt,\n",
    "    }\n",
    "\n",
    "def compute_logic_hash(rule: Dict[str, Any]) -> str:\n",
    "    payload = canonical_rule_payload(rule)\n",
    "    data = json.dumps(payload, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "    return hashlib.sha256(data.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Convert arbitrary JSON-serializable values to map<string,string> for DQX.\"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "# --- Validation & processing functions (kept, with minor additions) ---\n",
    "def parse_output_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, \"r\") as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "    if \"dqx_config_table_name\" not in config or \"run_config_name\" not in config:\n",
    "        raise ValueError(\"Config must include 'dqx_config_table_name' and 'run_config_name'.\")\n",
    "    return config\n",
    "\n",
    "def validate_rules_file(rules, file_base: str, file_path: str):\n",
    "    problems = []\n",
    "    seen_names = set()\n",
    "    table_names = {r.get(\"table_name\") for r in rules if isinstance(r, dict)}\n",
    "    if len(table_names) != 1:\n",
    "        problems.append(f\"Inconsistent table_name values in {file_path}: {table_names}\")\n",
    "    expected_table = file_base\n",
    "    try:\n",
    "        tn = list(table_names)[0]\n",
    "        if tn.split(\".\")[-1] != expected_table:\n",
    "            problems.append(f\"Table name in rules ({tn}) does not match filename ({expected_table}) in {file_path}\")\n",
    "    except Exception:\n",
    "        problems.append(f\"No valid table_name found in {file_path}\")\n",
    "    for rule in rules:\n",
    "        name = rule.get(\"name\")\n",
    "        if not name:\n",
    "            problems.append(f\"Missing rule name in {file_path}\")\n",
    "        if name in seen_names:\n",
    "            problems.append(f\"Duplicate rule name '{name}' in {file_path}\")\n",
    "        seen_names.add(name)\n",
    "    if problems:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {problems}\")\n",
    "\n",
    "def validate_rule_fields(rule, file_path: str):\n",
    "    problems = []\n",
    "    required_fields = [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "    for field in required_fields:\n",
    "        if not rule.get(field):\n",
    "            problems.append(f\"Missing required field '{field}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        problems.append(f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"criticality\") not in {\"error\", \"warn\", \"warning\"}:\n",
    "        problems.append(f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        problems.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if problems:\n",
    "        raise ValueError(f\"Rule-level validation failed: {problems}\")\n",
    "\n",
    "def validate_with_dqx(rules, file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if status.has_errors:\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "def process_yaml_file(path: str, output_config: Dict[str, Any], time_zone: str = \"UTC\"):\n",
    "    \"\"\"Read one YAML file, validate, and flatten into rows for the table.\"\"\"\n",
    "    file_base = os.path.splitext(os.path.basename(path))[0]\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = yaml.safe_load(fh)\n",
    "    if isinstance(docs, dict):\n",
    "        docs = [docs]\n",
    "\n",
    "    validate_rules_file(docs, file_base, path)\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat_rules = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path)\n",
    "\n",
    "        # Canonical logic hash (independent of name/run_config/criticality)\n",
    "        logic_hash = compute_logic_hash(rule)\n",
    "\n",
    "        check_dict = rule[\"check\"]\n",
    "        function = check_dict.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\")\n",
    "\n",
    "        for_each = check_dict.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\")\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\")\n",
    "\n",
    "        arguments = check_dict.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map<string,string> (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "        else:\n",
    "            user_metadata = {}\n",
    "\n",
    "        # Inject rule_id into user_metadata so the runner can pick it up directly\n",
    "        user_metadata.setdefault(\"rule_id\", logic_hash)\n",
    "\n",
    "        flat_rules.append({\n",
    "            \"hash_id\":       logic_hash,           # CHANGED: same as logic_hash\n",
    "            \"logic_hash\":    logic_hash,           # NEW\n",
    "            \"table_name\":    rule[\"table_name\"],\n",
    "            \"name\":          rule[\"name\"],\n",
    "            \"criticality\":   rule[\"criticality\"],\n",
    "            \"check\": {\n",
    "                \"function\":        function,\n",
    "                \"for_each_column\": for_each if for_each else None,\n",
    "                \"arguments\":       arguments if arguments else None,\n",
    "            },\n",
    "            \"filter\":         rule.get(\"filter\"),\n",
    "            \"run_config_name\":rule[\"run_config_name\"],\n",
    "            \"user_metadata\":  user_metadata if user_metadata else None,\n",
    "            \"yaml_path\":      path,\n",
    "            \"active\":         rule.get(\"active\", True),\n",
    "            \"created_by\":     \"AdminUser\",\n",
    "            \"created_at\":     now,\n",
    "            \"updated_by\":     None,\n",
    "            \"updated_at\":     None,\n",
    "        })\n",
    "\n",
    "    # Semantic validation by DQX engine\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat_rules\n",
    "\n",
    "def ensure_delta_table(spark: SparkSession, delta_table_name: str):\n",
    "    if not spark.catalog.tableExists(delta_table_name):\n",
    "        print(f\"Creating new Delta table at {delta_table_name}\")\n",
    "        empty_df = spark.createDataFrame([], TABLE_SCHEMA)\n",
    "        empty_df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "    else:\n",
    "        print(f\"Delta table already exists at {delta_table_name}\")\n",
    "\n",
    "def upsert_rules_into_delta(spark: SparkSession, rules, delta_table_name: str):\n",
    "    if not rules:\n",
    "        print(\"No rules to write, skipping upsert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nWriting rules to Delta table '{delta_table_name}'...\")\n",
    "    print(f\"Number of rules to write: {len(rules)}\")\n",
    "\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            \"target.yaml_path = source.yaml_path AND target.table_name = source.table_name AND target.name = source.name\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    except Exception:\n",
    "        print(\"Delta merge failed (likely first write). Writing full table.\")\n",
    "        df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "    print(f\"Successfully wrote {df.count()} rules to '{delta_table_name}'.\")\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules):\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    print(\"\\n==== Dry Run: Rules DataFrame to be uploaded ====\")\n",
    "    df.show(truncate=False, n=50)\n",
    "    print(f\"Total rules: {df.count()}\")\n",
    "    return df\n",
    "\n",
    "def validate_all_rules(rules_dir: str, output_config: Dict[str, Any], fail_fast: bool = True):\n",
    "    errors = []\n",
    "    print(f\"Starting validation for all YAML rule files in '{rules_dir}'\")\n",
    "    for fname in sorted(os.listdir(rules_dir)):\n",
    "        if not fname.endswith((\".yaml\", \".yml\")) or fname.startswith(\".\"):\n",
    "            continue\n",
    "        full_path = os.path.join(rules_dir, fname)\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            file_base = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            with open(full_path, \"r\") as fh:\n",
    "                docs = yaml.safe_load(fh)\n",
    "            if isinstance(docs, dict):\n",
    "                docs = [docs]\n",
    "            validate_rules_file(docs, file_base, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "    \n",
    "# === Cell 5: Thin Orchestrator (holds state; logic stays in pure functions) ===\n",
    "class RunMode(Enum):\n",
    "    VALIDATE = auto()\n",
    "    DRY_RUN  = auto()\n",
    "    WRITE    = auto()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LoaderConfig:\n",
    "    output_config_path: str\n",
    "    rules_dir: str\n",
    "    time_zone: str = \"America/Chicago\"\n",
    "\n",
    "class DQXRuleLoader:\n",
    "    \"\"\"Tiny facade for clear phases & logging; recomputes rules on every run.\"\"\"\n",
    "    def __init__(self, spark: SparkSession, cfg: LoaderConfig):\n",
    "        self.spark = spark\n",
    "        self.cfg = cfg\n",
    "\n",
    "        section(\"PARSING OUTPUT CONFIG\")\n",
    "        self.output_config = parse_output_config(cfg.output_config_path)\n",
    "        self.delta_table   = self.output_config[\"dqx_config_table_name\"]\n",
    "\n",
    "    def _load_all_rules(self) -> List[Dict[str, Any]]:\n",
    "        section(\"LOADING + FLATTENING RULES FROM YAML\")\n",
    "        rules: List[Dict[str, Any]] = []\n",
    "        rules_dir = self.cfg.rules_dir.rstrip(\"/\")\n",
    "\n",
    "        for fname in sorted(os.listdir(rules_dir)):\n",
    "            if not fname.endswith((\".yaml\", \".yml\")) or fname.startswith(\".\"):\n",
    "                continue\n",
    "            path = os.path.join(rules_dir, fname)\n",
    "            rules.extend(process_yaml_file(path, self.output_config, time_zone=self.cfg.time_zone))\n",
    "        print(f\"Collected {len(rules)} rule rows from YAML.\")\n",
    "        return rules\n",
    "\n",
    "    def run(self, mode: RunMode) -> None:\n",
    "        section(\"ENVIRONMENT\")\n",
    "        print_notebook_env(self.spark, local_timezone=self.cfg.time_zone)\n",
    "\n",
    "        rules = self._load_all_rules()\n",
    "\n",
    "        if mode is RunMode.VALIDATE:\n",
    "            section(\"VALIDATE-ONLY\")\n",
    "            validate_all_rules(self.cfg.rules_dir, self.output_config)\n",
    "            return\n",
    "\n",
    "        if mode is RunMode.DRY_RUN:\n",
    "            section(\"DRY RUN (SHOW DATAFRAME)\")\n",
    "            print_rules_df(self.spark, rules)\n",
    "            return\n",
    "\n",
    "        section(\"WRITE (UPSERT INTO DELTA)\")\n",
    "        ensure_delta_table(self.spark, self.delta_table)\n",
    "        upsert_rules_into_delta(self.spark, rules, self.delta_table)\n",
    "        print(f\"Finished writing rules to '{self.delta_table}'.\")\n",
    "        \n",
    "        \n",
    "# === Cell 6: Entrypoint ===\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "loader = DQXRuleLoader(\n",
    "    spark,\n",
    "    LoaderConfig(\n",
    "        output_config_path=OUTPUT_CONFIG_PATH,\n",
    "        rules_dir=RULES_DIR,\n",
    "        time_zone=TIME_ZONE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mode = {\n",
    "    \"VALIDATE\": RunMode.VALIDATE,\n",
    "    \"DRY_RUN\":  RunMode.DRY_RUN,\n",
    "    \"WRITE\":    RunMode.WRITE,\n",
    "}.get(MODE.upper(), RunMode.WRITE)\n",
    "\n",
    "loader.run(mode)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
