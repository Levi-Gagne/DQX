# dqx_checks_config/job_run_audit.yaml

# 1) Row-level: basic presence
- table_name: dq_prd.monitoring.job_run_audit
  name: run_id_is_not_null
  criticality: error
  run_config_name: default
  check:
    function: is_not_null
    arguments:
      column: run_id

# 2) Dataset-level: unique key
- table_name: dq_prd.monitoring.job_run_audit
  name: run_id_is_unique
  criticality: error
  run_config_name: default
  check:
    function: is_unique
    arguments:
      columns: [run_id]

# 3) Row-level: temporal ordering
- table_name: dq_prd.monitoring.job_run_audit
  name: end_after_or_equal_start
  criticality: error
  run_config_name: default
  check:
    function: sql_expression
    arguments:
      expression: "end_time IS NULL OR end_time >= start_time"

# 4) Row-level + filter: conditional rule (only TERMINATED runs)
- table_name: dq_prd.monitoring.job_run_audit
  name: result_state_allowed_when_terminated
  criticality: warn
  run_config_name: default
  filter: "state = 'TERMINATED'"
  check:
    function: is_in_list
    arguments:
      column: result_state
      allowed: ["SUCCESS", "FAILED", "CANCELED", "INTERNAL_ERROR"]

# 5) Row-level: apply to multiple text columns (for_each_column)
- table_name: dq_prd.monitoring.job_run_audit
  name: required_text_fields_not_empty
  criticality: warn
  run_config_name: default
  check:
    function: is_not_null_and_not_empty
    for_each_column:
      - job_name
      - description
      - run_page_url

# 6) Row-level + metadata: rolling recency window
- table_name: dq_prd.monitoring.job_run_audit
  name: start_time_within_90_days
  criticality: warn
  run_config_name: default
  check:
    function: sql_expression
    arguments:
      expression: "start_time >= current_timestamp() - INTERVAL 90 DAYS"
  user_metadata:
    alert_channel: "ops"
    playbook: "job-latency"
    owner: "data-eng"

# 7) Row-level: regex on URL format
- table_name: dq_prd.monitoring.job_run_audit
  name: run_page_url_is_https
  criticality: warn
  run_config_name: default
  check:
    function: regex_match
    arguments:
      column: run_page_url
      regex: "^https://.+"

# 8) Dataset-level: composite uniqueness (one run per job+start_time)
- table_name: dq_prd.monitoring.job_run_audit
  name: unique_job_id_start_time
  criticality: error
  run_config_name: default
  check:
    function: is_unique
    arguments:
      columns: [job_id, start_time]

# 9) Row-level: numeric range on raw epoch
- table_name: dq_prd.monitoring.job_run_audit
  name: start_time_raw_is_positive
  criticality: error
  run_config_name: default
  check:
    function: is_in_range
    arguments:
      column: start_time_raw
      min_limit: 1

# 10) Row-level + filter: failed runs must carry an error_code
- table_name: dq_prd.monitoring.job_run_audit
  name: failed_runs_have_error_code
  criticality: error
  run_config_name: default
  filter: "result_state = 'FAILED'"
  check:
    function: sql_expression
    arguments:
      expression: "error_code IS NOT NULL AND trim(error_code) <> ''"