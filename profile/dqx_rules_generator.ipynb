{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4fe261-69b0-4d1a-a5a0-cb4d445b418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DQX Rules Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2168addb-58e3-4aae-853c-7e2f844cda7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx\n",
    "\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "import databricks.labs.dqx\n",
    "print(databricks.labs.dqx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f54cc1dd-098c-41d0-8bbf-66d0312ce651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import inspect\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "from pyspark.sql import SparkSession, types as T, DataFrame\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        name_param: str,\n",
    "        output_table: str,\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,\n",
    "        created_by: Optional[str] = \"admin\",\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_table = output_table\n",
    "        self.profile_options = self._validate_profile_options(profile_options)\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _allowed_profile_options():\n",
    "        return set(inspect.signature(DQProfiler.profile).parameters.keys()) - {\"self\", \"df\"}\n",
    "\n",
    "    def _validate_profile_options(self, opts: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        allowed = self._allowed_profile_options()\n",
    "        valid = {k: v for k, v in (opts or {}).items() if k in allowed}\n",
    "        ignored = set(opts or {}).difference(allowed)\n",
    "        if ignored:\n",
    "            print(f\"[WARN] Ignored invalid profile_options keys: {sorted(ignored)}\")\n",
    "        return valid\n",
    "\n",
    "    def _get_rules_output_schema(self):\n",
    "        return T.StructType([\n",
    "            T.StructField(\"batch_id\", T.IntegerType(), True),\n",
    "            T.StructField(\"pipeline\", T.StringType(), True),\n",
    "            T.StructField(\"is_pipeline\", T.BooleanType(), True),\n",
    "            T.StructField(\"source\", T.StringType(), True),\n",
    "            T.StructField(\"catalog\", T.StringType(), True),\n",
    "            T.StructField(\"schema\", T.StringType(), True),\n",
    "            T.StructField(\"table\", T.StringType(), True),\n",
    "            T.StructField(\"profile_options\", T.ArrayType(\n",
    "                T.StructType([\n",
    "                    T.StructField(\"key\", T.StringType(), False),\n",
    "                    T.StructField(\"value\", T.StringType(), True)\n",
    "                ])\n",
    "            ), True),\n",
    "            T.StructField(\"rule_name\", T.StringType(), True),\n",
    "            T.StructField(\"rule_constraint\", T.StringType(), True),\n",
    "            T.StructField(\"created_by\", T.StringType(), True),\n",
    "            T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "        ])\n",
    "\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        # === PRINT: Values passed in this run ===\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:           {self.mode}\")\n",
    "        print(f\"name_param:     {self.name_param}\")\n",
    "        print(f\"output_table:   {self.output_table}\")\n",
    "        print(f\"exclude_pattern:{self.exclude_pattern}\")\n",
    "        print(f\"created_by:     {self.created_by}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        mode = self.mode\n",
    "        name_param = self.name_param\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "\n",
    "        discovered = []\n",
    "\n",
    "        if mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                pipeline_tables = [x for x in pipeline_tables if x]\n",
    "                print(f\"Found tables for pipeline '{pipeline_name}': {pipeline_tables}\")\n",
    "                discovered += pipeline_tables\n",
    "            print(f\"All discovered tables from pipelines: {discovered}\")\n",
    "\n",
    "        elif mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            catalog_tables = []\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                found = [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "                catalog_tables += found\n",
    "                if found:\n",
    "                    print(f\"Tables in {catalog}.{s}: {found}\")\n",
    "            discovered = catalog_tables\n",
    "            print(f\"All discovered tables in catalog: {discovered}\")\n",
    "\n",
    "        elif mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            schema_tables = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "            discovered = schema_tables\n",
    "            print(f\"Tables in schema {catalog}.{schema}: {schema_tables}\")\n",
    "\n",
    "        elif mode == \"table\":\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            tables = [t.strip() for t in name_param.split(\",\") if t.strip()]\n",
    "            for t in tables:\n",
    "                if t.count(\".\") != 2:\n",
    "                    raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "            discovered = tables\n",
    "            print(f\"Tables to be profiled: {discovered}\")\n",
    "\n",
    "        # Exclude\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "        print(\"\\nFinal table list to generate DQX rules for:\")\n",
    "        print(discovered)\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    def _get_next_batch_id(self) -> int:\n",
    "        table_exists = self.spark._jsparkSession.catalog().tableExists(self.output_table)\n",
    "        if not table_exists:\n",
    "            return 1\n",
    "        try:\n",
    "            existing = self.spark.table(self.output_table)\n",
    "            max_id = existing.agg({\"batch_id\": \"max\"}).collect()[0][0]\n",
    "            return (max_id or 0) + 1\n",
    "        except Exception:\n",
    "            return 1\n",
    "\n",
    "    def _dict_to_kv_array(self, d: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        return [{\"key\": k, \"value\": str(v)} for k, v in sorted(d.items())]\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            # === PRINT: Starting rule generation section ===\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "            all_rules = []\n",
    "            utc_now = datetime.now(timezone.utc)\n",
    "            used_options = self.profile_options\n",
    "            profile_options_array = self._dict_to_kv_array(used_options)\n",
    "            batch_id = self._get_next_batch_id()\n",
    "            print(f\"[RUN] Using batch_id={batch_id}\")\n",
    "            for fq_table in tables:\n",
    "                parts = fq_table.split('.')\n",
    "                cat, sch, tab = (parts + [None, None, None])[:3]\n",
    "                is_pipeline = self.mode == \"pipeline\"\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table: {fq_table}\")\n",
    "                    self.spark.table(fq_table).show(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    summary_stats, profiles = profiler.profile(df, **used_options)\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "                for rule_name, constraint in rules_dict.items():\n",
    "                    all_rules.append({\n",
    "                        \"batch_id\": batch_id,\n",
    "                        \"pipeline\": self.name_param if is_pipeline else None,\n",
    "                        \"is_pipeline\": is_pipeline,\n",
    "                        \"source\": fq_table,\n",
    "                        \"catalog\": cat,\n",
    "                        \"schema\": sch,\n",
    "                        \"table\": tab,\n",
    "                        \"profile_options\": profile_options_array,\n",
    "                        \"rule_name\": rule_name,\n",
    "                        \"rule_constraint\": constraint,\n",
    "                        \"created_by\": self.created_by,\n",
    "                        \"created_at\": utc_now,\n",
    "                    })\n",
    "            if all_rules:\n",
    "                print(f\"[RUN] Writing {len(all_rules)} rules to output table: {self.output_table}\")\n",
    "                schema = self._get_rules_output_schema()\n",
    "                rules_df = self.spark.createDataFrame(all_rules, schema=schema)\n",
    "                output_schema = \".\".join(self.output_table.split(\".\")[:-1])\n",
    "                self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {output_schema}\")\n",
    "                rules_df.write.mode(\"append\").saveAsTable(self.output_table)\n",
    "                print(f\"[RUN] Successfully wrote to table: {self.output_table}\")\n",
    "            else:\n",
    "                print(\"[INFO] No rules generated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "# No changes to the example usage below:\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        \"max_in_count\": 10,\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": None,\n",
    "        \"limit\": 1000,\n",
    "        \"profile_types\": None,\n",
    "        \"min_length\": None,\n",
    "        \"max_length\": None,\n",
    "        \"include_histograms\": False,\n",
    "        \"min_value\": None,\n",
    "        \"max_value\": None,\n",
    "    }\n",
    "    RuleGenerator(\n",
    "        mode=\"pipeline\",\n",
    "        name_param=\"pl_zoo_bronze\",\n",
    "        output_table=\"dq_dev.expectations.dqx_expectations\",\n",
    "        profile_options=profile_options,\n",
    "        exclude_pattern=None,\n",
    "        created_by=\"levi\",\n",
    "    ).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29833b1c-0236-46f9-a6f9-adb15dd55ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import inspect\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Optional, Dict, Any\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from pyspark.sql import SparkSession, types as T, DataFrame\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        name_param: str,\n",
    "        output_table: str,\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,\n",
    "        created_by: Optional[str] = \"admin\",\n",
    "        columns: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_table = output_table\n",
    "        self.profile_options = self._validate_profile_options(profile_options)\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _allowed_profile_options():\n",
    "        return set(inspect.signature(DQProfiler.profile).parameters.keys()) - {\"self\", \"df\"}\n",
    "\n",
    "    def _validate_profile_options(self, opts: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        allowed = self._allowed_profile_options()\n",
    "        valid = {k: v for k, v in (opts or {}).items() if k in allowed}\n",
    "        ignored = set(opts or {}).difference(allowed)\n",
    "        if ignored:\n",
    "            print(f\"[WARN] Ignored invalid profile_options keys: {sorted(ignored)}\")\n",
    "        return valid\n",
    "\n",
    "    def _get_rules_output_schema(self):\n",
    "        return T.StructType([\n",
    "            T.StructField(\"batch_id\", T.IntegerType(), True),\n",
    "            T.StructField(\"pipeline\", T.StringType(), True),\n",
    "            T.StructField(\"is_pipeline\", T.BooleanType(), True),\n",
    "            T.StructField(\"source\", T.StringType(), True),\n",
    "            T.StructField(\"catalog\", T.StringType(), True),\n",
    "            T.StructField(\"schema\", T.StringType(), True),\n",
    "            T.StructField(\"table\", T.StringType(), True),\n",
    "            T.StructField(\"columns\", T.ArrayType(T.StringType()), True),\n",
    "            # -- Audit fields (order matters here) --\n",
    "            T.StructField(\"profile_options\", T.ArrayType(\n",
    "                T.StructType([\n",
    "                    T.StructField(\"key\", T.StringType(), False),\n",
    "                    T.StructField(\"value\", T.StringType(), True)\n",
    "                ])\n",
    "            ), True),\n",
    "            T.StructField(\"rule_generator_params\", T.ArrayType(\n",
    "                T.StructType([\n",
    "                    T.StructField(\"key\", T.StringType(), False),\n",
    "                    T.StructField(\"value\", T.StringType(), True)\n",
    "                ])\n",
    "            ), True),\n",
    "            T.StructField(\"created_by\", T.StringType(), True),\n",
    "            T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "            # -- End audit fields --\n",
    "            T.StructField(\"rule_name\", T.StringType(), True),\n",
    "            T.StructField(\"rule_constraint\", T.StringType(), True),\n",
    "        ])\n",
    "\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:           {self.mode}\")\n",
    "        print(f\"name_param:     {self.name_param}\")\n",
    "        print(f\"output_table:   {self.output_table}\")\n",
    "        print(f\"exclude_pattern:{self.exclude_pattern}\")\n",
    "        print(f\"created_by:     {self.created_by}\")\n",
    "        print(f\"columns:        {self.columns}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        mode = self.mode\n",
    "        name_param = self.name_param\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "\n",
    "        # Validate that columns can only be used in table mode\n",
    "        if self.columns is not None and mode != \"table\":\n",
    "            raise ValueError(\"The 'columns' parameter can only be used in mode='table'.\")\n",
    "\n",
    "        discovered = []\n",
    "\n",
    "        if mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                pipeline_tables = [x for x in pipeline_tables if x]\n",
    "                print(f\"Found tables for pipeline '{pipeline_name}': {pipeline_tables}\")\n",
    "                discovered += pipeline_tables\n",
    "            print(f\"All discovered tables from pipelines: {discovered}\")\n",
    "\n",
    "        elif mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            catalog_tables = []\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                found = [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "                catalog_tables += found\n",
    "                if found:\n",
    "                    print(f\"Tables in {catalog}.{s}: {found}\")\n",
    "            discovered = catalog_tables\n",
    "            print(f\"All discovered tables in catalog: {discovered}\")\n",
    "\n",
    "        elif mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            schema_tables = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "            discovered = schema_tables\n",
    "            print(f\"Tables in schema {catalog}.{schema}: {schema_tables}\")\n",
    "\n",
    "        elif mode == \"table\":\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            tables = [t.strip() for t in name_param.split(\",\") if t.strip()]\n",
    "            for t in tables:\n",
    "                if t.count(\".\") != 2:\n",
    "                    raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "            discovered = tables\n",
    "            print(f\"Tables to be profiled: {discovered}\")\n",
    "\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "        print(\"\\nFinal table list to generate DQX rules for:\")\n",
    "        print(discovered)\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    def _get_next_batch_id(self) -> int:\n",
    "        table_exists = self.spark._jsparkSession.catalog().tableExists(self.output_table)\n",
    "        if not table_exists:\n",
    "            return 1\n",
    "        try:\n",
    "            existing = self.spark.table(self.output_table)\n",
    "            max_id = existing.agg({\"batch_id\": \"max\"}).collect()[0][0]\n",
    "            return (max_id or 0) + 1\n",
    "        except Exception:\n",
    "            return 1\n",
    "\n",
    "    def _dict_to_kv_array(self, d: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        return [{\"key\": k, \"value\": str(v)} for k, v in sorted(d.items())]\n",
    "\n",
    "    def _get_rule_generator_params(self) -> List[Dict[str, str]]:\n",
    "        # Audit parameters passed to main\n",
    "        params = {\n",
    "            \"mode\": self.mode,\n",
    "            \"name_param\": self.name_param,\n",
    "            \"output_table\": self.output_table,\n",
    "            \"columns\": json.dumps(self.columns) if self.columns is not None else \"None\",\n",
    "            \"exclude_pattern\": str(self.exclude_pattern),\n",
    "            \"created_by\": self.created_by,\n",
    "        }\n",
    "        return [{\"key\": k, \"value\": str(v)} for k, v in params.items()]\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "            all_rules = []\n",
    "            utc_now = datetime.now(timezone.utc)\n",
    "            used_options = self.profile_options\n",
    "            profile_options_array = self._dict_to_kv_array(used_options)\n",
    "            rule_generator_params_array = self._get_rule_generator_params()\n",
    "            batch_id = self._get_next_batch_id()\n",
    "            print(f\"[RUN] Using batch_id={batch_id}\")\n",
    "            for fq_table in tables:\n",
    "                parts = fq_table.split('.')\n",
    "                cat, sch, tab = (parts + [None, None, None])[:3]\n",
    "                is_pipeline = self.mode == \"pipeline\"\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table: {fq_table}\")\n",
    "                    self.spark.table(fq_table).show(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    if self.mode == \"table\" and self.columns is not None:\n",
    "                        summary_stats, profiles = profiler.profile(df, cols=self.columns, **used_options)\n",
    "                    else:\n",
    "                        summary_stats, profiles = profiler.profile(df, **used_options)\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "                for rule_name, constraint in rules_dict.items():\n",
    "                    rule_columns = []\n",
    "                    if self.columns is not None:\n",
    "                        rule_columns = self.columns\n",
    "                    elif isinstance(rule_name, str) and rule_name in df.columns:\n",
    "                        rule_columns = [rule_name]\n",
    "                    all_rules.append({\n",
    "                        \"batch_id\": batch_id,\n",
    "                        \"pipeline\": self.name_param if is_pipeline else None,\n",
    "                        \"is_pipeline\": is_pipeline,\n",
    "                        \"source\": fq_table,\n",
    "                        \"catalog\": cat,\n",
    "                        \"schema\": sch,\n",
    "                        \"table\": tab,\n",
    "                        \"columns\": rule_columns,\n",
    "                        \"profile_options\": profile_options_array,\n",
    "                        \"rule_generator_params\": rule_generator_params_array,\n",
    "                        \"created_by\": self.created_by,\n",
    "                        \"created_at\": utc_now,\n",
    "                        \"rule_name\": rule_name,\n",
    "                        \"rule_constraint\": constraint,\n",
    "                    })\n",
    "            if all_rules:\n",
    "                print(f\"[RUN] Writing {len(all_rules)} rules to output table: {self.output_table}\")\n",
    "                schema = self._get_rules_output_schema()\n",
    "                rules_df = self.spark.createDataFrame(all_rules, schema=schema)\n",
    "                output_schema = \".\".join(self.output_table.split(\".\")[:-1])\n",
    "                self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {output_schema}\")\n",
    "                rules_df.write.mode(\"append\").saveAsTable(self.output_table)\n",
    "                print(f\"[RUN] Successfully wrote to table: {self.output_table}\")\n",
    "            else:\n",
    "                print(\"[INFO] No rules generated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        \"max_in_count\": 10,\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": None,\n",
    "        \"limit\": 1000,\n",
    "        \"profile_types\": None,\n",
    "        \"min_length\": None,\n",
    "        \"max_length\": None,\n",
    "        \"include_histograms\": False,\n",
    "        \"min_value\": None,\n",
    "        \"max_value\": None,\n",
    "    }\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",\n",
    "        name_param=\"dq_dev.lmg_expectations.zoo_animal_inventory_stg\",\n",
    "        output_table=\"dq_dev.expectations.dqx_expectations\",\n",
    "        profile_options=profile_options,\n",
    "        columns=[\"animal_id\", \"species\"],\n",
    "        exclude_pattern=None,\n",
    "        created_by=\"LMG\",\n",
    "    ).run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dqx_rules_generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
