{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea4df70b-d2e0-4a54-bc10-e40a21fbb059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DQX POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3cfc9d-34f8-4b64-a656-7add08fd5a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025ba2cf-d454-4c62-919a-6b8366be7a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from typing import List, Optional\n",
    "import inspect\n",
    "\n",
    "# -- DQX profiling options: all known keys --\n",
    "profile_options = {\n",
    "    # \"round\": True,                 # (Removed - not valid for this profiler version)\n",
    "    \"max_in_count\": 10,            # Max distinct values for is_in rule\n",
    "    \"distinct_ratio\": 0.05,        # Max unique/total ratio for is_in rule\n",
    "    \"max_null_ratio\": 0.01,        # Max null fraction to allow is_not_null rule\n",
    "    \"remove_outliers\": True,       # Remove outliers for min/max\n",
    "    \"outlier_columns\": [],         # Only these columns get outlier removal (empty=all numerics)\n",
    "    \"num_sigmas\": 3,               # Stddev for outlier removal (z-score cutoff)\n",
    "    \"trim_strings\": True,          # Strip whitespace before profiling strings\n",
    "    \"max_empty_ratio\": 0.01,       # Max empty string ratio for is_not_null_or_empty\n",
    "    \"sample_fraction\": 0.3,        # Row fraction to sample\n",
    "    \"sample_seed\": None,           # Seed for reproducibility (set int for deterministic)\n",
    "    \"limit\": 1000,                 # Max number of rows to profile\n",
    "    \"profile_types\": None,         # List of rule types (e.g. [\"is_in\", \"is_not_null\"]); None=default\n",
    "    \"min_length\": None,            # Min string length to consider (None disables)\n",
    "    \"max_length\": None,            # Max string length to consider (None disables)\n",
    "    \"include_histograms\": False,   # Compute histograms as part of profiling\n",
    "    \"min_value\": None,             # Numeric min override (None disables)\n",
    "    \"max_value\": None,             # Numeric max override (None disables)\n",
    "}\n",
    "\n",
    "def valid_profile_options(profile_options):\n",
    "    \"\"\"Filter out unsupported keys for DQProfiler.profile()\"\"\"\n",
    "    valid_keys = set(inspect.signature(DQProfiler.profile).parameters.keys()) - {\"self\", \"df\"}\n",
    "    return {k: v for k, v in profile_options.items() if k in valid_keys}\n",
    "\n",
    "# --- Table Discovery ---\n",
    "def discover_output_tables(\n",
    "    pipeline_name: str,\n",
    "    sdk_client: Optional[object] = None\n",
    ") -> List[str]:\n",
    "    w = sdk_client or WorkspaceClient()\n",
    "    pipelines = list(w.pipelines.list_pipelines())\n",
    "    pl = next((p for p in pipelines if p.name == pipeline_name), None)\n",
    "    if not pl:\n",
    "        print(f\"[ERROR] Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "        raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "    latest_update = pl.latest_updates[0].update_id\n",
    "    print(f\"[INFO] Using latest update ID: {latest_update} for pipeline: {pipeline_name}\")\n",
    "    events = w.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "    tables = set()\n",
    "    empty_pages = 0\n",
    "    buffer = []\n",
    "    it = iter(events)\n",
    "    while True:\n",
    "        buffer.clear()\n",
    "        try:\n",
    "            for _ in range(250):\n",
    "                buffer.append(next(it))\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        page_tables = {\n",
    "            getattr(ev.origin, \"flow_name\", None)\n",
    "            for ev in buffer\n",
    "            if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "        }\n",
    "        page_tables.discard(None)\n",
    "        if page_tables:\n",
    "            tables |= page_tables\n",
    "            empty_pages = 0\n",
    "            print(f\"[DEBUG] Found tables in this page: {sorted(page_tables)}\")\n",
    "        else:\n",
    "            empty_pages += 1\n",
    "            print(f\"[DEBUG] No tables found in this page. Empty pages count: {empty_pages}\")\n",
    "        if empty_pages >= 2 or not buffer:\n",
    "            break\n",
    "    found = sorted(tables)\n",
    "    if not found:\n",
    "        print(f\"[ERROR] No output tables found for pipeline '{pipeline_name}' using SDK event logs.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Found {len(found)} tables for pipeline '{pipeline_name}': {found}\")\n",
    "    return found\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_schema_name(table_name: str) -> str:\n",
    "    parts = table_name.split(\".\")\n",
    "    if len(parts) == 3:\n",
    "        return f\"{parts[0]}.{parts[1]}\"\n",
    "    elif len(parts) == 2:\n",
    "        return parts[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid table name: {table_name}\")\n",
    "\n",
    "def ensure_schema_exists(spark, table_name: str):\n",
    "    schema = get_schema_name(table_name)\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "\n",
    "def table_exists(spark, table_name):\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[table_exists] Table {table_name} not found or not readable: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_table_with_schema(spark, table_name):\n",
    "    ensure_schema_exists(spark, table_name)\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"pipeline\", T.StringType(), True),\n",
    "        T.StructField(\"table\", T.StringType(), True),\n",
    "        T.StructField(\"name\", T.StringType(), True),\n",
    "        T.StructField(\"constraint\", T.StringType(), True),\n",
    "    ])\n",
    "    empty_df = spark.createDataFrame([], schema)\n",
    "    empty_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "def generate_dlt_expectations_for_table(table_name, ws, spark, profile_options):\n",
    "    if not table_exists(spark, table_name):\n",
    "        print(f\"Skipping missing table: {table_name}\")\n",
    "        return None\n",
    "    df = spark.table(table_name)\n",
    "    profiler = DQProfiler(ws)\n",
    "    generator = DQDltGenerator(ws)\n",
    "    # Only valid options passed!\n",
    "    summary_stats, profiles = profiler.profile(df, **valid_profile_options(profile_options))\n",
    "    return generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "\n",
    "def create_or_overwrite_table(spark, df, output_table):\n",
    "    ensure_schema_exists(spark, output_table)\n",
    "    if table_exists(spark, output_table):\n",
    "        df.write.mode(\"overwrite\").saveAsTable(output_table)\n",
    "    else:\n",
    "        df.write.saveAsTable(output_table)\n",
    "\n",
    "def filter_existing_tables(spark, table_list):\n",
    "    existing = []\n",
    "    for t in table_list:\n",
    "        try:\n",
    "            print(f\"[filter_existing_tables] Trying to preview table: {t}\")\n",
    "            spark.table(t).show(1)\n",
    "            existing.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Table {t} not readable in Spark: {e}\")\n",
    "    return existing\n",
    "\n",
    "# --- Main Orchestration ---\n",
    "\n",
    "def generate_and_save_all_expectations(pipeline_name, output_table, profile_options):\n",
    "    ws = WorkspaceClient()\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if not spark:\n",
    "        raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "    print(f\"Using Spark session: {spark}\")\n",
    "    tables = discover_output_tables(pipeline_name, sdk_client=ws)\n",
    "    print(f\"Discovered tables for pipeline '{pipeline_name}': {tables}\")\n",
    "    existing_tables = filter_existing_tables(spark, tables)\n",
    "    print(f\"Tables readable in Spark: {existing_tables}\")\n",
    "    all_rules = []\n",
    "    for tbl in existing_tables:\n",
    "        print(f\"Profiling and generating expectations for table: {tbl}\")\n",
    "        rules = generate_dlt_expectations_for_table(tbl, ws, spark, profile_options)\n",
    "        if not rules:\n",
    "            continue\n",
    "        for name, constraint in rules.items():\n",
    "            all_rules.append((pipeline_name, tbl, name, constraint))\n",
    "    if all_rules:\n",
    "        rules_df = spark.createDataFrame(all_rules, schema=[\"pipeline\", \"table\", \"name\", \"constraint\"])\n",
    "        create_or_overwrite_table(spark, rules_df, output_table)\n",
    "        print(f\"Wrote {len(all_rules)} expectations to table '{output_table}'.\")\n",
    "    else:\n",
    "        print(\"No rules generated—check pipeline/table names and data.\")\n",
    "        if not table_exists(spark, output_table):\n",
    "            create_table_with_schema(spark, output_table)\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "pipeline_name = \"pl_zoo_bronze\"\n",
    "output_constraints_table = \"dq_dev.expectations.dqx_expectations\"\n",
    "\n",
    "generate_and_save_all_expectations(pipeline_name, output_constraints_table, profile_options)\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark and table_exists(spark, output_constraints_table):\n",
    "    display(spark.table(output_constraints_table))\n",
    "else:\n",
    "    print(f\"Table {output_constraints_table} does not exist after execution.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8784507272633992,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dqx_poc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
