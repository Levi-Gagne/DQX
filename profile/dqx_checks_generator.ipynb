{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4fe261-69b0-4d1a-a5a0-cb4d445b418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DQX Rules Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8383f8f1-9db5-46eb-abc3-d472fbc64507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- DQX profiling options: all known keys --\n",
    "profile_options = {\n",
    "    # \"round\": True,                 # (Removed - not valid for this profiler version)\n",
    "    \"max_in_count\": 10,            # Max distinct values for is_in rule\n",
    "    \"distinct_ratio\": 0.05,        # Max unique/total ratio for is_in rule\n",
    "    \"max_null_ratio\": 0.01,        # Max null fraction to allow is_not_null rule\n",
    "    \"remove_outliers\": True,       # Remove outliers for min/max\n",
    "    \"outlier_columns\": [],         # Only these columns get outlier removal (empty=all numerics)\n",
    "    \"num_sigmas\": 3,               # Stddev for outlier removal (z-score cutoff)\n",
    "    \"trim_strings\": True,          # Strip whitespace before profiling strings\n",
    "    \"max_empty_ratio\": 0.01,       # Max empty string ratio for is_not_null_or_empty\n",
    "    \"sample_fraction\": 0.3,        # Row fraction to sample\n",
    "    \"sample_seed\": None,           # Seed for reproducibility (set int for deterministic)\n",
    "    \"limit\": 1000,                 # Max number of rows to profile\n",
    "    \"profile_types\": None,         # List of rule types (e.g. [\"is_in\", \"is_not_null\"]); None=default\n",
    "    \"min_length\": None,            # Min string length to consider (None disables)\n",
    "    \"max_length\": None,            # Max string length to consider (None disables)\n",
    "    \"include_histograms\": False,   # Compute histograms as part of profiling\n",
    "    \"min_value\": None,             # Numeric min override (None disables)\n",
    "    \"max_value\": None,             # Numeric max override (None disables)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2168addb-58e3-4aae-853c-7e2f844cda7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8baa8a-c510-47e7-b107-3204ff7b2c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29833b1c-0236-46f9-a6f9-adb15dd55ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import inspect\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Optional, Dict, Any, Tuple, Literal\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,                       # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param: str,                 # pipeline name(s) CSV | catalog | catalog.schema | fully qualified table(s) CSV\n",
    "        output_format: str,              # \"yaml\" | \"table\"\n",
    "        output_location: str,            # yaml: folder or full file path; table: catalog.schema.table\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,     # e.g. \".foo_*\" (table-name glob with leading dot)\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,       # None => whole table; only valid if mode==\"table\"\n",
    "        run_config_name: str = \"default\",          # DQX run group tag on checks\n",
    "        criticality: str = \"warn\",                 # \"warn\" | \"error\"\n",
    "        yaml_key_order: Literal[\"engine\", \"custom\"] = \"engine\",  # \"engine\" (DQX save) | \"custom\" (strict key order)\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_location = output_location\n",
    "        # Pass options through as-is; let the profiler decide what to use.\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.yaml_key_order = yaml_key_order\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "        if self.output_format not in {\"yaml\", \"table\"}:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table'.\")\n",
    "        if self.output_format == \"yaml\" and not self.output_location:\n",
    "            raise ValueError(\"When output_format='yaml', provide output_location (folder or file).\")\n",
    "\n",
    "    # -------------------- discovery helpers --------------------\n",
    "\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:             {self.mode}\")\n",
    "        print(f\"name_param:       {self.name_param}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_location:  {self.output_location}\")\n",
    "        print(f\"exclude_pattern:  {self.exclude_pattern}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"yaml_key_order:   {self.yaml_key_order}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        mode = self.mode\n",
    "        name_param = self.name_param\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "\n",
    "        if self.columns is not None and mode != \"table\":\n",
    "            raise ValueError(\"The 'columns' parameter can only be used in mode='table'.\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "        if mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                pipeline_tables = [x for x in pipeline_tables if x]\n",
    "                print(f\"Found tables for pipeline '{pipeline_name}': {pipeline_tables}\")\n",
    "                discovered += pipeline_tables\n",
    "            print(f\"All discovered tables from pipelines: {discovered}\")\n",
    "\n",
    "        elif mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            catalog_tables = []\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                found = [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "                catalog_tables += found\n",
    "                if found:\n",
    "                    print(f\"Tables in {catalog}.{s}: {found}\")\n",
    "            discovered = catalog_tables\n",
    "            print(f\"All discovered tables in catalog: {discovered}\")\n",
    "\n",
    "        elif mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            schema_tables = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "            discovered = schema_tables\n",
    "            print(f\"Tables in schema {catalog}.{schema}: {schema_tables}\")\n",
    "\n",
    "        elif mode == \"table\":\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            tables = [t.strip() for t in name_param.split(\",\") if t.strip()]\n",
    "            for t in tables:\n",
    "                if t.count(\".\") != 2:\n",
    "                    raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "            discovered = tables\n",
    "            print(f\"Tables to be profiled: {discovered}\")\n",
    "\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "        print(\"\\nFinal table list to generate DQX rules for:\")\n",
    "        print(discovered)\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    # -------------------- storage config helpers --------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        \"\"\"\n",
    "        Choose appropriate StorageConfig for file-like output:\n",
    "          - /Volumes/...  -> VolumeFileChecksStorageConfig\n",
    "          - /...          -> WorkspaceFileChecksStorageConfig\n",
    "          - else          -> FileChecksStorageConfig (driver-local)\n",
    "        \"\"\"\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dq_constraint_to_check(\n",
    "        rule_name: str,\n",
    "        constraint_sql: str,\n",
    "        table_name: str,\n",
    "        criticality: str,\n",
    "        run_config_name: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert a profiler constraint (SQL) into a DQX check dict.\n",
    "        Key order: table_name, name, criticality, run_config_name, check\n",
    "        (insertion order preserved in Python 3.7+)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": criticality,\n",
    "            \"run_config_name\": run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    \"expression\": constraint_sql,\n",
    "                    \"name\": rule_name,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _write_yaml_ordered(self, checks: List[Dict[str, Any]], path: str) -> None:\n",
    "        \"\"\"\n",
    "        Dump YAML preserving key order and upload:\n",
    "          - /Volumes/... | dbfs:/... | /dbfs/... -> dbutils.fs.put\n",
    "          - /Shared/... or any workspace path -> Workspace Files API\n",
    "          - else -> local driver path\n",
    "        \"\"\"\n",
    "        yaml_str = yaml.safe_dump(checks, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils  # available in notebooks\n",
    "        except Exception:\n",
    "            dbutils = None\n",
    "\n",
    "        if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "            if not dbutils:\n",
    "                raise RuntimeError(\"dbutils not available to write to DBFS/Volumes.\")\n",
    "            target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "            dbutils.fs.put(target, yaml_str, True)\n",
    "            print(f\"[RUN] Wrote ordered YAML to {path}\")\n",
    "            return\n",
    "\n",
    "        if path.startswith(\"/\"):\n",
    "            wc = WorkspaceClient()\n",
    "            wc.files.upload(path=path, contents=yaml_str.encode(\"utf-8\"), overwrite=True)\n",
    "            print(f\"[RUN] Wrote ordered YAML to workspace file: {path}\")\n",
    "            return\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(yaml_str)\n",
    "        print(f\"[RUN] Wrote ordered YAML to local path: {path}\")\n",
    "\n",
    "    # -------------------- main entrypoint --------------------\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "            # Echo profiler options (now passed through as-is)\n",
    "            print(\"[RUN] Profiler options in effect:\")\n",
    "            for k, v in self.profile_options.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "\n",
    "            dq_engine = DQEngine(WorkspaceClient())\n",
    "            used_options = self.profile_options\n",
    "\n",
    "            total_checks = 0\n",
    "            for fq_table in tables:\n",
    "                parts = fq_table.split('.')\n",
    "                if len(parts) != 3:\n",
    "                    print(f\"[WARN] Skipping invalid table name: {fq_table}\")\n",
    "                    continue\n",
    "                cat, sch, tab = parts\n",
    "\n",
    "                # Verify table readable\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table readability: {fq_table}\")\n",
    "                    self.spark.table(fq_table).limit(1).collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    if self.mode == \"table\" and self.columns is not None:\n",
    "                        summary_stats, profiles = profiler.profile(df, cols=self.columns, **used_options)\n",
    "                    else:\n",
    "                        summary_stats, profiles = profiler.profile(df, **used_options)\n",
    "\n",
    "                    # Generate constraints (DLT-style) as Python dict\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except TypeError as e:\n",
    "                    # If profiler rejects unexpected kwargs, retry with no options\n",
    "                    print(f\"[WARN] Profiler rejected some options ({e}). Retrying with defaults...\")\n",
    "                    summary_stats, profiles = profiler.profile(df)\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert constraints -> DQX checks list (with our key order)\n",
    "                checks: List[Dict[str, Any]] = []\n",
    "                for rule_name, constraint in (rules_dict or {}).items():\n",
    "                    checks.append(\n",
    "                        self._dq_constraint_to_check(\n",
    "                            rule_name=rule_name,\n",
    "                            constraint_sql=constraint,\n",
    "                            table_name=fq_table,\n",
    "                            criticality=self.criticality,\n",
    "                            run_config_name=self.run_config_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if not checks:\n",
    "                    print(f\"[INFO] No checks generated for {fq_table}.\")\n",
    "                    continue\n",
    "\n",
    "                # Determine output target per table\n",
    "                if self.output_format == \"yaml\":\n",
    "                    # If output_location is a directory, write {table}.yaml under it.\n",
    "                    # If it's a file path ending with .yml/.yaml, use as-is.\n",
    "                    if self.output_location.endswith(\".yaml\") or self.output_location.endswith(\".yml\"):\n",
    "                        path = self.output_location\n",
    "                    else:\n",
    "                        path = self.output_location.rstrip(\"/\") + f\"/{tab}.yaml\"\n",
    "\n",
    "                    if self.yaml_key_order == \"engine\":\n",
    "                        cfg = self._infer_file_storage_config(path)\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks via DQX to: {path}\")\n",
    "                        dq_engine.save_checks(checks, config=cfg)\n",
    "                    else:\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks with strict key order to: {path}\")\n",
    "                        self._write_yaml_ordered(checks, path)\n",
    "\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "                elif self.output_format == \"table\":\n",
    "                    cfg = self._table_storage_config(\n",
    "                        table_fqn=self.output_location,\n",
    "                        run_config_name=self.run_config_name,\n",
    "                        mode=\"append\"\n",
    "                    )\n",
    "                    print(f\"[RUN] Appending {len(checks)} checks to table: {self.output_location} (run_config_name={self.run_config_name})\")\n",
    "                    dq_engine.save_checks(checks, config=cfg)\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported output_format: {self.output_format}\")\n",
    "\n",
    "            if total_checks:\n",
    "                print(f\"[RUN] Successfully saved {total_checks} checks.\")\n",
    "            else:\n",
    "                print(\"[INFO] No checks were saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "\n",
    "# -------------------- Usage example --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        \"max_in_count\": 10,\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": None,\n",
    "        \"limit\": 1000,\n",
    "        \"profile_types\": None,\n",
    "        \"min_length\": None,\n",
    "        \"max_length\": None,\n",
    "        \"include_histograms\": False,\n",
    "        \"min_value\": None,\n",
    "        \"max_value\": None,\n",
    "    }\n",
    "\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"dq_prd.monitoring.job_run_audit\",   # depends on mode\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"/Shared/dqx_checks\",           # yaml: folder or /Shared/foo.yml; table: catalog.schema.table\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",                      # tag checks for filtering when applying\n",
    "        criticality=\"warn\",                             # \"warn\" | \"error\"\n",
    "        yaml_key_order=\"custom\",                        # \"engine\" (DQX write) | \"custom\" (strict key order in YAML)\n",
    "    ).run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dqx_checks_generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
