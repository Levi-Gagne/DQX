{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eee61a2-011c-4ef8-ba15-23a1feef850c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DQX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaaa1444-dea5-4903-8a9a-a920c9bcc371",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kim's Notebook"
    }
   },
   "outputs": [],
   "source": [
    "https://adb-5943863453538272.12.azuredatabricks.net/editor/notebooks/4441860942440814?o=5943863453538272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db71ab93-6f9f-42c4-a5cf-aab8380cacd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DQX Dashboard"
    }
   },
   "outputs": [],
   "source": [
    "databricks labs dqx open-dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b21625d-5222-462d-8790-7313492cd62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04eae810-2a55-4e7e-b823-69924ccad859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DQX Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9de4f3c-0397-4fd5-862b-498eb9b27e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### DQEngine Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a458ab4c-05ce-4610-89a4-723f161a7af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Method                                 | Purpose                                                        | Local?    | Notes                                                   |\n",
    "|-----------------------------------------|----------------------------------------------------------------|-----------|---------------------------------------------------------|\n",
    "| apply_checks                           | Apply DQRule checks to DataFrame, adds _warning/_error cols    | Yes       | Use for reporting-only; does not split DF               |\n",
    "| apply_checks_and_split                  | Apply checks, returns valid/quarantine DFs                     | Yes       | Use if you want to split valid/invalid rows             |\n",
    "| apply_checks_and_save_in_table          | Apply checks, save valid/quarantine DFs to Delta tables        | No        | Needs OutputConfig and QuarantineConfig                 |\n",
    "| apply_checks_by_metadata                | Apply YAML/Delta table checks to DF, adds _warning/_error      | Yes       | Ideal for rules from YAML/Delta; no split               |\n",
    "| apply_checks_by_metadata_and_split      | Same, but returns valid/quarantine DFs                         | Yes       | Only use if you want to split output                    |\n",
    "| apply_checks_by_metadata_and_save_in_table | Metadata checks, save valid/quarantine to Delta tables        | No        | Needs OutputConfig and QuarantineConfig                 |\n",
    "| validate_checks                        | Validates structure/types of check config                      | Yes       | Use in CI/CD for YAML validation                        |\n",
    "| get_invalid                            | Extract rows with warnings/errors                              | Yes       |                                                        |\n",
    "| get_valid                              | Extract only valid rows                                        | Yes       |                                                        |\n",
    "| load_checks_from_local_file             | Read checks from YAML/JSON file                                | Yes       | For CI/CD/local dev                                     |\n",
    "| save_checks_in_local_file               | Write checks to YAML/JSON                                      | Yes       |                                                        |\n",
    "| load_checks_from_workspace_file         | Load checks from Databricks workspace file                     | No        | Only in DBX context                                     |\n",
    "| save_checks_in_workspace_file           | Save checks to workspace file                                  | No        |                                                        |\n",
    "| load_checks_from_installation           | Load checks from install dir                                   | No        |                                                        |\n",
    "| save_checks_in_installation             | Save checks to install dir                                     | No        |                                                        |\n",
    "| save_results_in_table                   | Save (output_df, quarantine_df) to Delta tables                | No        | Needs OutputConfig (and optional QuarantineConfig)      |\n",
    "| load_run_config                        | Load run config from install dir                               | No        |                                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d68581d-b467-4832-83c0-4e13c789c593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Pre-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d7c1fd-6207-47e9-a1f9-25e1c5bd9327",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DQRowRule"
    }
   },
   "outputs": [],
   "source": [
    "ROW_LEVEL_FUNCTIONS = {\n",
    "    \"is_not_null\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_empty\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_null_and_not_empty\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": [\"trim_strings\"]\n",
    "    },\n",
    "    \"is_in_list\": {\n",
    "        \"args\": [\"column\", \"allowed\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_null_and_is_in_list\": {\n",
    "        \"args\": [\"column\", \"allowed\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_null_and_not_empty_array\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_in_range\": {\n",
    "        \"args\": [\"column\", \"min_limit\", \"max_limit\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_in_range\": {\n",
    "        \"args\": [\"column\", \"min_limit\", \"max_limit\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_less_than\": {\n",
    "        \"args\": [\"column\", \"limit\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_not_greater_than\": {\n",
    "        \"args\": [\"column\", \"limit\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_valid_date\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": [\"date_format\"]\n",
    "    },\n",
    "    \"is_valid_timestamp\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": [\"timestamp_format\"]\n",
    "    },\n",
    "    \"is_not_in_future\": {\n",
    "        \"args\": [\"column\", \"offset\"],\n",
    "        \"optional\": [\"curr_timestamp\"]\n",
    "    },\n",
    "    \"is_not_in_near_future\": {\n",
    "        \"args\": [\"column\", \"offset\"],\n",
    "        \"optional\": [\"curr_timestamp\"]\n",
    "    },\n",
    "    \"is_older_than_n_days\": {\n",
    "        \"args\": [\"column\", \"days\"],\n",
    "        \"optional\": [\"curr_date\", \"negate\"]\n",
    "    },\n",
    "    \"is_older_than_col2_for_n_days\": {\n",
    "        \"args\": [\"column1\", \"column2\", \"days\"],\n",
    "        \"optional\": [\"negate\"]\n",
    "    },\n",
    "    \"regex_match\": {\n",
    "        \"args\": [\"column\", \"regex\"],\n",
    "        \"optional\": [\"negate\"]\n",
    "    },\n",
    "    \"is_valid_ipv4_address\": {\n",
    "        \"args\": [\"column\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"is_ipv4_address_in_cidr\": {\n",
    "        \"args\": [\"column\", \"cidr_block\"],\n",
    "        \"optional\": []\n",
    "    },\n",
    "    \"sql_expression\": {\n",
    "        \"args\": [\"expression\"],\n",
    "        \"optional\": [\"msg\", \"name\", \"negate\", \"columns\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf9db11-4f54-49ae-a2a5-3311dd31f742",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DQDatasetRule"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_LEVEL_FUNCTIONS = {\n",
    "    \"is_unique\": {\n",
    "        \"args\": [\"columns\"],\n",
    "        \"optional\": [\"nulls_distinct\"],\n",
    "    },\n",
    "    \"is_aggr_not_greater_than\": {\n",
    "        \"args\": [\"limit\"],\n",
    "        \"optional\": [\"column\", \"aggr_type\", \"group_by\"],\n",
    "    },\n",
    "    \"is_aggr_not_less_than\": {\n",
    "        \"args\": [\"limit\"],\n",
    "        \"optional\": [\"column\", \"aggr_type\", \"group_by\"],\n",
    "    },\n",
    "    \"is_aggr_equal\": {\n",
    "        \"args\": [\"limit\"],\n",
    "        \"optional\": [\"column\", \"aggr_type\", \"group_by\"],\n",
    "    },\n",
    "    \"is_aggr_not_equal\": {\n",
    "        \"args\": [\"limit\"],\n",
    "        \"optional\": [\"column\", \"aggr_type\", \"group_by\"],\n",
    "    },\n",
    "    \"foreign_key\": {\n",
    "        \"args\": [\"columns\", \"ref_columns\"],\n",
    "        \"optional\": [\"ref_df_name\", \"ref_table\", \"negate\"],\n",
    "    },\n",
    "    \"sql_query\": {\n",
    "        \"args\": [\"query\", \"merge_columns\", \"condition_column\"],\n",
    "        \"optional\": [\"input_placeholder\", \"msg\", \"name\", \"negate\"],\n",
    "    },\n",
    "    \"compare_datasets\": {\n",
    "        \"args\": [\"columns\", \"ref_columns\"],\n",
    "        \"optional\": [\"exclude_columns\", \"ref_df_name\", \"ref_table\", \"check_missing_records\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8daa1b4-b7af-4796-ab4e-f3ad6fd3d843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####  [Output Column Structure](https://databrickslabs.github.io/dqx/docs/guide/quality_checks/#quality-check-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a58b2d4-e63c-4c77-85ae-181de72b5795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Field**         | **Description**                                                                 |\n",
    "|-------------------|---------------------------------------------------------------------------------|\n",
    "| `name`            | Name of the check (string type).                                                |\n",
    "| `message`         | Message describing the quality issue (string type).                             |\n",
    "| `columns`         | Name of the column(s) where the quality issue was found (string type).          |\n",
    "| `filter`          | Filter applied if any (string type).                                            |\n",
    "| `function`        | Rule/check function applied (string type).                                      |\n",
    "| `run_time`        | Timestamp when the check was executed (timestamp type).                         |\n",
    "| `user_metadata`   | Optional key-value custom metadata provided by the user (dictionary type).      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "919c8560-81c6-4b6d-9138-2f7c18844e3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Result Columns Structure"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, TimestampType, MapType\n",
    "\n",
    "dq_result_item_schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), nullable=True),\n",
    "        StructField(\"message\", StringType(), nullable=True),\n",
    "        StructField(\"columns\", ArrayType(StringType()), nullable=True),\n",
    "        StructField(\"filter\", StringType(), nullable=True),\n",
    "        StructField(\"function\", StringType(), nullable=True),\n",
    "        StructField(\"run_time\", TimestampType(), nullable=True),\n",
    "        StructField(\"user_metadata\", MapType(StringType(), StringType()), nullable=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dq_result_schema = ArrayType(dq_result_item_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27703e96-1596-463d-8c2d-699dce749fe7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Result Stored Column"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"name\": \"col_city_is_null\",\n",
    "    \"message\": \"Column 'city' is null\",\n",
    "    \"columns\": [\"city\"],\n",
    "    \"filter\": \"country = 'Poland'\",\n",
    "    \"function\": \"is_not_null\",\n",
    "    \"run_time\": \"2025-01-01 14:31:21\",\n",
    "    \"user_metadata\": {\"key1\": \"value1\", \"key2\": \"value2\"},\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e9b611-d414-4d52-90f6-c85e81d3439c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7172daa7-99bc-4efd-a098-61af4cba6b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f205d0d3-2db5-4b33-8484-cc53649d5d5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rules Template"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Section 1 – Simple row‑level checks\n",
    "#\n",
    "# These rules operate on a single column at a time for each row.  Only the\n",
    "# `column` argument is mandatory.  Additional arguments apply only to certain\n",
    "# functions and are shown commented out with explanations.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error                     # 'error' => quarantine row on failure; 'warn' => flag but keep row\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_not_null                # any row-level built-in function\n",
    "    arguments:\n",
    "      column: col1                       # name or expression of a column in the table being checked\n",
    "      # allowed: [1, 2, 3]               # for is_in_list / is_not_null_and_is_in_list: values the column must belong to\n",
    "      # min_limit: 0                     # for is_in_range / is_not_in_range: lower bound (inclusive)\n",
    "      # max_limit: 10                    # for is_in_range / is_not_in_range: upper bound (inclusive)\n",
    "      # limit: 5                         # for is_not_less_than / is_not_greater_than: numeric/date/timestamp limit\n",
    "      # date_format: yyyy-MM-dd          # for is_valid_date: expected date pattern\n",
    "      # timestamp_format: yyyy-MM-dd HH:mm:ss   # for is_valid_timestamp: expected timestamp pattern\n",
    "      # offset: 86400                    # for is_not_in_future / is_not_in_near_future: offset in seconds\n",
    "      # curr_timestamp: '2025-08-05T00:00:00'   # override current timestamp; string or timestamp literal\n",
    "      # days: 7                          # is_older_than_n_days / is_older_than_col2_for_n_days\n",
    "      # column2: other_col               # required for is_older_than_col2_for_n_days\n",
    "      # regex: '[A-Z]{3}[0-9]{4}'        # regex_match\n",
    "      # negate: false                    # set true to invert the match\n",
    "      # cidr_block: '192.168.1.0/24'     # is_ipv4_address_in_cidr\n",
    "      # trim_strings: true               # is_not_null_and_not_empty\n",
    "\n",
    "################################################################################\n",
    "# Section 2 – Row‑level checks applied to multiple columns individually\n",
    "#\n",
    "# Use `for_each_column` when the same single‑column rule should be executed on\n",
    "# several columns.  Each entry must be a valid column name or expression from\n",
    "# the table being checked.  There is no hard limit on the number of columns.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_not_null\n",
    "    for_each_column:\n",
    "      - col1                             # first column name in the dataset\n",
    "      - col2                             # second column name in the dataset\n",
    "      # - col3                          # add additional column names as needed; each must exist in the dataset\n",
    "\n",
    "################################################################################\n",
    "# Section 3 – Row‑level checks on complex types (struct, map, array)\n",
    "#\n",
    "# These templates target elements within complex columns.  Use dot notation\n",
    "# (`struct_field`) for struct fields, and `try_element_at` for map or array\n",
    "# elements.  For array aggregations (e.g. max/min), include a numeric limit.\n",
    "################################################################################\n",
    "\n",
    "# Single element from a complex type\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_not_null\n",
    "    arguments:\n",
    "      column: col8.field1                # struct field; must refer to an existing field\n",
    "      # column: try_element_at(col7, 'key1')  # map element lookup; key must exist\n",
    "      # column: try_element_at(col4, 0)       # array element lookup by zero-based index\n",
    "      # Any optional parameters from Section 1 can be included here (e.g. regex)\n",
    "\n",
    "# Aggregating an array (e.g. ensure max element ≤ 10)\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_not_greater_than        # or is_not_less_than\n",
    "    arguments:\n",
    "      column: array_max(col4)            # aggregate function applied to an array column\n",
    "      limit: 10                          # required for array aggregation; numeric threshold for comparison\n",
    "\n",
    "################################################################################\n",
    "# Section 4 – Row‑level SQL expression\n",
    "#\n",
    "# For complex row‑level logic, use `sql_expression`.  This rule does not take a\n",
    "# `column` field; instead provide an SQL predicate.  Optional fields allow you to\n",
    "# customise messages, naming and inversion behaviour.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"col3 >= col2 AND col3 <= 10\"  # SQL predicate; fails when expression is True\n",
    "      # msg: \"col3 out of range\"         # optional message to include in result rows\n",
    "      # name: \"col3_range_check\"         # optional name for the resulting check column (appears in result DataFrame)\n",
    "      # negate: false                    # boolean: set true to invert the logic (fail when expression is False)\n",
    "      # columns: [col2, col3]            # optional list of one or more column names used only for reporting; does not affect logic\n",
    "\n",
    "################################################################################\n",
    "# Section 5 – Dataset‑level uniqueness check\n",
    "#\n",
    "# Ensures that the specified columns are unique across all rows.  A failure is\n",
    "# logged for every duplicate row found.  Use `nulls_distinct` to control how\n",
    "# NULL values are treated.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_unique\n",
    "    arguments:\n",
    "      columns:\n",
    "        - col1                            # key columns (one or more) used to test uniqueness\n",
    "        # - col2                         # add additional columns for composite key\n",
    "      # nulls_distinct: true              # default true: treat NULLs as distinct; set false to consider NULLs equal\n",
    "\n",
    "################################################################################\n",
    "# Section 6 – Dataset‑level aggregation checks\n",
    "#\n",
    "# Compare aggregated values against a threshold.  Specify `aggr_type` (count,\n",
    "# sum, avg, min, max).  For count, omit the column; otherwise provide the\n",
    "# column being aggregated.  `group_by` can list one or more columns to group\n",
    "# rows before aggregation.\n",
    "################################################################################\n",
    "\n",
    "# Count of all rows must not exceed 10\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_aggr_not_greater_than\n",
    "    arguments:\n",
    "      aggr_type: count                   # aggregation type (count, sum, avg, min, max)\n",
    "      limit: 10                          # numeric or literal threshold for comparison\n",
    "\n",
    "# Count of non-null values in col2 must not exceed 10\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_aggr_not_greater_than\n",
    "    arguments:\n",
    "      column: col2                       # column to aggregate (omit for count of all rows)\n",
    "      aggr_type: count\n",
    "      limit: 10\n",
    "\n",
    "# Count of col2 grouped by col3 must not exceed 10\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_aggr_not_greater_than\n",
    "    arguments:\n",
    "      column: col2\n",
    "      aggr_type: count\n",
    "      group_by:\n",
    "        - col3                          # optional grouping; one or more columns; must exist in the table\n",
    "      limit: 10\n",
    "\n",
    "################################################################################\n",
    "# Section 7 – Dataset‑level foreign key check\n",
    "#\n",
    "# Validates that values in the source dataset exist in a reference dataset.\n",
    "# Provide either a reference DataFrame name (`ref_df_name`) or a fully qualified\n",
    "# table name (`ref_table`), not both.  `negate` flips the logic to flag rows\n",
    "# that do exist in the reference.\n",
    "################################################################################\n",
    "\n",
    "# Single-column foreign key using a reference DataFrame\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: foreign_key\n",
    "    arguments:\n",
    "      columns:\n",
    "        - col1                           # column(s) in source dataset\n",
    "      ref_columns:\n",
    "        - ref_col1                       # matching column(s) in reference dataset\n",
    "      ref_df_name: ref_df_key            # key to locate the reference DataFrame in ref_dfs\n",
    "      # negate: false                    # optional; set true to fail rows that exist in the reference\n",
    "\n",
    "# Composite-key foreign key using a table\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: foreign_key\n",
    "    arguments:\n",
    "      columns:\n",
    "        - col1\n",
    "        - col2\n",
    "      ref_columns:\n",
    "        - ref_col1\n",
    "        - ref_col2\n",
    "      ref_table: catalog.schema.ref_table   # fully qualified table name used as the reference\n",
    "\n",
    "################################################################################\n",
    "# Section 8 – Dataset‑level SQL query\n",
    "#\n",
    "# Executes an arbitrary SQL query across the dataset (and optional reference data).\n",
    "# The query must return all `merge_columns` plus a boolean `condition_column`.\n",
    "# The check fails when the condition column is True (unless `negate` is true).\n",
    "# Use `input_placeholder` to name the input DataFrame within the SQL query.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: sql_query\n",
    "    arguments:\n",
    "      query: |\n",
    "        SELECT col1, col2, SUM(col3) = 0 AS condition\n",
    "        FROM {{ input_view }}                  # placeholder referencing the input dataset\n",
    "        GROUP BY col1, col2\n",
    "      merge_columns:                           # must exist in input DataFrame and query result\n",
    "        - col1\n",
    "        - col2\n",
    "      condition_column: condition              # boolean; True means failure\n",
    "      input_placeholder: input_view            # alias used inside double braces in the SQL\n",
    "      # msg: \"Aggregated col3 is zero\"       # optional message added to result rows\n",
    "      # name: \"check_sum_col3_zero\"         # optional name of the resulting check column\n",
    "      # negate: false                        # optional; set true to invert the meaning (fail when condition is False)\n",
    "\n",
    "################################################################################\n",
    "# Section 9 – Dataset‑level compare datasets\n",
    "#\n",
    "# Compares two datasets at row and column level.  Provide matching key columns\n",
    "# for both source and reference.  You can exclude columns from the comparison\n",
    "# and enable detection of missing records via full outer join.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: compare_datasets\n",
    "    arguments:\n",
    "      columns:\n",
    "        - col1                           # key columns in the source dataset\n",
    "        - col2\n",
    "      ref_columns:\n",
    "        - ref_col1                       # corresponding key columns in the reference dataset\n",
    "        - ref_col2\n",
    "      # ref_df_name: ref_df_key         # OR use ref_table: catalog.schema.ref_table\n",
    "      # exclude_columns:\n",
    "      #   - col7                        # columns to ignore differences for\n",
    "      # check_missing_records: true     # if true, perform FULL OUTER JOIN to detect missing rows; increases result size\n",
    "\n",
    "################################################################################\n",
    "# Section 10 – Dataset‑level multi‑key application (`for_each_column`)\n",
    "#\n",
    "# Use this construct for functions that accept a `columns` argument (e.g.\n",
    "# `is_unique`).  Each inner list defines a distinct key on which the check will\n",
    "# run independently.\n",
    "################################################################################\n",
    "\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  criticality: error\n",
    "  run_config_name: default               # added run configuration name\n",
    "  check:\n",
    "    function: is_unique\n",
    "    for_each_column:\n",
    "      - [col1]                           # first uniqueness key: single column in the dataset; run uniqueness check on col1 alone\n",
    "      - [col2, col3]                     # second key: composite of col2 and col3; run uniqueness check on (col2, col3)\n",
    "      # - [col4, col5, col6]             # additional keys; each inner list must contain existing column names\n",
    "\n",
    "################################################################################\n",
    "# End of template with placeholders\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9caf5412-839f-4169-bc54-4b568035ca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply check to multiple columns (simple col, struct, map and array)\n",
    "- table_name: catalog.schema.table       # added table name placeholder\n",
    "  name: rule_name\n",
    "  criticality: error\n",
    "  check:\n",
    "    function: is_not_null\n",
    "    for_each_column:\n",
    "    - col1 # col\n",
    "    - col8.field1 # struct col\n",
    "    - try_element_at(col7, 'key1') # map col\n",
    "    - try_element_at(col4, 1) # array col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c36f99-bb6c-46b7-82be-52b34dd0ad46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| employee_id | age |  salary | hire_date  |\n",
    "|------------:|----:|--------:|-----------|\n",
    "| 1           | 30 |  50000  | 2020-01-15 |\n",
    "| 2           | 45 |  80000  | 2018-05-30 |\n",
    "| 3           | 25 |  45000  | 2022-07-10 |\n",
    "| 4           | 40 |  70000  | 2019-09-20 |\n",
    "| 5           | 29 |      0  | 2021-03-12 |\n",
    "| 6           | 60 | 120000  | 2010-11-05 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "668c7e90-1e05-4c57-8397-67020ba39137",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rules Example"
    }
   },
   "outputs": [],
   "source": [
    "# Section 1 – Simple row-level check\n",
    "# Ensure salary is within a reasonable range [0, 200000].\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: salary_range\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_in_range\n",
    "    arguments:\n",
    "      column: salary\n",
    "      min_limit: 0\n",
    "      max_limit: 200000\n",
    "\n",
    "# Section 2 – Row-level check applied individually\n",
    "# Ensure both age and salary are at least 1.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: age_salary_min\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_not_less_than\n",
    "    for_each_column:\n",
    "      - age\n",
    "      - salary\n",
    "    arguments:\n",
    "      limit: 1\n",
    "\n",
    "# Section 3 – Complex types\n",
    "# (a) Validate that hire_date is a valid date (format yyyy-MM-dd).\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: valid_hire_date\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_valid_date\n",
    "    arguments:\n",
    "      column: hire_date\n",
    "      date_format: yyyy-MM-dd\n",
    "\n",
    "# (b) Ensure salary is all numeric digits.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: salary_numeric\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: regex_match\n",
    "    arguments:\n",
    "      column: salary\n",
    "      regex: '^[0-9]+$'\n",
    "      negate: false\n",
    "\n",
    "# (c) Ensure employee_id is present.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: employee_id_exists\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_not_null\n",
    "    arguments:\n",
    "      column: employee_id\n",
    "\n",
    "# (d) Ensure salary does not exceed 200000.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: max_salary_le_200k\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_not_greater_than\n",
    "    arguments:\n",
    "      column: salary\n",
    "      limit: 200000\n",
    "\n",
    "# Section 4 – Row-level SQL expression\n",
    "# Ensure salary is at least 1000 times age.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: salary_vs_age_rule\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"salary >= age * 1000\"\n",
    "      msg: \"Salary is unexpectedly low for age\"\n",
    "      name: \"salary_vs_age\"\n",
    "      columns: [salary, age]\n",
    "\n",
    "# Section 5 – Dataset-level uniqueness\n",
    "# Ensure employee_id is unique.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: unique_employee_id\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_unique\n",
    "    arguments:\n",
    "      columns: [employee_id]\n",
    "      nulls_distinct: false\n",
    "\n",
    "# Section 6 – Dataset-level aggregation\n",
    "# Ensure the average salary is at least 30000.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: avg_salary_min\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_aggr_not_less_than\n",
    "    arguments:\n",
    "      column: salary\n",
    "      aggr_type: avg\n",
    "      limit: 30000\n",
    "\n",
    "# Section 7 – Foreign key\n",
    "# Ensure employee_id exists in a master employee table.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: employee_fk_check\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: foreign_key\n",
    "    arguments:\n",
    "      columns: [employee_id]\n",
    "      ref_columns: [employee_id]\n",
    "      ref_table: catalog.hr.employees_master\n",
    "      negate: false\n",
    "\n",
    "# Section 8 – Dataset-level SQL query\n",
    "# Ensure there are at least two employees in each age group.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: age_group_count_check\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: sql_query\n",
    "    arguments:\n",
    "      query: |\n",
    "        SELECT age,\n",
    "               COUNT(*) < 2 AS condition      -- condition = True when there are < 2 employees in the group\n",
    "        FROM {{ input_view }}\n",
    "        GROUP BY age\n",
    "      merge_columns: [age]\n",
    "      condition_column: condition\n",
    "      input_placeholder: input_view\n",
    "      msg: \"Age group has fewer than two employees\"\n",
    "      name: \"min_two_employees_per_age\"\n",
    "\n",
    "# Section 9 – Compare datasets\n",
    "# Compare current employees against last month's snapshot by employee_id, ignoring salary.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: compare_employees\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: compare_datasets\n",
    "    arguments:\n",
    "      columns: [employee_id]\n",
    "      ref_columns: [employee_id]\n",
    "      ref_df_name: last_month                 # Reference DataFrame name\n",
    "      exclude_columns: [salary]               # Ignore salary differences\n",
    "      check_missing_records: true             # Identify missing or new employee_ids\n",
    "\n",
    "# Section 10 – Multi-key uniqueness\n",
    "# Ensure both employee_id and (age, salary) keys are unique.\n",
    "- table_name: dq_dev.company.employees\n",
    "  name: multi_key_uniqueness\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_unique\n",
    "    for_each_column:\n",
    "      - [employee_id]                          # First key\n",
    "      - [age, salary]                          # Second key (composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed521c8-bd1c-443d-8c29-d0bd038599c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CLA Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5df83d4-3bd6-4381-a297-e32fd14dd648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### wkdy_dim_project.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b6febd0-82cf-4a4f-83fa-7ba438b96732",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Original - wkdy_dim_project.yaml"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset level\n",
    "- check:\n",
    "  arguments:\n",
    "    columns: project_key\n",
    "  function: is_unique\n",
    "  criticality: error\n",
    "name: project_key_is_not_unique\n",
    "#row level\n",
    "- check:\n",
    "    arguments:\n",
    "      column: project_key\n",
    "    function: is_not_null\n",
    "  criticality: error\n",
    "  name: project_key_is_null\n",
    "  \n",
    "- check:\n",
    "    arguments:\n",
    "      column: project_status\n",
    "    function: is_not_null\n",
    "  criticality: error\n",
    "  name: project_status_is_null\n",
    "  \n",
    "  - check:\n",
    "    arguments:\n",
    "      allowed:\n",
    "        - Active\n",
    "        - Closed\n",
    "        - Pending Close\n",
    "        - Schedule Pending\n",
    "        - Suspended\n",
    "      column: project_status\n",
    "    function: is_in_list\n",
    "  criticality: warning\n",
    "  name: project_status_is_new_value\n",
    "  \n",
    " # project type is not null or Unknown\n",
    " - check:\n",
    "    arguments:\n",
    "      expression: project_type_name is not null AND project_type_name != 'Unknown'\n",
    "    function: sql_expression\n",
    "  criticality: warning\n",
    "  name: project_type_is_not_null_or_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b344615f-8a6a-4a6e-be1c-208a130b16d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reformatted - wkdy_dim_project.yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Enforce uniqueness of project_key across the table (dataset-level)\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: project_key_is_not_unique\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_unique\n",
    "    arguments:\n",
    "      columns: [project_key]\n",
    "\n",
    "# project_key must not be null (row-level)\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: project_key_is_null\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_not_null\n",
    "    arguments:\n",
    "      column: project_key\n",
    "\n",
    "# project_status must not be null (row-level)\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: project_status_is_null\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_not_null\n",
    "    arguments:\n",
    "      column: project_status\n",
    "\n",
    "# project_status must be from allowed set (row-level)\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: project_status_is_new_value\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_in_list\n",
    "    arguments:\n",
    "      column: project_status\n",
    "      allowed:\n",
    "        - Active\n",
    "        - Closed\n",
    "        - Pending Close\n",
    "        - Schedule Pending\n",
    "        - Suspended\n",
    "\n",
    "# project_type_name must not be null or 'Unknown'\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: project_type_is_not_null_or_unknown\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"project_type_name is not null AND project_type_name != 'Unknown'\"\n",
    "\n",
    "# email should match pattern (basic email format)\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: email_is_not_valid\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: regex_match\n",
    "    arguments:\n",
    "      column: email\n",
    "      regex: \"^(.+)@(.+)$\"\n",
    "\n",
    "# project_start_date must be <= project_end_date\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: project_start_after_end_date\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"coalesce(project_start_date, '1900-01-01') <= coalesce(project_end_date, '9999-12-31')\"\n",
    "\n",
    "# first_activity_date must be <= last_activity_date\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: first_activity_date_after_last_activity_date\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"coalesce(first_activity_date, '1900-01-01') <= coalesce(last_activity_date, '9999-12-31')\"\n",
    "\n",
    "# _created_date must be <= _last_updated_date\n",
    "- table_name: de_prd.gold.wkdy_dim_project\n",
    "  name: created_date_after_last_updated_date\n",
    "  criticality: warn\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"coalesce(_created_date, '1900-01-01') <= coalesce(_last_updated_date, '9999-12-31')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9d8644b-759a-42d7-99a4-905aa5a39f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e322881c-2869-42bf-9731-c0c6408d79ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DQX Package Installation"
    }
   },
   "outputs": [],
   "source": [
    "# === Cell 1: Install DQX Package (if your cluster image doesn't already include it) ===\n",
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c16683-2835-4554-88ca-8ea476227f63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Libraries"
    }
   },
   "outputs": [],
   "source": [
    "# === Cell 2: Restart Python to pick up libs (Databricks convention) ===\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19594b5c-1ef7-4503-a471-07964cd1eb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load DQX Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d8087e-80ad-4dfe-8957-f2c2bbf02c47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Checks"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import yaml\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "\n",
    "\n",
    "# --- Unified schema, with DQX columns sandwiched ---\n",
    "# DQX core columns must match the docs:\n",
    "# name (STRING), criticality (STRING), check (STRUCT), filter (STRING),\n",
    "# run_config_name (STRING), user_metadata (MAP<STRING,STRING>)\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"hash_id\", T.StringType(), False),\n",
    "    T.StructField(\"table_name\", T.StringType(), False),\n",
    "\n",
    "    # DQX fields begin here\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"criticality\", T.StringType(), False),\n",
    "    T.StructField(\n",
    "        \"check\",\n",
    "        T.StructType([\n",
    "            T.StructField(\"function\", T.StringType(), False),\n",
    "            T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "            T.StructField(\"arguments\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "        ]),\n",
    "        False,\n",
    "    ),\n",
    "    T.StructField(\"filter\", T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\", T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # your ops fields\n",
    "    T.StructField(\"yaml_path\", T.StringType(), False),\n",
    "    T.StructField(\"active\", T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\", T.StringType(), False),\n",
    "    T.StructField(\"created_at\", T.StringType(), False),  # stored as ISO string; we may cast on write\n",
    "    T.StructField(\"updated_by\", T.StringType(), True),\n",
    "    T.StructField(\"updated_at\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "def compute_hash(rule_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"Stable hash over the identifying fields of a rule.\"\"\"\n",
    "    relevant = {\n",
    "        k: rule_dict[k]\n",
    "        for k in [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "        if k in rule_dict\n",
    "    }\n",
    "    return hashlib.md5(json.dumps(relevant, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert a dict of arbitrary JSON-serializable values to map<string,string>\n",
    "    required by DQX (lists/dicts -> JSON, bool -> 'true'/'false', else str()).\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_yaml_file(path: str, output_config: Dict[str, Any], time_zone: str = \"UTC\"):\n",
    "    \"\"\"Read one YAML file, validate, and flatten into rows for the table.\"\"\"\n",
    "    file_base = os.path.splitext(os.path.basename(path))[0]\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = yaml.safe_load(fh)\n",
    "    if isinstance(docs, dict):\n",
    "        docs = [docs]\n",
    "\n",
    "    validate_rules_file(docs, file_base, path)\n",
    "\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat_rules = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path)\n",
    "\n",
    "        h = compute_hash(rule)\n",
    "        check_dict = rule[\"check\"]\n",
    "\n",
    "        # Strong typing for DQX struct:\n",
    "        function = check_dict.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\")\n",
    "\n",
    "        for_each = check_dict.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\")\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\")\n",
    "\n",
    "        arguments = check_dict.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)  # enforce map<string,string>\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map<string,string> (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        flat_rules.append({\n",
    "            \"hash_id\": h,\n",
    "            \"table_name\": rule[\"table_name\"],\n",
    "\n",
    "            \"name\": rule[\"name\"],\n",
    "            \"criticality\": rule[\"criticality\"],\n",
    "            \"check\": {\n",
    "                \"function\": function,\n",
    "                \"for_each_column\": for_each if for_each else None,\n",
    "                \"arguments\": arguments if arguments else None,\n",
    "            },\n",
    "            \"filter\": rule.get(\"filter\"),\n",
    "            \"run_config_name\": rule[\"run_config_name\"],\n",
    "            \"user_metadata\": user_metadata if user_metadata else None,\n",
    "\n",
    "            \"yaml_path\": path,\n",
    "            \"active\": rule.get(\"active\", True),\n",
    "            \"created_by\": \"AdminUser\",\n",
    "            \"created_at\": now,\n",
    "            \"updated_by\": None,\n",
    "            \"updated_at\": None,\n",
    "        })\n",
    "\n",
    "    # Validate with DQX engine (semantic)\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat_rules\n",
    "\n",
    "\n",
    "def parse_output_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, \"r\") as fh:\n",
    "        config = yaml.safe_load(fh)\n",
    "    # Expect the new keys\n",
    "    required = [\"dqx_checks_config_table_name\", \"dqx_yaml_checks\", \"run_config_name\"]\n",
    "    missing = [k for k in required if k not in config]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Config missing required keys: {missing}\")\n",
    "    return config\n",
    "\n",
    "\n",
    "def validate_rules_file(rules, file_base: str, file_path: str):\n",
    "    problems = []\n",
    "    seen_names = set()\n",
    "    table_names = {r.get(\"table_name\") for r in rules if isinstance(r, dict)}\n",
    "    if len(table_names) != 1:\n",
    "        problems.append(f\"Inconsistent table_name values in {file_path}: {table_names}\")\n",
    "    expected_table = file_base\n",
    "    try:\n",
    "        tn = list(table_names)[0]\n",
    "        if tn.split(\".\")[-1] != expected_table:\n",
    "            problems.append(\n",
    "                f\"Table name in rules ({tn}) does not match filename ({expected_table}) in {file_path}\"\n",
    "            )\n",
    "    except Exception:\n",
    "        problems.append(f\"No valid table_name found in {file_path}\")\n",
    "\n",
    "    for rule in rules:\n",
    "        name = rule.get(\"name\")\n",
    "        if not name:\n",
    "            problems.append(f\"Missing rule name in {file_path}\")\n",
    "        if name in seen_names:\n",
    "            problems.append(f\"Duplicate rule name '{name}' in {file_path}\")\n",
    "        seen_names.add(name)\n",
    "\n",
    "    if problems:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {problems}\")\n",
    "\n",
    "\n",
    "def validate_rule_fields(rule, file_path: str):\n",
    "    problems = []\n",
    "    required_fields = [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "    for field in required_fields:\n",
    "        if not rule.get(field):\n",
    "            problems.append(\n",
    "                f\"Missing required field '{field}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "            )\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        problems.append(\n",
    "            f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if rule.get(\"criticality\") not in {\"error\", \"warn\", \"warning\"}:\n",
    "        problems.append(\n",
    "            f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        problems.append(\n",
    "            f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if problems:\n",
    "        raise ValueError(f\"Rule-level validation failed: {problems}\")\n",
    "\n",
    "\n",
    "def validate_with_dqx(rules, file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if status.has_errors:\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "\n",
    "def ensure_delta_table(spark: SparkSession, delta_table_name: str):\n",
    "    if not spark.catalog.tableExists(delta_table_name):\n",
    "        print(f\"Creating new Delta table at {delta_table_name}\")\n",
    "        empty_df = spark.createDataFrame([], TABLE_SCHEMA)\n",
    "        empty_df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "    else:\n",
    "        print(f\"Delta table already exists at {delta_table_name}\")\n",
    "\n",
    "\n",
    "def upsert_rules_into_delta(spark: SparkSession, rules, delta_table_name: str):\n",
    "    if not rules:\n",
    "        print(\"No rules to write, skipping upsert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nWriting rules to Delta table '{delta_table_name}'...\")\n",
    "    print(f\"Number of rules to write: {len(rules)}\")\n",
    "\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "\n",
    "    # Cast audit timestamps to actual TIMESTAMP in the sink\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            \"target.yaml_path = source.yaml_path AND target.table_name = source.table_name AND target.name = source.name\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    except Exception:\n",
    "        print(\"Delta merge failed (likely first write). Writing full table.\")\n",
    "        df.write.format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "    print(f\"Successfully wrote {df.count()} rules to '{delta_table_name}'.\")\n",
    "\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules):\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return\n",
    "    df = spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    print(\"\\n==== Dry Run: Rules DataFrame to be uploaded ====\")\n",
    "    df.show(truncate=False, n=50)\n",
    "    print(f\"Total rules: {df.count()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_all_rules(rules_dir: str, output_config: Dict[str, Any], fail_fast: bool = True):\n",
    "    errors = []\n",
    "    print(f\"Starting validation for all YAML rule files in '{rules_dir}'\")\n",
    "    for fname in os.listdir(rules_dir):\n",
    "        if not fname.endswith((\".yaml\", \".yml\")):\n",
    "            continue\n",
    "        full_path = os.path.join(rules_dir, fname)\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            file_base = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            with open(full_path, \"r\") as fh:\n",
    "                docs = yaml.safe_load(fh)\n",
    "            if isinstance(docs, dict):\n",
    "                docs = [docs]\n",
    "            validate_rules_file(docs, file_base, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def main(\n",
    "    output_config_path: str = \"resources/dqx_config.yaml\",\n",
    "    rules_dir: Optional[str] = None,\n",
    "    time_zone: str = \"America/Chicago\",\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False\n",
    "):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    print_notebook_env(spark, local_timezone=time_zone)\n",
    "\n",
    "    output_config = parse_output_config(output_config_path)\n",
    "\n",
    "    # pick up rules_dir from config if not provided explicitly\n",
    "    rules_dir = rules_dir or output_config[\"dqx_yaml_checks\"]\n",
    "\n",
    "    delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "\n",
    "    all_rules = []\n",
    "    for fname in os.listdir(rules_dir):\n",
    "        if not fname.endswith((\".yaml\", \".yml\")):\n",
    "            continue\n",
    "        full_path = os.path.join(rules_dir, fname)\n",
    "        file_rules = process_yaml_file(full_path, output_config, time_zone=time_zone)\n",
    "        all_rules.extend(file_rules)\n",
    "\n",
    "    if validate_only:\n",
    "        print(\"\\nValidation only: not writing any rules.\")\n",
    "        validate_all_rules(rules_dir, output_config)\n",
    "        return\n",
    "\n",
    "    if dry_run:\n",
    "        print_rules_df(spark, all_rules)\n",
    "        return\n",
    "\n",
    "    ensure_delta_table(spark, delta_table_name)\n",
    "    upsert_rules_into_delta(spark, all_rules, delta_table_name)\n",
    "    print(f\"Finished writing rules to '{delta_table_name}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main(dry_run=True)\n",
    "    # main(validate_only=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "920ab055-2694-42ae-ba16-1c5863eb41e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 02_run_dqx_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21f5dc06-9bff-4f53-a9b6-051c74d2e14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dqx/info/two_rules_test.yaml\n",
    "\n",
    "# Row-level: Filter rows where 'result_state'='FAILED' & Annotate where 'state'='TERMINATED'\n",
    "- table_name: dq_prd.monitoring.job_run_audit\n",
    "  name: failed_state_is_terminated\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  filter: \"result_state = 'FAILED'\"\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"state = 'TERMINATED'\"\n",
    "\n",
    "# Dataset-level: composite uniqueness (one run per job+start_time)\n",
    "- table_name: dq_prd.monitoring.job_run_audit\n",
    "  name: unique_job_id_start_time\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  check:\n",
    "    function: is_unique\n",
    "    arguments:\n",
    "      columns: [job_id, start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a1f1a0-2daf-4867-9b55-d4d525ca2d0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load DQX Checks - External Yaml File"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from collections import defaultdict\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "yaml_path = \"two_rules_test.yaml\"\n",
    "\n",
    "# 1) Load YAML\n",
    "with open(yaml_path, \"r\") as fh:\n",
    "    rules = yaml.safe_load(fh) or []\n",
    "if isinstance(rules, dict):\n",
    "    rules = [rules]\n",
    "\n",
    "# 2) Group by table\n",
    "by_table = defaultdict(list)\n",
    "for r in rules:\n",
    "    t = r.get(\"table_name\")\n",
    "    if t:\n",
    "        by_table[t].append(r)\n",
    "\n",
    "print(f\"Loaded {sum(len(v) for v in by_table.values())} rule(s) across {len(by_table)} table(s): {list(by_table.keys())}\")\n",
    "\n",
    "engine = DQEngine(WorkspaceClient())\n",
    "\n",
    "# 3) Apply per table\n",
    "for table, tbl_rules in by_table.items():\n",
    "    print(f\"\\n=== Applying {len(tbl_rules)} rule(s) to {table} ===\")\n",
    "    df = spark.table(table)\n",
    "\n",
    "    annotated = engine.apply_checks_by_metadata(df, tbl_rules)\n",
    "\n",
    "    # DQX may use _error/_warning or _errors/_warnings depending on version\n",
    "    err_col = \"_errors\"  if \"_errors\"  in annotated.columns else \"_error\"\n",
    "    wrn_col = \"_warnings\" if \"_warnings\" in annotated.columns else \"_warning\"\n",
    "\n",
    "    # Summary\n",
    "    agg = (\n",
    "        annotated.select(\n",
    "            F.size(F.col(err_col)).alias(\"e_sz\"),\n",
    "            F.size(F.col(wrn_col)).alias(\"w_sz\"),\n",
    "        )\n",
    "        .agg(\n",
    "            F.coalesce(F.sum(\"e_sz\"), F.lit(0)).alias(\"errors\"),\n",
    "            F.coalesce(F.sum(\"w_sz\"), F.lit(0)).alias(\"warnings\"),\n",
    "        )\n",
    "        .collect()[0]\n",
    "    )\n",
    "    print(f\"Hit summary -> errors={int(agg['errors'])}, warnings={int(agg['warnings'])}\")\n",
    "\n",
    "    # Keep only rows that actually have hits\n",
    "    hits_only = annotated.filter(\n",
    "        (F.col(err_col).isNotNull() & (F.size(F.col(err_col)) > 0)) |\n",
    "        (F.col(wrn_col).isNotNull() & (F.size(F.col(wrn_col)) > 0))\n",
    "    )\n",
    "\n",
    "    # Explode per array (no _outer), then union; drop all-null just in case\n",
    "    errors = (\n",
    "        hits_only.filter(F.size(F.col(err_col)) > 0)\n",
    "        .select(F.explode(F.col(err_col)).alias(\"r\"))\n",
    "        .select(\"r.*\")\n",
    "    )\n",
    "    warnings = (\n",
    "        hits_only.filter(F.size(F.col(wrn_col)) > 0)\n",
    "        .select(F.explode(F.col(wrn_col)).alias(\"r\"))\n",
    "        .select(\"r.*\")\n",
    "    )\n",
    "    hits = errors.unionByName(warnings, allowMissingColumns=True).na.drop(\"all\")\n",
    "\n",
    "    print(\"\\nSummary by rule:\")\n",
    "    hits.groupBy(\"name\", \"function\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "    print(\"\\nViolations (one row per hit):\")\n",
    "    try:\n",
    "        display(hits.select(\"name\", \"message\", \"columns\", \"filter\", \"function\", \"run_time\", \"user_metadata\"))\n",
    "    except NameError:\n",
    "        hits.select(\"name\", \"message\", \"columns\", \"filter\", \"function\", \"run_time\", \"user_metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b819eae4-2369-43fd-aa1a-75d2e3b2cb90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load DQX Checks - Code Defined Yaml File"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from collections import defaultdict\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import TableChecksStorageConfig\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ============\n",
    "# Define checks inline (YAML string → Python list[dict])\n",
    "# Update table_name if needed\n",
    "# ============\n",
    "checks = yaml.safe_load(\"\"\"\n",
    "- table_name: dq_prd.monitoring.job_run_audit\n",
    "  name: failed_state_is_terminated\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  filter: \"result_state = 'FAILED'\"\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"state = 'TERMINATED'\"\n",
    "\n",
    "- table_name: dq_prd.monitoring.job_run_audit\n",
    "  name: failed_runs_have_error_code\n",
    "  criticality: error\n",
    "  run_config_name: default\n",
    "  filter: \"result_state = 'FAILED'\"\n",
    "  check:\n",
    "    function: sql_expression\n",
    "    arguments:\n",
    "      expression: \"error_code IS NOT NULL AND trim(error_code) <> ''\"\n",
    "\"\"\")\n",
    "\n",
    "# ============\n",
    "# Validate with DQX (will catch wrong types/args early)\n",
    "# ============\n",
    "status = DQEngine.validate_checks(checks)\n",
    "if getattr(status, \"has_errors\", False):\n",
    "    print(\"Validation errors:\\n\" + status.to_string())\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ============\n",
    "# Group by table and run\n",
    "# ============\n",
    "by_table = defaultdict(list)\n",
    "for r in checks:\n",
    "    by_table[r[\"table_name\"]].append(r)\n",
    "\n",
    "engine = DQEngine(WorkspaceClient())\n",
    "\n",
    "for table, tbl_rules in by_table.items():\n",
    "    print(f\"\\n=== Applying {len(tbl_rules)} rule(s) to {table} ===\")\n",
    "    df = spark.table(table)\n",
    "\n",
    "    annotated = engine.apply_checks_by_metadata(df, tbl_rules)\n",
    "\n",
    "    # DQX may expose _error/_warning or _errors/_warnings depending on version\n",
    "    err_col = \"_errors\" if \"_errors\" in annotated.columns else \"_error\"\n",
    "    wrn_col = \"_warnings\" if \"_warnings\" in annotated.columns else \"_warning\"\n",
    "\n",
    "    # Quick summary\n",
    "    agg = (\n",
    "        annotated.select(\n",
    "            F.size(F.col(err_col)).cast(\"long\").alias(\"e_sz\"),\n",
    "            F.size(F.col(wrn_col)).cast(\"long\").alias(\"w_sz\"),\n",
    "        )\n",
    "        .agg(\n",
    "            F.coalesce(F.sum(\"e_sz\"), F.lit(0).cast(\"long\")).alias(\"errors\"),\n",
    "            F.coalesce(F.sum(\"w_sz\"), F.lit(0).cast(\"long\")).alias(\"warnings\"),\n",
    "        )\n",
    "        .collect()[0]\n",
    "    )\n",
    "    print(f\"Hit summary -> errors={int(agg['errors'])}, warnings={int(agg['warnings'])}\")\n",
    "\n",
    "    # Show per-rule counts\n",
    "    hits_only = annotated.filter(\n",
    "        (F.col(err_col).isNotNull() & (F.size(F.col(err_col)) > 0)) |\n",
    "        (F.col(wrn_col).isNotNull() & (F.size(F.col(wrn_col)) > 0))\n",
    "    )\n",
    "    errors = (\n",
    "        hits_only.filter(F.size(F.col(err_col)) > 0)\n",
    "        .select(F.explode(F.col(err_col)).alias(\"r\"))\n",
    "        .select(\"r.*\")\n",
    "    )\n",
    "    warnings = (\n",
    "        hits_only.filter(F.size(F.col(wrn_col)) > 0)\n",
    "        .select(F.explode(F.col(wrn_col)).alias(\"r\"))\n",
    "        .select(\"r.*\")\n",
    "    )\n",
    "    hits = errors.unionByName(warnings, allowMissingColumns=True)\n",
    "\n",
    "    print(\"\\nSummary by rule:\")\n",
    "    hits.groupBy(\"name\", \"function\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "    print(\"\\nSample violations:\")\n",
    "    hits.select(\"name\", \"message\", \"columns\", \"filter\", \"function\", \"run_time\", \"user_metadata\").show(20, truncate=False)\n",
    "\n",
    "# ============\n",
    "# OPTIONAL: save these inline checks into your config table for future runs\n",
    "# (uncomment to persist)\n",
    "# ============\n",
    "# engine.save_checks(\n",
    "#     checks,\n",
    "#     config=TableChecksStorageConfig(\n",
    "#         location=\"dq_dev.dqx.checks_config\",   # <-- set to your checks Delta table FQN\n",
    "#         run_config_name=\"default\",\n",
    "#         mode=\"append\"                          # or \"overwrite\"\n",
    "#     )\n",
    "# )\n",
    "# print(\"Saved checks to dq_dev.dqx.checks_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361d744b-bb29-41eb-be17-adfb89dfc317",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DQX Defined in Class"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.rule import DQRowRule\n",
    "from databricks.labs.dqx import check_funcs\n",
    "\n",
    "# Target table → DataFrame\n",
    "df = spark.table(\"dq_prd.monitoring.job_run_audit\")\n",
    "\n",
    "# Define checks as DQX classes\n",
    "checks = [\n",
    "    DQRowRule(\n",
    "        name=\"run_id_is_not_null\",\n",
    "        criticality=\"error\",\n",
    "        check_func=check_funcs.is_not_null,\n",
    "        column=\"run_id\",\n",
    "    ),\n",
    "    DQRowRule(\n",
    "        name=\"failed_state_is_terminated\",\n",
    "        criticality=\"error\",\n",
    "        filter=\"result_state = 'FAILED'\",\n",
    "        check_func=check_funcs.sql_expression,\n",
    "        check_func_kwargs={\n",
    "            \"expression\": \"state = 'TERMINATED'\",  # assert condition; fails when not met\n",
    "            \"msg\": \"FAILED run not in TERMINATED state\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Apply checks → adds `_warning` / `_error` columns at the end\n",
    "dq = DQEngine(WorkspaceClient())\n",
    "df_checked = dq.apply_checks(df, checks)\n",
    "\n",
    "# Show the result; rows with issues will have non-empty arrays in `_error` / `_warning`\n",
    "display(df_checked)\n",
    "# If you only want affected rows during testing:\n",
    "# display(df_checked.where((F.size(\"_error\") > 0) | (F.size(\"_warning\") > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2846d252-350c-4519-92c9-38ee149177ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run DQX & Fingerprint Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec10797-0af7-4550-a334-80ea51fee8b1",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755016728702}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Yaml Defined Rules - 'errors' & 'warning' columns"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import FileChecksStorageConfig\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Target table → DF\n",
    "df = spark.table(\"de_prd.gold.wkdy_dim_project\")\n",
    "\n",
    "# 2) Load checks from YAML (adjust path if your cwd differs)\n",
    "dq = DQEngine(WorkspaceClient())\n",
    "checks = dq.load_checks(\n",
    "    FileChecksStorageConfig(location=\"wkdy_dim_project.yaml\")\n",
    ")\n",
    "\n",
    "# 3) Apply checks → appends `_warning` / `_error` array<struct> columns\n",
    "df_checked = dq.apply_checks_by_metadata(df, checks)\n",
    "\n",
    "# Show only affected rows (keep full table shape, just filtered)\n",
    "#display(df_checked.where((F.size(\"_error\") > 0) | (F.size(\"_warning\") > 0)))\n",
    "\n",
    "# If you want everything, not filtered:\n",
    "display(df_checked)\n",
    "\n",
    "# (Optional) Need a flat “result dataframe” of issues matching your schema?\n",
    "# errs = (df_checked\n",
    "#   .select(F.explode(F.array_union(\"_error\",\"_warning\")).alias(\"r\"))\n",
    "#   .select(\"r.name\",\"r.message\",\"r.columns\",\"r.filter\",\"r.function\",\"r.run_time\",\"r.user_metadata\"))\n",
    "# display(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61da59b-51f1-495a-ac10-de6574ebb735",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755016798367}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Collect Only Annotated Records"
    }
   },
   "outputs": [],
   "source": [
    "# keep rows where _errors exists and has items\n",
    "df_filtered = df_checked.where(F.col(\"_errors\").isNotNull() & (F.size(\"_errors\") > 0))\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba618ab4-3d22-414e-a510-bf81784c92bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Error Rows Count"
    }
   },
   "outputs": [],
   "source": [
    "df_COUNT = df_filtered.count()\n",
    "print(f\"There are {df_COUNT:,} rows with errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddcb110f-2c9a-4085-b212-1e9b112207f7",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"rule\":238},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755028029098}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Error and Warning Counts Analysis"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Pick the actual column names DQX created\n",
    "err_col  = \"_errors\"  if \"_errors\"  in df_checked.columns else \"_error\"\n",
    "warn_col = \"_warnings\" if \"_warnings\" in df_checked.columns else \"_warning\"\n",
    "\n",
    "# Rows with at least one error\n",
    "df_filtered = df_checked.where(F.size(F.col(err_col)) > 0)\n",
    "\n",
    "# 1) Total rows that have any error\n",
    "rows_with_errors = df_filtered.count()\n",
    "print(f\"Rows with any error: {rows_with_errors:,}\")\n",
    "\n",
    "# 2) Count violations per error rule\n",
    "error_counts = (\n",
    "    df_filtered\n",
    "      .select(F.explode(F.col(err_col)).alias(\"e\"))\n",
    "      .groupBy(F.col(\"e.name\").alias(\"rule\"))\n",
    "      .count()\n",
    "      .withColumnRenamed(\"count\", \"violations\")\n",
    "      .orderBy(F.desc(\"violations\"))\n",
    ")\n",
    "display(error_counts)\n",
    "\n",
    "# 3) (Optional) warnings breakdown too\n",
    "if warn_col in df_checked.columns:\n",
    "    warning_counts = (\n",
    "        df_checked.where(F.size(F.col(warn_col)) > 0)\n",
    "          .select(F.explode(F.col(warn_col)).alias(\"w\"))\n",
    "          .groupBy(F.col(\"w.name\").alias(\"rule\"))\n",
    "          .count()\n",
    "          .withColumnRenamed(\"count\", \"violations\")\n",
    "          .orderBy(F.desc(\"violations\"))\n",
    "    )\n",
    "    display(warning_counts)\n",
    "\n",
    "# 4) (Optional) total violation events (sum across all rules; a single row can contribute >1)\n",
    "total_error_events = (\n",
    "    df_filtered\n",
    "      .select(F.explode(F.col(err_col)).alias(\"e\"))\n",
    "      .count()\n",
    ")\n",
    "print(f\"Total error events (all rules): {total_error_events:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b98dc96-c02e-42ed-bc63-d4be5f136d6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Equivalent of Above Results - *They Should Match*"
    }
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "-- check_name: project_type_is_not_null_or_unknown\n",
    "SELECT COUNT(*) AS violation_count\n",
    "FROM de_prd.gold.wkdy_dim_project\n",
    "WHERE project_type_name IS NULL\n",
    "   OR trim(project_type_name) = ''\n",
    "   OR lower(trim(project_type_name)) = 'unknown';\n",
    "\n",
    "##############################################\n",
    "-- check_name: project_status_is_new_value\n",
    "SELECT COUNT(*) AS violation_count\n",
    "FROM de_prd.gold.wkdy_dim_project\n",
    "WHERE project_status IS NOT NULL\n",
    "  AND lower(trim(project_status)) NOT IN (\n",
    "    'active','closed','pending close','schedule pending','suspended'\n",
    "  );\n",
    "\n",
    "##############################################\n",
    "-- check_name: project_start_after_end_date\n",
    "SELECT COUNT(*) AS violation_count\n",
    "FROM de_prd.gold.wkdy_dim_project\n",
    "WHERE coalesce(project_start_date, date '1900-01-01')\n",
    "    > coalesce(project_end_date,   date '9999-12-31');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95a06b3-1bd0-4a1e-a3f5-4abf8ffa722e",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_errors\":224,\"row_kv\":187,\"row_kv_fp\":220},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755028186953}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Generate Stable Error Fingerprints"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# pick singular vs plural, depending on your DQX version\n",
    "errors_col = \"_errors\" if \"_errors\" in df_filtered.columns else \"_error\"\n",
    "\n",
    "# array<struct<column:string,value:string>> for all non-system cols\n",
    "base_cols = sorted([c for c in df_filtered.columns if not c.startswith(\"_\")])\n",
    "row_kv = F.array(*[F.struct(F.lit(c).alias(\"column\"), F.col(c).cast(\"string\").alias(\"value\")) for c in base_cols])\n",
    "\n",
    "# stable fingerprints (JSON of ordered arrays → sha2)\n",
    "row_kv_fp = F.sha2(F.to_json(row_kv), 256)\n",
    "\n",
    "# normalize errors for stable hashing (exclude volatile fields like run_time/user_metadata)\n",
    "err_norm = F.transform(\n",
    "    F.col(errors_col),\n",
    "    lambda r: F.struct(\n",
    "        r[\"name\"].alias(\"name\"),\n",
    "        r[\"message\"].alias(\"message\"),\n",
    "        F.array_sort(r[\"columns\"]).alias(\"columns\"),\n",
    "        r[\"filter\"].alias(\"filter\"),\n",
    "        r[\"function\"].alias(\"function\"),\n",
    "    ),\n",
    ")\n",
    "errors_fp = F.sha2(F.to_json(F.array_sort(err_norm)), 256)\n",
    "\n",
    "df_out = df_filtered.select(\n",
    "    F.col(errors_col).alias(\"_errors\"),\n",
    "    row_kv.alias(\"row_kv\"),\n",
    "    row_kv_fp.alias(\"row_kv_fp\"),\n",
    "    errors_fp.alias(\"errors_fp\"),\n",
    ")\n",
    "\n",
    "display(df_out)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DQX_POC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
