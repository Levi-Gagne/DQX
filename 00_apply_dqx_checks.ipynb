{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apply DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f57c20-e8da-4036-99b3-0339862aeda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply_dqx_checks.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, List, Optional\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import TableChecksStorageConfig\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F, types as T\n",
    "from utils.color import Color\n",
    "\n",
    "# =========================\n",
    "# Schema for DQX checks log\n",
    "# =========================\n",
    "DQX_CHECKS_LOG_SCHEMA = T.StructType([\n",
    "    T.StructField(\"result_id\",       T.StringType(),  False),\n",
    "    T.StructField(\"rule_id\",         T.StringType(),  False),\n",
    "    T.StructField(\"source_table\",    T.StringType(),  False),\n",
    "    T.StructField(\"run_config_name\", T.StringType(),  False),\n",
    "    T.StructField(\"severity\",        T.StringType(),  False),\n",
    "    T.StructField(\"name\",            T.StringType(),  True),\n",
    "    T.StructField(\"message\",         T.StringType(),  True),\n",
    "    T.StructField(\"columns\",         T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"filter\",          T.StringType(),  True),\n",
    "    T.StructField(\"function\",        T.StringType(),  True),\n",
    "    T.StructField(\"run_time\",        T.TimestampType(), True),\n",
    "    T.StructField(\"user_metadata\",   T.MapType(T.StringType(), T.StringType()), True),\n",
    "    T.StructField(\"created_by\",      T.StringType(),  False),\n",
    "    T.StructField(\"created_at\",      T.TimestampType(), False),\n",
    "    T.StructField(\"updated_by\",      T.StringType(),  True),\n",
    "    T.StructField(\"updated_at\",      T.TimestampType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "class DQXCheckRunner:\n",
    "    def __init__(self, spark: SparkSession, engine: DQEngine):\n",
    "        self.spark = spark\n",
    "        self.engine = engine\n",
    "\n",
    "    # --- Printing helpers ---\n",
    "    def print_info(self, msg: str) -> None:\n",
    "        print(f\"{Color.b}{Color.aqua_blue}{msg}{Color.r}\")\n",
    "\n",
    "    def print_warn(self, msg: str) -> None:\n",
    "        print(f\"{Color.b}{Color.yellow}{msg}{Color.r}\")\n",
    "\n",
    "    def print_error(self, msg: str) -> None:\n",
    "        print(f\"{Color.b}{Color.candy_red}{msg}{Color.r}\")\n",
    "\n",
    "    # --- File/YAML helpers ---\n",
    "    def read_yaml(self, path: str) -> Dict[str, Any]:\n",
    "        # Adapt for DBFS or workspace paths\n",
    "        if path.startswith(\"dbfs:/\"):\n",
    "            path = path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "        elif path.startswith(\"/Workspace/\"):\n",
    "            # Workspace-relative; adjust if needed to actual FS mount\n",
    "            pass\n",
    "        with open(path, \"r\") as fh:\n",
    "            return yaml.safe_load(fh) or {}\n",
    "\n",
    "    # --- Table helpers ---\n",
    "    def ensure_table_with_schema(self, full_name: str) -> None:\n",
    "        if not self.spark.catalog.tableExists(full_name):\n",
    "            cat, sch, _ = full_name.split(\".\")\n",
    "            self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "            self.spark.createDataFrame([], DQX_CHECKS_LOG_SCHEMA) \\\n",
    "                .write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "\n",
    "    def build_rule_id_lookup(self, checks_table: str) -> DataFrame:\n",
    "        return (\n",
    "            self.spark.read.table(checks_table)\n",
    "            .select(\n",
    "                F.col(\"table_name\").alias(\"hm_table_name\"),\n",
    "                F.col(\"run_config_name\").alias(\"hm_run_config_name\"),\n",
    "                F.col(\"name\").alias(\"hm_name\"),\n",
    "                F.col(\"hash_id\").alias(\"rule_id\"),\n",
    "            ).dropDuplicates()\n",
    "        )\n",
    "\n",
    "    def load_checks_for_run_config(self, checks_table: str, rc_name: str) -> List[dict]:\n",
    "        return self.engine.load_checks(\n",
    "            config=TableChecksStorageConfig(location=checks_table, run_config_name=rc_name)\n",
    "        )\n",
    "\n",
    "    def group_checks_by_table(self, checks: List[dict]) -> Dict[str, List[dict]]:\n",
    "        out: Dict[str, List[dict]] = {}\n",
    "        for c in checks:\n",
    "            t = c.get(\"table_name\")\n",
    "            if t:\n",
    "                out.setdefault(t, []).append(c)\n",
    "        return out\n",
    "\n",
    "    def explode_result_array(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        array_col: str,\n",
    "        severity_literal: str,\n",
    "        source_table: str,\n",
    "        run_config_name: str,\n",
    "        created_by: str\n",
    "    ) -> DataFrame:\n",
    "        if array_col not in df.columns:\n",
    "            return self.spark.createDataFrame([], DQX_CHECKS_LOG_SCHEMA)\n",
    "        exploded = df.select(F.explode_outer(F.col(array_col)).alias(\"r\")).select(\"r.*\")\n",
    "        if exploded.rdd.isEmpty():\n",
    "            return self.spark.createDataFrame([], DQX_CHECKS_LOG_SCHEMA)\n",
    "        cols_no_rule_id = [f.name for f in DQX_CHECKS_LOG_SCHEMA.fields if f.name != \"rule_id\"]\n",
    "        return (\n",
    "            exploded\n",
    "            .withColumn(\"result_id\",       F.expr(\"uuid()\"))\n",
    "            .withColumn(\"source_table\",    F.lit(source_table))\n",
    "            .withColumn(\"run_config_name\", F.lit(run_config_name))\n",
    "            .withColumn(\"severity\",        F.lit(severity_literal))\n",
    "            .withColumn(\"created_by\",      F.lit(created_by))\n",
    "            .withColumn(\"created_at\",      F.current_timestamp())\n",
    "            .withColumn(\"updated_by\",      F.lit(None).cast(T.StringType()))\n",
    "            .withColumn(\"updated_at\",      F.lit(None).cast(T.TimestampType()))\n",
    "            .select(cols_no_rule_id)\n",
    "        )\n",
    "\n",
    "    def apply_checks_for_table(\n",
    "        self, src_table: str, tbl_checks: List[dict]\n",
    "    ) -> Optional[DataFrame]:\n",
    "        try:\n",
    "            df_src = self.spark.read.table(src_table)\n",
    "        except Exception as e:\n",
    "            self.print_error(f\"Cannot read {src_table}: {e}\")\n",
    "            return None\n",
    "        try:\n",
    "            return self.engine.apply_checks_by_metadata(df_src, tbl_checks)\n",
    "        except Exception as e:\n",
    "            self.print_error(f\"apply_checks_by_metadata failed for {src_table}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def write_hits(\n",
    "        self, out_df: DataFrame, results_table: str, mode: str, options: Dict[str, str]\n",
    "    ) -> int:\n",
    "        if out_df.rdd.isEmpty():\n",
    "            self.print_info(\"No rows to write.\")\n",
    "            return 0\n",
    "        out_df = out_df.select([f.name for f in DQX_CHECKS_LOG_SCHEMA.fields])\n",
    "        written = out_df.count()\n",
    "        out_df.write.format(\"delta\").mode(mode).options(**options).saveAsTable(results_table)\n",
    "        return written\n",
    "\n",
    "    def apply_for_run_config(\n",
    "        self,\n",
    "        checks_table: str,\n",
    "        rc_name: str,\n",
    "        results_table: str,\n",
    "        write_mode: str,\n",
    "        write_options: Dict[str, str],\n",
    "        created_by: str\n",
    "    ) -> int:\n",
    "        self.print_info(f\"RUN-CONFIG: {rc_name}\")\n",
    "        self.ensure_table_with_schema(results_table)\n",
    "\n",
    "        try:\n",
    "            checks = self.load_checks_for_run_config(checks_table, rc_name)\n",
    "        except Exception as e:\n",
    "            self.print_error(f\"Failed to load checks for {rc_name}: {e}\")\n",
    "            return 0\n",
    "\n",
    "        if not checks:\n",
    "            self.print_warn(f\"No checks found for {rc_name}\")\n",
    "            return 0\n",
    "\n",
    "        # Fail-fast validation (even if done upstream)\n",
    "        status = DQEngine.validate_checks(checks)\n",
    "        if getattr(status, \"has_errors\", False):\n",
    "            self.print_error(f\"Invalid checks for {rc_name}: {status}\")\n",
    "            return 0\n",
    "\n",
    "        hash_map_df = F.broadcast(self.build_rule_id_lookup(checks_table))\n",
    "        by_table = self.group_checks_by_table(checks)\n",
    "        total_written = 0\n",
    "\n",
    "        for src_table, tbl_checks in by_table.items():\n",
    "            self.print_info(f\"Table: {src_table} | checks: {len(tbl_checks)}\")\n",
    "            annotated = self.apply_checks_for_table(src_table, tbl_checks)\n",
    "            if annotated is None:\n",
    "                continue\n",
    "\n",
    "            errors_df   = self.explode_result_array(annotated, \"_error\",   \"error\",   src_table, rc_name, created_by)\n",
    "            warnings_df = self.explode_result_array(annotated, \"_warning\", \"warning\", src_table, rc_name, created_by)\n",
    "            out_df = errors_df.unionByName(warnings_df, allowMissingColumns=True)\n",
    "            if out_df.rdd.isEmpty():\n",
    "                self.print_info(\"No error/warning hits.\")\n",
    "                continue\n",
    "\n",
    "            out_df = out_df.join(\n",
    "                hash_map_df,\n",
    "                (out_df.source_table    == hash_map_df.hm_table_name) &\n",
    "                (out_df.run_config_name == hash_map_df.hm_run_config_name) &\n",
    "                (out_df.name            == hash_map_df.hm_name),\n",
    "                \"left\"\n",
    "            ).drop(\"hm_table_name\", \"hm_run_config_name\", \"hm_name\")\n",
    "\n",
    "            missing = out_df.filter(F.col(\"rule_id\").isNull()).count()\n",
    "            if missing > 0:\n",
    "                self.print_warn(f\"{missing} hit(s) missing rule_id — dropped.\")\n",
    "                out_df = out_df.filter(F.col(\"rule_id\").isNotNull())\n",
    "            if out_df.rdd.isEmpty():\n",
    "                continue\n",
    "\n",
    "            written = self.write_hits(out_df, results_table, write_mode, write_options)\n",
    "            total_written += written\n",
    "            self.print_info(f\"Wrote {written} rows to {results_table} for {src_table}\")\n",
    "\n",
    "        return total_written\n",
    "\n",
    "\n",
    "def main(\n",
    "    output_config_yaml: str = \"dqx_output_config.yaml\",\n",
    "    results_table_override: Optional[str] = None,\n",
    "    created_by: str = \"AdminUser\"\n",
    ") -> None:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    engine = DQEngine(WorkspaceClient())\n",
    "    runner = DQXCheckRunner(spark, engine)\n",
    "\n",
    "    cfg = runner.read_yaml(output_config_yaml)\n",
    "    checks_table = cfg[\"dqx_config_table_name\"]\n",
    "    rc_map: Dict[str, Any] = cfg.get(\"run_config_name\", {}) or {}\n",
    "    if not rc_map:\n",
    "        raise ValueError(\"No run_config_name in config.\")\n",
    "\n",
    "    grand_total = 0\n",
    "    for rc_name, rc_cfg in rc_map.items():\n",
    "        out_cfg = (rc_cfg or {}).get(\"output_config\", {})\n",
    "        out_loc = results_table_override or out_cfg.get(\"location\")\n",
    "        out_mode = out_cfg.get(\"mode\", \"overwrite\")  # default overwrite\n",
    "        out_options = out_cfg.get(\"options\", {}) or {}\n",
    "        if not out_loc or out_loc.lower() == \"none\":\n",
    "            runner.print_warn(f\"{rc_name} has no output location — skipping.\")\n",
    "            continue\n",
    "        grand_total += runner.apply_for_run_config(\n",
    "            checks_table=checks_table,\n",
    "            rc_name=rc_name,\n",
    "            results_table=out_loc,\n",
    "            write_mode=out_mode,\n",
    "            write_options=out_options,\n",
    "            created_by=created_by\n",
    "        )\n",
    "\n",
    "    runner.print_info(f\"TOTAL rows written: {grand_total}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_apply_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
