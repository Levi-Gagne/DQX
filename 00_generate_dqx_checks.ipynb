{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4fe261-69b0-4d1a-a5a0-cb4d445b418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8383f8f1-9db5-46eb-abc3-d472fbc64507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- DQX profiling options: all known keys --\n",
    "profile_options = {\n",
    "    # \"round\": True,                 # (Removed - not valid for this profiler version)\n",
    "    \"max_in_count\": 10,            # Max distinct values for is_in rule\n",
    "    \"distinct_ratio\": 0.05,        # Max unique/total ratio for is_in rule\n",
    "    \"max_null_ratio\": 0.01,        # Max null fraction to allow is_not_null rule\n",
    "    \"remove_outliers\": True,       # Remove outliers for min/max\n",
    "    \"outlier_columns\": [],         # Only these columns get outlier removal (empty=all numerics)\n",
    "    \"num_sigmas\": 3,               # Stddev for outlier removal (z-score cutoff)\n",
    "    \"trim_strings\": True,          # Strip whitespace before profiling strings\n",
    "    \"max_empty_ratio\": 0.01,       # Max empty string ratio for is_not_null_or_empty\n",
    "    \"sample_fraction\": 0.3,        # Row fraction to sample\n",
    "    \"sample_seed\": None,           # Seed for reproducibility (set int for deterministic)\n",
    "    \"limit\": 1000,                 # Max number of rows to profile\n",
    "    \"profile_types\": None,         # List of rule types (e.g. [\"is_in\", \"is_not_null\"]); None=default\n",
    "    \"min_length\": None,            # Min string length to consider (None disables)\n",
    "    \"max_length\": None,            # Max string length to consider (None disables)\n",
    "    \"include_histograms\": False,   # Compute histograms as part of profiling\n",
    "    \"min_value\": None,             # Numeric min override (None disables)\n",
    "    \"max_value\": None,             # Numeric max override (None disables)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2168addb-58e3-4aae-853c-7e2f844cda7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8baa8a-c510-47e7-b107-3204ff7b2c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29833b1c-0236-46f9-a6f9-adb15dd55ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import inspect\n",
    "from typing import List, Optional, Dict, Any, Literal\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "\n",
    "# Keys currently shown in DQX docs for profiler \"options\" (kept permissive; we only warn on extras)\n",
    "DOC_SUPPORTED_KEYS = {\n",
    "    \"sample_fraction\", \"sample_seed\", \"limit\",\n",
    "    \"remove_outliers\", \"outlier_columns\", \"num_sigmas\",\n",
    "    \"max_null_ratio\",\n",
    "    \"trim_strings\", \"max_empty_ratio\",\n",
    "    \"distinct_ratio\", \"max_in_count\",\n",
    "    \"round\",\n",
    "}\n",
    "\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,                       # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param: str,                 # pipeline CSV | catalog | catalog.schema | table FQN CSV\n",
    "        output_format: str,              # \"yaml\" | \"table\"\n",
    "        output_location: str,            # yaml: folder or file; table: catalog.schema.table\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,     # e.g. \".tmp_*\"\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,       # None => whole table (only if mode==\"table\")\n",
    "        run_config_name: str = \"default\",          # DQX run group tag\n",
    "        criticality: str = \"warn\",                 # \"warn\" | \"error\"\n",
    "        yaml_key_order: Literal[\"engine\", \"custom\"] = \"engine\",  # \"engine\" uses DQX save; \"custom\" enforces key order\n",
    "        include_table_name: bool = True,           # include table_name in each rule dict\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_location = output_location\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.yaml_key_order = yaml_key_order\n",
    "        self.include_table_name = include_table_name\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "        if self.output_format not in {\"yaml\", \"table\"}:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table'.\")\n",
    "        if self.output_format == \"yaml\" and not self.output_location:\n",
    "            raise ValueError(\"When output_format='yaml', provide output_location (folder or file).\")\n",
    "\n",
    "    # ---------- profile options: pass via 'options' kwarg, warn on unknowns ----------\n",
    "    def _profile_call_kwargs(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Build kwargs for DQProfiler.profile / profile_table.\n",
    "        We pass:\n",
    "          - cols=self.columns (when provided)\n",
    "          - options=self.profile_options  (dict; profiler reads keys internally)\n",
    "        We only WARN on keys not in the current documented set; we do not drop them.\n",
    "        \"\"\"\n",
    "        kwargs: Dict[str, Any] = {}\n",
    "        if self.columns is not None:\n",
    "            kwargs[\"cols\"] = self.columns\n",
    "        if self.profile_options:\n",
    "            unknown = sorted(set(self.profile_options) - DOC_SUPPORTED_KEYS)\n",
    "            if unknown:\n",
    "                print(f\"[INFO] Profiling options not in current docs (passing through anyway): {unknown}\")\n",
    "            kwargs[\"options\"] = self.profile_options\n",
    "        return kwargs\n",
    "\n",
    "    # ---------- discovery ----------\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:             {self.mode}\")\n",
    "        print(f\"name_param:       {self.name_param}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_location:  {self.output_location}\")\n",
    "        print(f\"exclude_pattern:  {self.exclude_pattern}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"yaml_key_order:   {self.yaml_key_order}\")\n",
    "        print(f\"include_table_name: {self.include_table_name}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if self.mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{self.mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "        if self.columns is not None and self.mode != \"table\":\n",
    "            raise ValueError(\"The 'columns' parameter can only be used in mode='table'.\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "        if self.mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in self.name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                discovered += [x for x in pipeline_tables if x]\n",
    "\n",
    "        elif self.mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = self.name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                discovered += [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if self.name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = self.name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            discovered = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        else:  # table\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            tables = [t.strip() for t in self.name_param.split(\",\") if t.strip()]\n",
    "            for t in tables:\n",
    "                if t.count(\".\") != 2:\n",
    "                    raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "            discovered = tables\n",
    "\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "        print(\"\\nFinal table list to generate DQX rules for:\")\n",
    "        print(discovered)\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    # ---------- storage config helpers ----------\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def _workspace_files_upload(path: str, payload: bytes) -> None:\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            wc.files.upload(file_path=path, contents=payload, overwrite=True)  # newer SDK\n",
    "        except TypeError:\n",
    "            wc.files.upload(path=path, contents=payload, overwrite=True)       # older SDK\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_parent(path: str) -> None:\n",
    "        \"\"\"Create parent dir for local paths.\"\"\"\n",
    "        parent = os.path.dirname(path)\n",
    "        if parent and not os.path.exists(parent):\n",
    "            os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dbfs_parent(dbutils, path: str) -> None:\n",
    "        parent = path.rsplit(\"/\", 1)[0] if \"/\" in path else path\n",
    "        if parent:\n",
    "            dbutils.fs.mkdirs(parent)\n",
    "\n",
    "    # ---------- DQX check shaping ----------\n",
    "    def _dq_constraint_to_check(\n",
    "        self,\n",
    "        rule_name: str,\n",
    "        constraint_sql: str,\n",
    "        table_name: str,\n",
    "        criticality: str,\n",
    "        run_config_name: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert a profiler constraint (SQL) into a DQX check dict.\n",
    "        Key order: table_name, name, criticality, run_config_name, check (insertion order).\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": criticality,\n",
    "            \"run_config_name\": run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    \"expression\": constraint_sql,\n",
    "                    \"name\": rule_name,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if self.include_table_name:\n",
    "            d = {\"table_name\": table_name, **d}\n",
    "        return d\n",
    "\n",
    "    # ---------- YAML writers ----------\n",
    "    def _write_yaml_ordered(self, checks: List[Dict[str, Any]], path: str) -> None:\n",
    "        \"\"\"\n",
    "        Dump YAML preserving key order and upload:\n",
    "          - /Volumes/... | dbfs:/... | /dbfs/... -> dbutils.fs.put (mkdirs parent)\n",
    "          - /Shared/... (workspace files) -> Files API\n",
    "          - relative/local path -> os.makedirs + open(...)\n",
    "        \"\"\"\n",
    "        yaml_str = yaml.safe_dump(checks, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "        # DBFS / Volumes\n",
    "        if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "            try:\n",
    "                from databricks.sdk.runtime import dbutils\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"dbutils is required to write to DBFS/Volumes.\")\n",
    "            target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "            self._ensure_dbfs_parent(dbutils, target.rsplit(\"/\", 1)[0])\n",
    "            dbutils.fs.put(target, yaml_str, True)\n",
    "            print(f\"[RUN] Wrote ordered YAML to {path}\")\n",
    "            return\n",
    "\n",
    "        # Workspace files\n",
    "        if path.startswith(\"/\"):\n",
    "            self._workspace_files_upload(path, yaml_str.encode(\"utf-8\"))\n",
    "            print(f\"[RUN] Wrote ordered YAML to workspace file: {path}\")\n",
    "            return\n",
    "\n",
    "        # Local (driver) relative/absolute filesystem\n",
    "        full_path = os.path.abspath(path)\n",
    "        self._ensure_parent(full_path)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(yaml_str)\n",
    "        print(f\"[RUN] Wrote ordered YAML to local path: {full_path}\")\n",
    "\n",
    "    # ---------- main ----------\n",
    "    def run(self):\n",
    "        try:\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "            call_kwargs = self._profile_call_kwargs()\n",
    "            print(\"[RUN] Profiler call kwargs:\")\n",
    "            print(f\"  cols:    {call_kwargs.get('cols')}\")\n",
    "            print(f\"  options: {json.dumps(call_kwargs.get('options', {}), indent=2)}\")\n",
    "\n",
    "            dq_engine = DQEngine(WorkspaceClient())\n",
    "            total_checks = 0\n",
    "\n",
    "            for fq_table in tables:\n",
    "                if fq_table.count(\".\") != 2:\n",
    "                    print(f\"[WARN] Skipping invalid table name: {fq_table}\")\n",
    "                    continue\n",
    "                cat, sch, tab = fq_table.split(\".\")\n",
    "\n",
    "                # Verify readability\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table readability: {fq_table}\")\n",
    "                    self.spark.table(fq_table).limit(1).collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    # DataFrame profiling with options and optional cols\n",
    "                    summary_stats, profiles = profiler.profile(df, **call_kwargs)\n",
    "\n",
    "                    # If you prefer table-based API:\n",
    "                    # summary_stats, profiles = profiler.profile_table(table=fq_table, **call_kwargs)\n",
    "\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                checks: List[Dict[str, Any]] = []\n",
    "                for rule_name, constraint in (rules_dict or {}).items():\n",
    "                    checks.append(\n",
    "                        self._dq_constraint_to_check(\n",
    "                            rule_name=rule_name,\n",
    "                            constraint_sql=constraint,\n",
    "                            table_name=fq_table,\n",
    "                            criticality=self.criticality,\n",
    "                            run_config_name=self.run_config_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if not checks:\n",
    "                    print(f\"[INFO] No checks generated for {fq_table}.\")\n",
    "                    continue\n",
    "\n",
    "                # Destination selection\n",
    "                if self.output_format == \"yaml\":\n",
    "                    # Directory -> {table}.yaml ; or exact file path\n",
    "                    if self.output_location.endswith((\".yaml\", \".yml\")):\n",
    "                        path = self.output_location\n",
    "                    else:\n",
    "                        path = f\"{self.output_location.rstrip('/')}/{tab}.yaml\"\n",
    "\n",
    "                    if self.yaml_key_order == \"engine\":\n",
    "                        cfg = self._infer_file_storage_config(path)\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks via DQX to: {path}\")\n",
    "                        dq_engine.save_checks(checks, config=cfg)\n",
    "                    else:\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks with strict key order to: {path}\")\n",
    "                        self._write_yaml_ordered(checks, path)\n",
    "\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "                else:  # table\n",
    "                    cfg = self._table_storage_config(\n",
    "                        table_fqn=self.output_location,\n",
    "                        run_config_name=self.run_config_name,\n",
    "                        mode=\"append\"\n",
    "                    )\n",
    "                    print(f\"[RUN] Appending {len(checks)} checks to table: {self.output_location} (run_config_name={self.run_config_name})\")\n",
    "                    dq_engine.save_checks(checks, config=cfg)\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "            print(f\"[RUN] {'Successfully saved' if total_checks else 'No'} checks. Count: {total_checks}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "\n",
    "# -------------------- Usage example --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        # Sampling\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": 42,\n",
    "        \"limit\": 1000,\n",
    "        # Outliers\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        # Nulls / empties\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        # Distincts → is_in\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_in_count\": 10,\n",
    "        # Rounding\n",
    "        \"round\": True,\n",
    "        # (Keys not in current docs will still pass through; you’ll see a one-time INFO warning)\n",
    "        # \"include_histograms\": False,\n",
    "        # \"min_length\": None,\n",
    "        # \"max_length\": None,\n",
    "        # \"min_value\": None,\n",
    "        # \"max_value\": None,\n",
    "        # \"profile_types\": None,\n",
    "    }\n",
    "\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"dq_prd.monitoring.job_run_audit\",   # depends on mode\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"dqx_checks\",                   # yaml dir (local) OR \"/Shared/...\" OR \"dbfs:/...\" OR \"/Volumes/...\"\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"warn\",\n",
    "        yaml_key_order=\"custom\",                        # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_generate_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
