{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "234b5a48-ea46-4a18-bff5-f6f7f809a53a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DQX Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155eb0a8-86f0-48cd-ae4d-20f0dfa077f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2cdf8d-ffa0-48a3-bdfb-8198a7fd21f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df818459-a5fd-4360-a42a-0ccbbb8fb3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# CELL 1 — demo_spec.py  (Schemas + MEGASPEC + rule-class stubs + table directory)\n",
    "# =====================================================================================\n",
    "# Purpose: single source of truth for table names, schemas, row targets, and knobs.\n",
    "# =====================================================================================\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, BooleanType, DateType, TimestampType, DecimalType\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Globals / conventions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "DQX_CATALOG        = \"dq_dev\"\n",
    "DQX_SCHEMA         = \"dqx\"                     # base schema for demo tables\n",
    "QUARANTINE_TABLE   = f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_quarantine\"  # sink for ERROR rows\n",
    "DQ_OUTPUT_COLUMNS  = [\"warning\", \"error\"]      # columns your runner appends\n",
    "\n",
    "ROW_TARGETS = {\n",
    "    f\"{DQX_SCHEMA}.demo_employee\":   2_000,\n",
    "    f\"{DQX_SCHEMA}.demo_customer\":   1_000,\n",
    "    f\"{DQX_SCHEMA}.demo_project\":      600,\n",
    "    f\"{DQX_SCHEMA}.demo_timesheet\": 350_000,\n",
    "    f\"{DQX_SCHEMA}.demo_expense\":   120_000,\n",
    "}\n",
    "\n",
    "# Canonical table names (full paths) + groupings you can use anywhere\n",
    "TABLES = {\n",
    "    \"schema\": DQX_SCHEMA,\n",
    "    \"employee\":   f\"{DQX_SCHEMA}.demo_employee\",\n",
    "    \"customer\":   f\"{DQX_SCHEMA}.demo_customer\",\n",
    "    \"project\":    f\"{DQX_SCHEMA}.demo_project\",\n",
    "    \"timesheet\":  f\"{DQX_SCHEMA}.demo_timesheet\",\n",
    "    \"expense\":    f\"{DQX_SCHEMA}.demo_expense\",\n",
    "    \"quarantine\": QUARANTINE_TABLE,\n",
    "    # Groupings\n",
    "    \"sources\": [\n",
    "        f\"{DQX_SCHEMA}.demo_employee\",\n",
    "        f\"{DQX_SCHEMA}.demo_customer\",\n",
    "        f\"{DQX_SCHEMA}.demo_project\",\n",
    "        f\"{DQX_SCHEMA}.demo_timesheet\",\n",
    "        f\"{DQX_SCHEMA}.demo_expense\",\n",
    "    ],\n",
    "    \"facts\": [\n",
    "        f\"{DQX_SCHEMA}.demo_timesheet\",\n",
    "        f\"{DQX_SCHEMA}.demo_expense\",\n",
    "    ],\n",
    "    \"dims\": [\n",
    "        f\"{DQX_SCHEMA}.demo_employee\",\n",
    "        f\"{DQX_SCHEMA}.demo_customer\",\n",
    "        f\"{DQX_SCHEMA}.demo_project\",\n",
    "    ],\n",
    "    \"all\": [\n",
    "        f\"{DQX_SCHEMA}.demo_employee\",\n",
    "        f\"{DQX_SCHEMA}.demo_customer\",\n",
    "        f\"{DQX_SCHEMA}.demo_project\",\n",
    "        f\"{DQX_SCHEMA}.demo_timesheet\",\n",
    "        f\"{DQX_SCHEMA}.demo_expense\",\n",
    "        QUARANTINE_TABLE,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Spark Structured Schemas\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "demo_employee_schema = StructType([\n",
    "    StructField(\"employee_id\",        StringType(),  False),\n",
    "    StructField(\"full_name\",          StringType(),  False),\n",
    "    StructField(\"department\",         StringType(),  False),  # Consulting, Audit, Tax, IT, Ops\n",
    "    StructField(\"role\",               StringType(),  False),  # Engineer, Analyst, Consultant, Manager, Support\n",
    "    StructField(\"cost_center\",        StringType(),  True),   # CC-####\n",
    "    StructField(\"employment_status\",  StringType(),  False),  # Active, Leave, Terminated\n",
    "    StructField(\"hire_date\",          DateType(),    False),\n",
    "    StructField(\"termination_date\",   DateType(),    True),\n",
    "    StructField(\"work_email\",         StringType(),  True),\n",
    "    StructField(\"country_code\",       StringType(),  True),\n",
    "    # DQ columns appended later: warning, error\n",
    "])\n",
    "\n",
    "demo_customer_schema = StructType([\n",
    "    StructField(\"customer_id\",            StringType(),  False),\n",
    "    StructField(\"customer_name\",          StringType(),  False),\n",
    "    StructField(\"industry\",               StringType(),  False),  # Technology, Healthcare, Finance, Retail, Manufacturing\n",
    "    StructField(\"country_code\",           StringType(),  True),   # ISO-3166 alpha-2\n",
    "    StructField(\"status\",                 StringType(),  False),  # Active, Prospect, Inactive\n",
    "    StructField(\"onboarding_date\",        DateType(),    False),\n",
    "    StructField(\"primary_contact_email\",  StringType(),  True),\n",
    "    StructField(\"registration_number\",    StringType(),  True),   # national reg / tax-like id\n",
    "])\n",
    "\n",
    "demo_project_schema = StructType([\n",
    "    StructField(\"project_id\",          StringType(),       False),\n",
    "    StructField(\"customer_id\",         StringType(),       False),  # FK -> dqx.demo_customer\n",
    "    StructField(\"project_name\",        StringType(),       False),\n",
    "    StructField(\"status\",              StringType(),       False),  # Planned, Active, OnHold, Closed\n",
    "    StructField(\"start_date\",          DateType(),         False),\n",
    "    StructField(\"end_date\",            DateType(),         True),\n",
    "    StructField(\"manager_employee_id\", StringType(),       True),   # FK -> dqx.demo_employee\n",
    "    StructField(\"budget_amount\",       DecimalType(18, 2), True),\n",
    "    StructField(\"billing_model\",       StringType(),       False),  # T&M, Fixed, Retainer\n",
    "])\n",
    "\n",
    "demo_timesheet_schema = StructType([\n",
    "    StructField(\"timesheet_id\",  StringType(),       False),\n",
    "    StructField(\"employee_id\",   StringType(),       False),  # FK -> dqx.demo_employee\n",
    "    StructField(\"project_id\",    StringType(),       False),  # FK -> dqx.demo_project\n",
    "    StructField(\"work_date\",     DateType(),         False),\n",
    "    StructField(\"hours_worked\",  DecimalType(5, 2),  False),  # 0–24\n",
    "    StructField(\"work_type\",     StringType(),       False),  # Billable, NonBillable, Admin\n",
    "    StructField(\"source_system\", StringType(),       False),  # Workday, Jira, CSV\n",
    "    StructField(\"created_ts\",    TimestampType(),    False),\n",
    "])\n",
    "\n",
    "demo_expense_schema = StructType([\n",
    "    StructField(\"expense_id\",       StringType(),       False),\n",
    "    StructField(\"employee_id\",      StringType(),       False),  # FK -> dqx.demo_employee\n",
    "    StructField(\"project_id\",       StringType(),       True),   # FK -> dqx.demo_project (nullable)\n",
    "    StructField(\"expense_date\",     DateType(),         False),\n",
    "    StructField(\"category\",         StringType(),       False),  # Meals, Travel, Supplies, Software, Other\n",
    "    StructField(\"amount\",           DecimalType(18, 2), False),\n",
    "    StructField(\"currency_code\",    StringType(),       False),  # ISO-4217\n",
    "    StructField(\"merchant\",         StringType(),       True),\n",
    "    StructField(\"receipt_attached\", BooleanType(),      False),\n",
    "    StructField(\"submission_ts\",    TimestampType(),    False),\n",
    "])\n",
    "\n",
    "# Handy schema map (full paths)\n",
    "SCHEMAS_BY_TABLE = {\n",
    "    TABLES[\"employee\"]:  demo_employee_schema,\n",
    "    TABLES[\"customer\"]:  demo_customer_schema,\n",
    "    TABLES[\"project\"]:   demo_project_schema,\n",
    "    TABLES[\"timesheet\"]: demo_timesheet_schema,\n",
    "    TABLES[\"expense\"]:   demo_expense_schema,\n",
    "}\n",
    "\n",
    "TABLE_ORDER = [\n",
    "    TABLES[\"employee\"],\n",
    "    TABLES[\"customer\"],\n",
    "    TABLES[\"project\"],\n",
    "    TABLES[\"timesheet\"],\n",
    "    TABLES[\"expense\"],\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# MEGASPEC (nested, readable). Map this to dbldatagen/ddgen in your builder.\n",
    "# Fully qualified refs use the dqx schema.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "MEGASPEC = {\n",
    "    \"schema\": DQX_SCHEMA,\n",
    "    \"quarantine_table\": QUARANTINE_TABLE,\n",
    "    \"dq_output_columns\": DQ_OUTPUT_COLUMNS,          # appended by your rule runner\n",
    "    \"flags\": {\n",
    "        \"enforce_project_window\": True,              # error if outside project dates\n",
    "        \"allow_weekend_billable\": \"warn\",            # 'warn' | 'error' | 'off'\n",
    "    },\n",
    "    \"knobs\": {\n",
    "        \"receipt_threshold\": 75.00,                  # USD; >= threshold requires receipt\n",
    "        \"meal_limit\": 150.00,                        # warn if Meals > limit\n",
    "        \"travel_limit\": 500.00,                      # warn if Travel > limit\n",
    "        \"max_hours_per_day_error\": 24.0,             # error if > this\n",
    "        \"hi_hours_warn\": 12.0,                       # optional warn if > this\n",
    "        \"duplicate_timesheet_window_days\": 0,        # same-day dupes\n",
    "        \"duplicate_expense_exact_match\": True,       # dup key = (emp, merchant, date, amount)\n",
    "        \"valid_currency_codes\": [\"USD\",\"CAD\",\"MXN\",\"GBP\",\"INR\"],\n",
    "    },\n",
    "    \"tables\": {\n",
    "        f\"{DQX_SCHEMA}.demo_employee\": {\n",
    "            \"rows\": ROW_TARGETS[f\"{DQX_SCHEMA}.demo_employee\"],\n",
    "            \"cols\": {\n",
    "                \"employee_id\": {\"type\":\"sequence\",\"prefix\":\"E\",\"start\":1001},\n",
    "                \"full_name\": {\"type\":\"faker\",\"provider\":\"name\"},\n",
    "                \"department\": {\"type\":\"choice\",\"values\":[\"Consulting\",\"Audit\",\"Tax\",\"IT\",\"Ops\"],\"weights\":[0.35,0.20,0.20,0.15,0.10]},\n",
    "                \"role\": {\"type\":\"choice\",\"values\":[\"Engineer\",\"Analyst\",\"Consultant\",\"Manager\",\"Support\"]},\n",
    "                \"cost_center\": {\"type\":\"pattern\",\"format\":\"CC-{0000-9999}\",\"null_rate\":0.02},\n",
    "                \"employment_status\": {\"type\":\"choice\",\"values\":[\"Active\",\"Leave\",\"Terminated\"],\"weights\":[0.90,0.03,0.07]},\n",
    "                \"hire_date\": {\"type\":\"date\",\"start\":\"2018-01-01\",\"end\":\"2025-08-01\"},\n",
    "                \"termination_date\": {\"type\":\"conditional_date\",\"when\":\"employment_status == 'Terminated'\",\"offset_from\":\"hire_date\",\"min_days\":30,\"max_days\":2500,\"null_rate\":0.08},\n",
    "                \"work_email\": {\"type\":\"template\",\"template\":\"{first}.{last}@company.com\",\"invalid_rate\":0.03},\n",
    "                \"country_code\": {\"type\":\"choice\",\"values\":[\"US\",\"CA\",\"MX\",\"GB\",\"IN\"],\"invalid_rate\":0.005}\n",
    "            },\n",
    "            \"inject\": {\"EMP_TERM_DATE_BEFORE_HIRE\": 0.008}\n",
    "        },\n",
    "\n",
    "        f\"{DQX_SCHEMA}.demo_customer\": {\n",
    "            \"rows\": ROW_TARGETS[f\"{DQX_SCHEMA}.demo_customer\"],\n",
    "            \"cols\": {\n",
    "                \"customer_id\": {\"type\":\"sequence\",\"prefix\":\"C\",\"start\":5001},\n",
    "                \"customer_name\": {\"type\":\"faker\",\"provider\":\"company\"},\n",
    "                \"industry\": {\"type\":\"choice\",\"values\":[\"Technology\",\"Healthcare\",\"Finance\",\"Retail\",\"Manufacturing\"],\"weights\":[0.30,0.20,0.20,0.20,0.10]},\n",
    "                \"country_code\": {\"type\":\"choice\",\"values\":[\"US\",\"CA\",\"MX\",\"GB\",\"IN\"],\"invalid_rate\":0.005},\n",
    "                \"status\": {\"type\":\"choice\",\"values\":[\"Active\",\"Prospect\",\"Inactive\"],\"weights\":[0.75,0.15,0.10]},\n",
    "                \"onboarding_date\": {\"type\":\"date\",\"start\":\"2019-01-01\",\"end\":\"2025-08-01\"},\n",
    "                \"primary_contact_email\": {\"type\":\"faker\",\"provider\":\"email\",\"invalid_rate\":0.02},\n",
    "                \"registration_number\": {\"type\":\"pattern\",\"format\":\"RN-{00000000-99999999}\",\"dupe_rate_active\":0.01}\n",
    "            }\n",
    "        },\n",
    "\n",
    "        f\"{DQX_SCHEMA}.demo_project\": {\n",
    "            \"rows\": ROW_TARGETS[f\"{DQX_SCHEMA}.demo_project\"],\n",
    "            \"cols\": {\n",
    "                \"project_id\": {\"type\":\"sequence\",\"prefix\":\"P\",\"start\":10001},\n",
    "                \"customer_id\": {\"type\":\"fk\",\"ref\": f\"{DQX_SCHEMA}.demo_customer.customer_id\",\"null_rate\":0.01},\n",
    "                \"project_name\": {\"type\":\"template\",\"template\":\"Project {seq}\"},\n",
    "                \"status\": {\"type\":\"choice\",\"values\":[\"Planned\",\"Active\",\"OnHold\",\"Closed\"],\"weights\":[0.15,0.55,0.10,0.20]},\n",
    "                \"start_date\": {\"type\":\"date\",\"start\":\"2020-01-01\",\"end\":\"2025-03-01\"},\n",
    "                \"end_date\": {\"type\":\"date_or_null\",\"null_rate\":0.35,\"min_from\":\"start_date\",\"min_days\":30,\"max_days\":1200},\n",
    "                \"manager_employee_id\": {\"type\":\"fk\",\"ref\": f\"{DQX_SCHEMA}.demo_employee.employee_id\",\"null_rate\":0.03},\n",
    "                \"budget_amount\": {\"type\":\"decimal\",\"min\":10_000.00,\"max\":2_000_000.00,\"skew\":\"right\"},\n",
    "                \"billing_model\": {\"type\":\"choice\",\"values\":[\"T&M\",\"Fixed\",\"Retainer\"],\"weights\":[0.55,0.35,0.10]}\n",
    "            },\n",
    "            \"inject\": {\"PROJ_END_BEFORE_START\":0.007,\"PROJ_BUDGET_NONPOSITIVE\":0.004}\n",
    "        },\n",
    "\n",
    "        f\"{DQX_SCHEMA}.demo_timesheet\": {\n",
    "            \"rows\": ROW_TARGETS[f\"{DQX_SCHEMA}.demo_timesheet\"],\n",
    "            \"cols\": {\n",
    "                \"timesheet_id\": {\"type\":\"uuid\"},\n",
    "                \"employee_id\": {\"type\":\"fk\",\"ref\": f\"{DQX_SCHEMA}.demo_employee.employee_id\"},\n",
    "                \"project_id\": {\"type\":\"fk\",\"ref\": f\"{DQX_SCHEMA}.demo_project.project_id\"},\n",
    "                \"work_date\": {\"type\":\"date\",\"start\":\"2024-01-01\",\"end\":\"2025-08-10\",\"weekday_bias\":0.85},\n",
    "                \"hours_worked\": {\"type\":\"decimal\",\"min\":0.00,\"max\":12.00,\"step\":0.25,\"tail_spikes\":[20.00,24.00],\"tail_rate\":0.004},\n",
    "                \"work_type\": {\"type\":\"choice\",\"values\":[\"Billable\",\"NonBillable\",\"Admin\"],\"weights\":[0.70,0.25,0.05]},\n",
    "                \"source_system\": {\"type\":\"choice\",\"values\":[\"Workday\",\"Jira\",\"CSV\"],\"weights\":[0.70,0.20,0.10]},\n",
    "                \"created_ts\": {\"type\":\"timestamp_near\",\"base\":\"work_date\",\"min_offset_hours\":0,\"max_offset_hours\":72}\n",
    "            },\n",
    "            \"inject\": {\"TS_FUTURE_DATE\":0.003,\"TS_DUP_EMP_PROJ_DAY\":0.006,\"TS_NEG_OR_GT24\":0.003}\n",
    "        },\n",
    "\n",
    "        f\"{DQX_SCHEMA}.demo_expense\": {\n",
    "            \"rows\": ROW_TARGETS[f\"{DQX_SCHEMA}.demo_expense\"],\n",
    "            \"cols\": {\n",
    "                \"expense_id\": {\"type\":\"uuid\"},\n",
    "                \"employee_id\": {\"type\":\"fk\",\"ref\": f\"{DQX_SCHEMA}.demo_employee.employee_id\"},\n",
    "                \"project_id\": {\"type\":\"fk\",\"ref\": f\"{DQX_SCHEMA}.demo_project.project_id\",\"null_rate\":0.25},\n",
    "                \"expense_date\": {\"type\":\"date\",\"start\":\"2024-01-01\",\"end\":\"2025-08-10\",\"weekend_bias\":0.20},\n",
    "                \"category\": {\"type\":\"choice\",\"values\":[\"Meals\",\"Travel\",\"Supplies\",\"Software\",\"Other\"],\"weights\":[0.35,0.25,0.20,0.10,0.10]},\n",
    "                \"amount\": {\"type\":\"decimal\",\"min\":5.00,\"max\":5_000.00,\"skew\":\"right\",\"round_dollar_rate\":0.10},\n",
    "                \"currency_code\": {\"type\":\"choice\",\"values\":[\"USD\",\"CAD\",\"MXN\",\"GBP\",\"INR\"],\"invalid_rate\":0.002},\n",
    "                \"merchant\": {\"type\":\"choice\",\"values\":[\"Uber\",\"Lyft\",\"Delta\",\"AA\",\"Staples\",\"BestBuy\",\"Amazon\",\"LocalCafe\",\"HotelCo\",\"SoftwareCo\"]},\n",
    "                \"receipt_attached\": {\"type\":\"boolean\",\"true_rate\":0.92},\n",
    "                \"submission_ts\": {\"type\":\"timestamp_near\",\"base\":\"expense_date\",\"min_offset_hours\":0,\"max_offset_hours\":240}\n",
    "            },\n",
    "            \"inject\": {\"EXP_DUP\":0.008,\"EXP_NO_RECEIPT_OVER_75\":0.010,\"EXP_OUT_OF_POLICY\":0.020}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Rule classes (stubs). Define the .rules() bodies in a separate module or here.\n",
    "# Each .rules() should return list[dict]:\n",
    "#   { \"id\": str, \"level\": \"error\"|\"warn\", \"expr\": \"<Spark SQL>\", \"message\": str, ... }\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class BaseRuleSet:\n",
    "    \"\"\"Interface for a rule set; implement .rules() to return a list of rule dicts\"\"\"\n",
    "    def rules(self):\n",
    "        raise NotImplementedError(\"Implement .rules() to return a list of rule dicts\")\n",
    "\n",
    "class EmployeeRules(BaseRuleSet):\n",
    "    \"\"\"dqx.demo_employee — suggested IDs: EMP_TERM_DATE_MISSING, EMP_TERM_DATE_BEFORE_HIRE, EMP_EMAIL_DOMAIN_WARN, EMP_COUNTRY_CODE_WARN\"\"\"\n",
    "    pass\n",
    "\n",
    "class CustomerRules(BaseRuleSet):\n",
    "    \"\"\"dqx.demo_customer — suggested IDs: CUST_REG_DUP_ACTIVE, CUST_EMAIL_WARN, CUST_COUNTRY_CODE_WARN\"\"\"\n",
    "    pass\n",
    "\n",
    "class ProjectRules(BaseRuleSet):\n",
    "    \"\"\"dqx.demo_project — suggested IDs: PROJ_END_BEFORE_START, PROJ_BUDGET_NONPOSITIVE, PROJ_FK_CUSTOMER_MISSING, PROJ_MANAGER_FK_WARN, PROJ_CLOSED_END_NULL_WARN\"\"\"\n",
    "    pass\n",
    "\n",
    "class TimesheetRules(BaseRuleSet):\n",
    "    \"\"\"dqx.demo_timesheet — suggested IDs: TS_NEG_OR_GT24, TS_HI_HOURS_WARN, TS_FUTURE_DATE, TS_EMP_STATUS_ERROR, TS_DUP_EMP_PROJ_DAY, TS_OUTSIDE_PROJECT_WINDOW, TS_WEEKEND_BILLABLE\"\"\"\n",
    "    pass\n",
    "\n",
    "class ExpenseRules(BaseRuleSet):\n",
    "    \"\"\"dqx.demo_expense — suggested IDs: EXP_DUP, EXP_NO_RECEIPT_OVER_T, EXP_OOP, EXP_BAD_CCY, EXP_EMP_STATUS_ERROR, EXP_OUTSIDE_PROJECT_WINDOW\"\"\"\n",
    "    pass\n",
    "\n",
    "# Map table → rule class so a driver can instantiate dynamically\n",
    "RULE_CLASSES_BY_TABLE = {\n",
    "    TABLES[\"employee\"]:  EmployeeRules,\n",
    "    TABLES[\"customer\"]:  CustomerRules,\n",
    "    TABLES[\"project\"]:   ProjectRules,\n",
    "    TABLES[\"timesheet\"]: TimesheetRules,\n",
    "    TABLES[\"expense\"]:   ExpenseRules,\n",
    "}\n",
    "\n",
    "print(f\"\\n[SPEC LOADED] schema={DQX_SCHEMA}  sources={len(TABLES['sources'])}  quarantine={TABLES['quarantine']}\")\n",
    "print(f\"Row targets: \" + \", \".join([f\"{k.split('.')[-1]}={v:,}\" for k,v in ROW_TARGETS.items()]))\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 2 — Driver setup (imports, DB/schema, quarantine table)\n",
    "# =====================================================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from uuid import uuid4\n",
    "\n",
    "# Pull in the spec from Cell 1 (already in-memory if same notebook)\n",
    "from __main__ import (\n",
    "    DQX_SCHEMA,\n",
    "    TABLES,\n",
    "    ROW_TARGETS,\n",
    "    SCHEMAS_BY_TABLE,\n",
    "    MEGASPEC,\n",
    "    RULE_CLASSES_BY_TABLE,\n",
    "    QUARANTINE_TABLE\n",
    ")\n",
    "\n",
    "def banner(msg):\n",
    "    print(\"\\n\" + \"═\"*88)\n",
    "    print(f\" {msg}\")\n",
    "    print(\"═\"*88)\n",
    "\n",
    "banner(\"SETUP: Create schema and quarantine table\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DQX_SCHEMA}\")\n",
    "spark.sql(f\"USE {DQX_SCHEMA}\")\n",
    "\n",
    "run_id = str(uuid4())\n",
    "print(f\"Using schema: {DQX_SCHEMA}\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {QUARANTINE_TABLE} (\n",
    "  _source_table     STRING,\n",
    "  _rule_id          STRING,\n",
    "  _level            STRING,\n",
    "  _reason           STRING,\n",
    "  _run_id           STRING,\n",
    "  _event_ts         TIMESTAMP,\n",
    "  _row_payload_json STRING\n",
    ") USING delta\n",
    "\"\"\")\n",
    "print(f\"Quarantine table ready: {QUARANTINE_TABLE}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 3 — Helpers (small utilities for deterministic, lightweight data gen)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"HELPERS: small UDF-like helpers for datagen\")\n",
    "\n",
    "def seq_id(prefix: str, start: int):\n",
    "    return F.concat(F.lit(prefix), F.lpad((F.col(\"_rn\") + F.lit(start)).cast(\"string\"), 4, \"0\"))\n",
    "\n",
    "def choice_expr(options, weights=None, seed_col=\"_rand\"):\n",
    "    if not weights:\n",
    "        weights = [1.0/len(options)]*len(options)\n",
    "    cum = []\n",
    "    s = 0.0\n",
    "    for w in weights:\n",
    "        s += float(w); cum.append(s)\n",
    "    x = F.col(seed_col)\n",
    "    expr = None\n",
    "    for i, val in enumerate(options):\n",
    "        cond = (x <= cum[i])\n",
    "        expr = (F.when(cond, F.lit(val)) if expr is None\n",
    "                else expr.otherwise(F.when(cond, F.lit(val))))\n",
    "    return expr.otherwise(F.lit(options[-1]))\n",
    "\n",
    "def rand_date(start: str, end: str, rand_expr=\"rand()\"):\n",
    "    # random date between start and end inclusive\n",
    "    days = F.datediff(F.lit(end), F.lit(start))\n",
    "    return F.expr(f\"date_add('{start}', cast({rand_expr} * {days} as int))\")\n",
    "\n",
    "print(\"Helpers ready.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 4 — DIM: Employees\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"BUILD: dqx.demo_employee\")\n",
    "\n",
    "n_emp = ROW_TARGETS[TABLES[\"employee\"]]\n",
    "base = (spark.range(n_emp)\n",
    "        .withColumn(\"_rn\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1)\n",
    "        .withColumn(\"_rand\", F.rand(seed=42))\n",
    "        .withColumn(\"_rand2\", F.rand(seed=99)))\n",
    "\n",
    "emp_df = base.select(\n",
    "    seq_id(\"E\", 1001).alias(\"employee_id\"),\n",
    "    F.concat(F.lit(\"Emp \"), F.col(\"_rn\")).alias(\"full_name\"),\n",
    "    choice_expr([\"Consulting\",\"Audit\",\"Tax\",\"IT\",\"Ops\"], [0.35,0.2,0.2,0.15,0.1]).alias(\"department\"),\n",
    "    choice_expr([\"Engineer\",\"Analyst\",\"Consultant\",\"Manager\",\"Support\"]).alias(\"role\"),\n",
    "    F.when(F.rand() < 0.02, F.lit(None)).otherwise(F.concat(F.lit(\"CC-\"), F.lpad((F.rand()*9999).cast(\"int\").cast(\"string\"), 4, \"0\"))).alias(\"cost_center\"),\n",
    "    choice_expr([\"Active\",\"Leave\",\"Terminated\"], [0.90,0.03,0.07], seed_col=\"_rand2\").alias(\"employment_status\"),\n",
    "    rand_date(\"2018-01-01\", \"2025-08-01\", \"rand()\").alias(\"hire_date\"),\n",
    "    F.lit(None).cast(\"date\").alias(\"termination_date\"),\n",
    "    F.concat(F.lit(\"emp\"), F.col(\"_rn\").cast(\"string\"), F.lit(\"@company.com\")).alias(\"work_email\"),\n",
    "    choice_expr([\"US\",\"CA\",\"MX\",\"GB\",\"IN\"], [0.5,0.15,0.1,0.15,0.1]).alias(\"country_code\")\n",
    ")\n",
    "\n",
    "emp_df = (emp_df\n",
    "    # inject missing/invalid termination_date for a slice of Terminated\n",
    "    .withColumn(\"termination_date\",\n",
    "        F.when((F.col(\"employment_status\")==\"Terminated\") & (F.rand() < 0.08), F.lit(None).cast(\"date\"))\n",
    "         .otherwise(F.when(F.col(\"employment_status\")==\"Terminated\",\n",
    "                           F.expr(\"date_add(hire_date, cast(rand()*2500 + 30 as int))\"))\n",
    "                    .otherwise(F.lit(None).cast(\"date\")))\n",
    "    )\n",
    "    .withColumn(\"work_email\",\n",
    "        F.when(F.rand() < 0.03, F.concat(F.lit(\"emp\"), F.col(\"employee_id\").substr(F.length(F.col(\"employee_id\"))-3,4), F.lit(\"@gmail.com\")))\n",
    "         .otherwise(F.col(\"work_email\"))\n",
    "    )\n",
    "    .withColumn(\"country_code\",\n",
    "        F.when(F.rand() < 0.005, F.lit(\"XX\")).otherwise(F.col(\"country_code\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "emp_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"employee\"])\n",
    "print(f\"Wrote: {TABLES['employee']}  rows={spark.table(TABLES['employee']).count():,}\")\n",
    "display(spark.table(TABLES[\"employee\"]).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 5 — DIM: Customers\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"BUILD: dqx.demo_customer\")\n",
    "\n",
    "n_cust = ROW_TARGETS[TABLES[\"customer\"]]\n",
    "base = spark.range(n_cust).withColumn(\"_rn\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1)\n",
    "\n",
    "cust_df = base.select(\n",
    "    seq_id(\"C\", 5001).alias(\"customer_id\"),\n",
    "    F.concat(F.lit(\"Customer \"), F.col(\"_rn\")).alias(\"customer_name\"),\n",
    "    choice_expr([\"Technology\",\"Healthcare\",\"Finance\",\"Retail\",\"Manufacturing\"], [0.3,0.2,0.2,0.2,0.1]).alias(\"industry\"),\n",
    "    choice_expr([\"US\",\"CA\",\"MX\",\"GB\",\"IN\"]).alias(\"country_code\"),\n",
    "    choice_expr([\"Active\",\"Prospect\",\"Inactive\"], [0.75,0.15,0.10]).alias(\"status\"),\n",
    "    rand_date(\"2019-01-01\", \"2025-08-01\", \"rand()\").alias(\"onboarding_date\"),\n",
    "    F.when(F.rand() < 0.02, F.concat(F.lit(\"contact\"), F.col(\"_rn\").cast(\"string\")))\n",
    "     .otherwise(F.concat(F.lit(\"contact\"), F.col(\"_rn\").cast(\"string\"), F.lit(\"@example.com\"))).alias(\"primary_contact_email\"),\n",
    "    F.concat(F.lit(\"RN-\"), F.lpad((F.rand()*99999999).cast(\"int\").cast(\"string\"), 8, \"0\")).alias(\"registration_number\")\n",
    ")\n",
    "\n",
    "# duplicate registration_number among Actives (~1%)\n",
    "dupes = cust_df.where(F.col(\"status\")==\"Active\").limit(int(n_cust*0.01)).select(\n",
    "    F.col(\"registration_number\").alias(\"dup_reg\")\n",
    ")\n",
    "cust_df = (cust_df.join(dupes.limit(1), how=\"left\")\n",
    "           .withColumn(\"registration_number\",\n",
    "               F.when(F.rand()<0.01, F.col(\"dup_reg\")).otherwise(F.col(\"registration_number\")))\n",
    "           .drop(\"dup_reg\",\"_rn\"))\n",
    "\n",
    "cust_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"customer\"])\n",
    "print(f\"Wrote: {TABLES['customer']}  rows={spark.table(TABLES['customer']).count():,}\")\n",
    "display(spark.table(TABLES[\"customer\"]).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 6 — DIM: Projects (FKs to Customer & Employee)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"BUILD: dqx.demo_project\")\n",
    "\n",
    "n_proj = ROW_TARGETS[TABLES[\"project\"]]\n",
    "emp_lu = spark.table(TABLES[\"employee\"]).select(\"employee_id\").withColumn(\"emp_idx\", F.row_number().over(Window.orderBy(\"employee_id\")) - 1)\n",
    "cust_lu = spark.table(TABLES[\"customer\"]).select(\"customer_id\").withColumn(\"cust_idx\", F.row_number().over(Window.orderBy(\"customer_id\")) - 1)\n",
    "emp_count, cust_count = emp_lu.count(), cust_lu.count()\n",
    "\n",
    "proj_base = spark.range(n_proj).withColumn(\"_rn\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1)\n",
    "\n",
    "proj_df = (proj_base.select(\n",
    "    seq_id(\"P\", 10001).alias(\"project_id\"),\n",
    "    (F.col(\"_rn\") % cust_count).alias(\"cust_idx\"),\n",
    "    F.concat(F.lit(\"Project \"), F.col(\"_rn\")).alias(\"project_name\"),\n",
    "    choice_expr([\"Planned\",\"Active\",\"OnHold\",\"Closed\"], [0.15,0.55,0.10,0.20]).alias(\"status\"),\n",
    "    rand_date(\"2020-01-01\", \"2025-03-01\", \"rand()\").alias(\"start_date\"),\n",
    "    F.lit(None).cast(\"date\").alias(\"end_date\"),\n",
    "    (F.col(\"_rn\") % emp_count).alias(\"emp_idx\"),\n",
    "    (F.rand()* (2_000_000 - 10_000) + 10_000).cast(\"decimal(18,2)\").alias(\"budget_amount\"),\n",
    "    choice_expr([\"T&M\",\"Fixed\",\"Retainer\"], [0.55,0.35,0.10]).alias(\"billing_model\")\n",
    ").join(cust_lu, \"cust_idx\", \"left\").join(emp_lu, \"emp_idx\", \"left\")\n",
    " .withColumn(\"end_date\", F.when(F.rand() < 0.65, F.expr(\"date_add(start_date, cast(rand()*1200 + 30 as int))\")).otherwise(F.lit(None).cast(\"date\")))\n",
    " .withColumn(\"budget_amount\", F.when(F.rand() < 0.004, F.lit(0).cast(\"decimal(18,2)\")).otherwise(F.col(\"budget_amount\")))\n",
    " .select(\"project_id\",\"customer_id\",\"project_name\",\"status\",\"start_date\",\"end_date\",\"employee_id\",\"budget_amount\",\"billing_model\")\n",
    " .withColumnRenamed(\"employee_id\",\"manager_employee_id\")\n",
    " .withColumn(\"end_date\", F.when(F.rand()<0.007, F.expr(\"date_add(start_date, -cast(rand()*90 + 1 as int))\")).otherwise(F.col(\"end_date\")))\n",
    ")\n",
    "\n",
    "proj_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"project\"])\n",
    "print(f\"Wrote: {TABLES['project']}  rows={spark.table(TABLES['project']).count():,}\")\n",
    "display(spark.table(TABLES[\"project\"]).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 7 — FACT: Timesheets (FKs to Employee & Project)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"BUILD: dqx.demo_timesheet\")\n",
    "\n",
    "n_ts = ROW_TARGETS[TABLES[\"timesheet\"]]\n",
    "emp_lu = spark.table(TABLES[\"employee\"]).select(\"employee_id\",\"employment_status\").withColumn(\"emp_idx\", F.row_number().over(Window.orderBy(\"employee_id\")) - 1)\n",
    "proj_lu = spark.table(TABLES[\"project\"]).select(\"project_id\",\"start_date\",\"end_date\").withColumn(\"proj_idx\", F.row_number().over(Window.orderBy(\"project_id\")) - 1)\n",
    "emp_count, proj_count = emp_lu.count(), proj_lu.count()\n",
    "\n",
    "ts_base = spark.range(n_ts).withColumn(\"_rn\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1)\n",
    "\n",
    "ts_df = (ts_base.select(\n",
    "    F.expr(\"uuid()\").alias(\"timesheet_id\"),\n",
    "    (F.col(\"_rn\") % emp_count).alias(\"emp_idx\"),\n",
    "    ((F.col(\"_rn\") * 13) % proj_count).alias(\"proj_idx\"),\n",
    "    rand_date(\"2024-01-01\",\"2025-08-10\",\"rand()\").alias(\"work_date\"),\n",
    "    (F.round(F.rand()*12,2)).cast(\"decimal(5,2)\").alias(\"hours_worked\"),\n",
    "    choice_expr([\"Billable\",\"NonBillable\",\"Admin\"], [0.70,0.25,0.05]).alias(\"work_type\"),\n",
    "    choice_expr([\"Workday\",\"Jira\",\"CSV\"], [0.70,0.20,0.10]).alias(\"source_system\"),\n",
    "    F.current_timestamp().alias(\"created_ts\")\n",
    ").join(emp_lu.select(\"employee_id\",\"emp_idx\"), \"emp_idx\", \"left\")\n",
    " .join(proj_lu.select(\"project_id\",\"proj_idx\",\"start_date\",\"end_date\"), \"proj_idx\", \"left\")\n",
    " .drop(\"emp_idx\",\"proj_idx\")\n",
    " .withColumn(\"work_date\", F.when(F.rand()<0.003, F.expr(\"date_add(current_date(), cast(rand()*5 + 1 as int))\")).otherwise(F.col(\"work_date\")))\n",
    " .withColumn(\"hours_worked\", F.when(F.rand()<0.003, F.when(F.rand()<0.5, F.lit(-1.0)).otherwise(F.lit(25.0))).otherwise(F.col(\"hours_worked\")))\n",
    ")\n",
    "\n",
    "# inject duplicates on (employee_id, project_id, work_date) (~0.6%)\n",
    "dupe_fraction = 0.006\n",
    "ts_df = ts_df.unionByName(ts_df.sample(withReplacement=True, fraction=dupe_fraction, seed=11))\n",
    "\n",
    "ts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"timesheet\"])\n",
    "print(f\"Wrote: {TABLES['timesheet']}  rows={spark.table(TABLES['timesheet']).count():,}\")\n",
    "display(spark.table(TABLES[\"timesheet\"]).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 8 — FACT: Expenses (FKs to Employee & Project)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"BUILD: dqx.demo_expense\")\n",
    "\n",
    "n_exp = ROW_TARGETS[TABLES[\"expense\"]]\n",
    "emp_lu = spark.table(TABLES[\"employee\"]).select(\"employee_id\",\"employment_status\").withColumn(\"emp_idx\", F.row_number().over(Window.orderBy(\"employee_id\")) - 1)\n",
    "proj_lu = spark.table(TABLES[\"project\"]).select(\"project_id\",\"start_date\",\"end_date\").withColumn(\"proj_idx\", F.row_number().over(Window.orderBy(\"project_id\")) - 1)\n",
    "emp_count, proj_count = emp_lu.count(), proj_lu.count()\n",
    "\n",
    "exp_base = spark.range(n_exp).withColumn(\"_rn\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1)\n",
    "\n",
    "exp_df = (exp_base.select(\n",
    "    F.expr(\"uuid()\").alias(\"expense_id\"),\n",
    "    (F.col(\"_rn\") % emp_count).alias(\"emp_idx\"),\n",
    "    F.when(F.rand()<0.25, F.lit(None)).otherwise(((F.col(\"_rn\") * 17) % proj_count)).alias(\"proj_idx\"),\n",
    "    rand_date(\"2024-01-01\",\"2025-08-10\",\"rand()\").alias(\"expense_date\"),\n",
    "    choice_expr([\"Meals\",\"Travel\",\"Supplies\",\"Software\",\"Other\"], [0.35,0.25,0.20,0.10,0.10]).alias(\"category\"),\n",
    "    (F.round((F.rand()**0.3) * 5000, 2)).cast(\"decimal(18,2)\").alias(\"amount\"),\n",
    "    choice_expr([\"USD\",\"CAD\",\"MXN\",\"GBP\",\"INR\"]).alias(\"currency_code\"),\n",
    "    choice_expr([\"Uber\",\"Lyft\",\"Delta\",\"AA\",\"Staples\",\"BestBuy\",\"Amazon\",\"LocalCafe\",\"HotelCo\",\"SoftwareCo\"]).alias(\"merchant\"),\n",
    "    (F.rand() > 0.08).alias(\"receipt_attached\"),\n",
    "    F.current_timestamp().alias(\"submission_ts\")\n",
    ").join(emp_lu.select(\"employee_id\",\"emp_idx\"), \"emp_idx\", \"left\")\n",
    " .join(proj_lu.select(\"project_id\",\"proj_idx\",\"start_date\",\"end_date\"), \"proj_idx\", \"left\")\n",
    " .drop(\"emp_idx\",\"proj_idx\")\n",
    " .withColumn(\"currency_code\", F.when(F.rand()<0.002, F.lit(\"XXX\")).otherwise(F.col(\"currency_code\")))\n",
    " .withColumn(\"receipt_attached\",\n",
    "    F.when((F.col(\"amount\") >= MEGASPEC[\"knobs\"][\"receipt_threshold\"]) & (F.rand() < 0.5), F.lit(False))\n",
    "     .otherwise(F.col(\"receipt_attached\")))\n",
    ")\n",
    "\n",
    "# build dup set on (employee_id, merchant, expense_date, amount) (~0.8%)\n",
    "dup_frac = 0.008\n",
    "dups = (exp_df.sample(withReplacement=True, fraction=dup_frac, seed=12)\n",
    "        .select(\"employee_id\",\"merchant\",\"expense_date\",\"amount\")\n",
    "        .withColumn(\"dup_key\", F.lit(1)))\n",
    "exp_df = (exp_df.join(dups.drop(\"dup_key\"), [\"employee_id\",\"merchant\",\"expense_date\",\"amount\"], \"left\")\n",
    "               .unionByName(exp_df.where(F.col(\"dup_key\").isNotNull()).drop(\"dup_key\"))\n",
    "               .withColumn(\"dup_key\", F.lit(None).cast(\"int\")))\n",
    "\n",
    "exp_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"expense\"])\n",
    "print(f\"Wrote: {TABLES['expense']}  rows={spark.table(TABLES['expense']).count():,}\")\n",
    "display(spark.table(TABLES[\"expense\"]).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 9 — Rules: default builder (used if your RULE_CLASSES are still stubs)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"RULES: load from classes or fall back to default demo rules\")\n",
    "\n",
    "def build_default_rules():\n",
    "    k = MEGASPEC[\"knobs\"]\n",
    "    valid_ccy = \",\".join([f\"'{c}'\" for c in k[\"valid_currency_codes\"]])\n",
    "    weekend_level = MEGASPEC[\"flags\"][\"allow_weekend_billable\"]\n",
    "    weekend_level = \"warn\" if weekend_level not in (\"warn\",\"error\") else weekend_level\n",
    "\n",
    "    return {\n",
    "      TABLES[\"employee\"]: [\n",
    "        {\"id\":\"EMP_TERM_DATE_MISSING\",\"level\":\"error\",\"message\":\"Terminated missing termination_date\",\n",
    "         \"expr\":\"employment_status = 'Terminated' AND termination_date IS NULL\"},\n",
    "        {\"id\":\"EMP_TERM_DATE_BEFORE_HIRE\",\"level\":\"error\",\"message\":\"termination_date < hire_date\",\n",
    "         \"expr\":\"employment_status = 'Terminated' AND termination_date < hire_date\"},\n",
    "        {\"id\":\"EMP_EMAIL_DOMAIN_WARN\",\"level\":\"warn\",\"message\":\"Non-company or missing email\",\n",
    "         \"expr\":\"work_email IS NULL OR NOT work_email LIKE '%@company.com'\"},\n",
    "      ],\n",
    "      TABLES[\"customer\"]: [\n",
    "        {\"id\":\"CUST_REG_DUP_ACTIVE\",\"level\":\"error\",\"message\":\"Duplicate registration_number among Active\",\n",
    "         \"expr\": f\"\"\"\n",
    "            registration_number IS NOT NULL AND status = 'Active' AND registration_number IN (\n",
    "              SELECT registration_number FROM {TABLES[\"customer\"]}\n",
    "              WHERE registration_number IS NOT NULL AND status = 'Active'\n",
    "              GROUP BY registration_number HAVING COUNT(*) > 1\n",
    "            )\n",
    "         \"\"\"},\n",
    "      ],\n",
    "      TABLES[\"project\"]: [\n",
    "        {\"id\":\"PROJ_END_BEFORE_START\",\"level\":\"error\",\"message\":\"end_date before start_date\",\n",
    "         \"expr\":\"end_date IS NOT NULL AND end_date < start_date\"},\n",
    "        {\"id\":\"PROJ_FK_CUSTOMER_MISSING\",\"level\":\"error\",\"message\":\"customer_id not found\",\n",
    "         \"expr\":f\"customer_id NOT IN (SELECT customer_id FROM {TABLES['customer']})\"},\n",
    "      ],\n",
    "      TABLES[\"timesheet\"]: [\n",
    "        {\"id\":\"TS_NEG_OR_GT24\",\"level\":\"error\",\"message\":\"Hours must be within 0..24\",\n",
    "         \"expr\":f\"hours_worked < 0 OR hours_worked > {k['max_hours_per_day_error']}\"},\n",
    "        {\"id\":\"TS_FUTURE_DATE\",\"level\":\"error\",\"message\":\"Work date in future\",\n",
    "         \"expr\":\"work_date > current_date()\"},\n",
    "        {\"id\":\"TS_EMP_STATUS_ERROR\",\"level\":\"error\",\"message\":\"Non-active employee\",\n",
    "         \"expr\":f\"employee_id IN (SELECT employee_id FROM {TABLES['employee']} WHERE employment_status <> 'Active')\"},\n",
    "        {\"id\":\"TS_DUP_EMP_PROJ_DAY\",\"level\":\"warn\",\"message\":\"Duplicate emp+proj+day\",\n",
    "         \"expr\":f\"\"\"\n",
    "           (employee_id, project_id, work_date) IN (\n",
    "             SELECT employee_id, project_id, work_date\n",
    "             FROM {TABLES['timesheet']}\n",
    "             GROUP BY employee_id, project_id, work_date\n",
    "             HAVING COUNT(*) > 1\n",
    "           )\n",
    "         \"\"\"},\n",
    "        {\"id\":\"TS_OUTSIDE_PROJECT_WINDOW\",\"level\":\"error\",\"message\":\"Work outside project window\",\n",
    "         \"expr\":f\"\"\"\n",
    "           work_date < (SELECT start_date FROM {TABLES['project']} p WHERE p.project_id = {TABLES['timesheet']}.project_id)\n",
    "           OR (\n",
    "              (SELECT end_date FROM {TABLES['project']} p2 WHERE p2.project_id = {TABLES['timesheet']}.project_id) IS NOT NULL\n",
    "              AND work_date >\n",
    "                  (SELECT end_date FROM {TABLES['project']} p3 WHERE p3.project_id = {TABLES['timesheet']}.project_id)\n",
    "           )\n",
    "         \"\"\"},\n",
    "        {\"id\":\"TS_WEEKEND_BILLABLE\",\"level\":weekend_level,\"message\":\"Billable hours on weekend\",\n",
    "         \"expr\":\"work_type = 'Billable' AND date_format(work_date, 'E') IN ('Sat','Sun')\"},\n",
    "      ],\n",
    "      TABLES[\"expense\"]: [\n",
    "        {\"id\":\"EXP_DUP\",\"level\":\"error\",\"message\":\"Duplicate expense\",\n",
    "         \"expr\":f\"\"\"\n",
    "           (employee_id, merchant, expense_date, amount) IN (\n",
    "             SELECT employee_id, merchant, expense_date, amount\n",
    "             FROM {TABLES['expense']}\n",
    "             GROUP BY employee_id, merchant, expense_date, amount\n",
    "             HAVING COUNT(*) > 1\n",
    "           )\n",
    "         \"\"\"},\n",
    "        {\"id\":\"EXP_NO_RECEIPT_OVER_T\",\"level\":\"error\",\"message\":\"Receipt required at/over threshold\",\n",
    "         \"expr\":f\"amount >= {k['receipt_threshold']} AND receipt_attached = false\"},\n",
    "        {\"id\":\"EXP_OOP\",\"level\":\"warn\",\"message\":\"Out-of-policy amount (Meals>limit or Travel>limit)\",\n",
    "         \"expr\":f\"(category = 'Meals' AND amount > {k['meal_limit']}) OR (category = 'Travel' AND amount > {k['travel_limit']})\"},\n",
    "        {\"id\":\"EXP_BAD_CCY\",\"level\":\"error\",\"message\":\"Invalid currency code\",\n",
    "         \"expr\":f\"currency_code NOT IN ({valid_ccy})\"},\n",
    "        {\"id\":\"EXP_EMP_STATUS_ERROR\",\"level\":\"error\",\"message\":\"Non-active employee\",\n",
    "         \"expr\":f\"employee_id IN (SELECT employee_id FROM {TABLES['employee']} WHERE employment_status <> 'Active')\"},\n",
    "        {\"id\":\"EXP_OUTSIDE_PROJECT_WINDOW\",\"level\":\"error\",\"message\":\"Expense outside project window\",\n",
    "         \"expr\":f\"\"\"\n",
    "           project_id IS NOT NULL AND (\n",
    "             expense_date < (SELECT start_date FROM {TABLES['project']} p WHERE p.project_id = {TABLES['expense']}.project_id)\n",
    "             OR (\n",
    "               (SELECT end_date FROM {TABLES['project']} p2 WHERE p2.project_id = {TABLES['expense']}.project_id) IS NOT NULL\n",
    "               AND expense_date >\n",
    "                   (SELECT end_date FROM {TABLES['project']} p3 WHERE p3.project_id = {TABLES['expense']}.project_id)\n",
    "             )\n",
    "           )\n",
    "         \"\"\"},\n",
    "      ]\n",
    "    }\n",
    "\n",
    "def load_rules():\n",
    "    rules_by_table = {}\n",
    "    for tbl, cls in RULE_CLASSES_BY_TABLE.items():\n",
    "        try:\n",
    "            rset = cls()\n",
    "            rules = rset.rules()  # should return list[dict]\n",
    "            rules_by_table[tbl] = rules or []\n",
    "        except Exception:\n",
    "            rules_by_table[tbl] = []\n",
    "    defaults = build_default_rules()\n",
    "    for tbl in defaults:\n",
    "        if tbl not in rules_by_table or not rules_by_table[tbl]:\n",
    "            rules_by_table[tbl] = defaults[tbl]\n",
    "    return rules_by_table\n",
    "\n",
    "RULES = load_rules()\n",
    "print(\"Rules loaded for:\")\n",
    "for k in TABLES[\"sources\"]:\n",
    "    print(\"  •\", k, f\"({len(RULES.get(k, []))} rules)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 10 — Rule Runner (append warning/error, write quarantine, keep clean rows)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"APPLY: Run rules per table, quarantine errors, keep warned rows\")\n",
    "\n",
    "def apply_rules_to_table(table_fullname: str, rules: list, run_id: str):\n",
    "    if not rules:\n",
    "        print(f\"[{table_fullname}] No rules; skipping.\")\n",
    "        return\n",
    "\n",
    "    errs  = [r for r in rules if r.get(\"level\",\"\").lower()==\"error\"]\n",
    "    warns = [r for r in rules if r.get(\"level\",\"\").lower()==\"warn\"]\n",
    "\n",
    "    def _make_array_expr(rule_list):\n",
    "        if not rule_list:\n",
    "            return \"array()\"\n",
    "        parts = [f\"IF({r['expr']}, '{r['id']}', NULL)\" for r in rule_list]\n",
    "        return f\"array_remove(array({', '.join(parts)}), NULL)\"\n",
    "\n",
    "    error_arr_sql = _make_array_expr(errs)\n",
    "    warn_arr_sql  = _make_array_expr(warns)\n",
    "\n",
    "    flagged_sql = f\"\"\"\n",
    "      SELECT\n",
    "        t.*,\n",
    "        {warn_arr_sql}  AS warning,\n",
    "        {error_arr_sql} AS error\n",
    "      FROM {table_fullname} t\n",
    "    \"\"\"\n",
    "    flagged = spark.sql(flagged_sql)\n",
    "\n",
    "    if errs:\n",
    "        err_map = spark.createDataFrame(\n",
    "            [(e[\"id\"], e.get(\"message\",\"\")) for e in errs],\n",
    "            \"rule_id STRING, message STRING\"\n",
    "        )\n",
    "        error_rows = flagged.where(F.size(F.col(\"error\")) > 0)\n",
    "        exploded = (error_rows.select(\n",
    "            F.lit(table_fullname).alias(\"_source_table\"),\n",
    "            F.explode(F.col(\"error\")).alias(\"_rule_id\"),\n",
    "            F.lit(\"ERROR\").alias(\"_level\"),\n",
    "            F.lit(run_id).alias(\"_run_id\"),\n",
    "            F.current_timestamp().alias(\"_event_ts\"),\n",
    "            F.to_json(F.struct([F.col(c) for c in flagged.columns])).alias(\"_row_payload_json\")\n",
    "        ).join(err_map, F.col(\"_rule_id\")==F.col(\"rule_id\"), \"left\")\n",
    "         .select(\"_source_table\",\"_rule_id\",\"_level\",F.col(\"message\").alias(\"_reason\"),\"_run_id\",\"_event_ts\",\"_row_payload_json\"))\n",
    "        exploded.write.format(\"delta\").mode(\"append\").saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "    cleaned = flagged.where(F.size(F.col(\"error\")) == 0)\n",
    "    cleaned.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(table_fullname)\n",
    "\n",
    "    total  = flagged.count()\n",
    "    err_ct = flagged.where(F.size(F.col(\"error\")) > 0).count()\n",
    "    warn_ct= cleaned.where(F.size(F.col(\"warning\")) > 0).count()\n",
    "    print(f\"[{table_fullname}] total={total:,}  quarantined={err_ct:,}  warned_on_kept={warn_ct:,}\")\n",
    "\n",
    "for tbl in TABLES[\"sources\"]:\n",
    "    apply_rules_to_table(tbl, RULES.get(tbl, []), run_id)\n",
    "\n",
    "print(\"Done applying rules.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 11 — Demo Views (summaries you can show)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"DEMO: Quarantine summary by rule\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT _rule_id, COUNT(*) AS hits\n",
    "FROM {QUARANTINE_TABLE}\n",
    "WHERE _run_id = '{run_id}'\n",
    "GROUP BY _rule_id\n",
    "ORDER BY hits DESC\n",
    "\"\"\"))\n",
    "\n",
    "banner(\"DEMO: Which tables had warnings?\")\n",
    "for tbl in TABLES[\"sources\"]:\n",
    "    df = spark.table(tbl)\n",
    "    if \"warning\" in df.columns:\n",
    "        cnt = df.where(F.size(F.col(\"warning\")) > 0).count()\n",
    "        print(f\"  {tbl}: {cnt:,} rows with warnings\")\n",
    "\n",
    "banner(\"DEMO: Sample warned rows (expenses)\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT expense_id, employee_id, category, amount, currency_code, receipt_attached, warning\n",
    "FROM {TABLES['expense']}\n",
    "WHERE size(warning) > 0\n",
    "LIMIT 20\n",
    "\"\"\"))\n",
    "\n",
    "banner(\"DEMO: Row counts after quarantine\")\n",
    "display(spark.sql(f\"SHOW TABLES IN {DQX_SCHEMA}\"))\n",
    "for tbl in TABLES[\"sources\"]:\n",
    "    print(tbl, spark.table(tbl).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "166e836e-8021-41ff-adc1-a66290aa30cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 12 — Plant deterministic demo records (the \"smoking guns\")\n",
    "# =====================================================================================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "banner(\"PLANT: deterministic showcase rows & updates\")\n",
    "\n",
    "# -- 12.1 Make sure specific IDs exist (they do, given our generators):\n",
    "# Employees E1001..E2999, Projects P10001..P10600, Customers C5001..C5999\n",
    "\n",
    "# -- 12.2 Ensure E1099 is Terminated (to trigger EMP_STATUS rules)\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLES['employee']}\n",
    "SET employment_status = 'Terminated',\n",
    "    termination_date  = '2024-06-15'\n",
    "WHERE employee_id = 'E1099'\n",
    "\"\"\")\n",
    "print(\"Updated E1099 -> Terminated.\")\n",
    "\n",
    "# -- 12.3 Force a project window end to use in 'outside window' cases\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLES['project']}\n",
    "SET start_date = '2024-10-01',\n",
    "    end_date   = '2025-02-28',\n",
    "    status     = 'Closed'\n",
    "WHERE project_id = 'P10037'\n",
    "\"\"\")\n",
    "print(\"Updated P10037 -> Closed, end=2025-02-28.\")\n",
    "\n",
    "# -- 12.4 Duplicate Active customer registration numbers\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLES['customer']}\n",
    "SET status='Active', registration_number='RN-00001234'\n",
    "WHERE customer_id IN ('C5009','C5017')\n",
    "\"\"\")\n",
    "print(\"Set C5009/C5017 to Active with same registration_number.\")\n",
    "\n",
    "# -- 12.5 Insert planted TIMESHEETS\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {TABLES['timesheet']} (timesheet_id, employee_id, project_id, work_date, hours_worked,\n",
    "                                    work_type, source_system, created_ts)\n",
    "VALUES\n",
    "  ('TS_DEMO_001','E1015','P10012','2025-07-10',25.00,'Billable','CSV',current_timestamp()),    -- TS_NEG_OR_GT24\n",
    "  ('TS_DEMO_002','E1044','P10012','2025-07-13', 8.00,'Billable','CSV',current_timestamp()),    -- TS_WEEKEND_BILLABLE (Sun)\n",
    "  ('TS_DEMO_003','E1010','P10012', date_add(current_date(), 3), 2.00,'Admin','CSV',current_timestamp()), -- TS_FUTURE_DATE\n",
    "  ('TS_DEMO_004','E1099','P10012','2025-06-20', 3.50,'Billable','CSV',current_timestamp()),    -- TS_EMP_STATUS_ERROR\n",
    "  ('TS_DEMO_005','E1010','P10037','2025-07-01', 6.00,'Billable','CSV',current_timestamp()),    -- TS_OUTSIDE_PROJECT_WINDOW\n",
    "  ('TS_DEMO_006','E1033','P10012','2025-06-05', 4.00,'Billable','CSV',current_timestamp()),    -- TS_DUP pair A\n",
    "  ('TS_DEMO_007','E1033','P10012','2025-06-05', 4.00,'Billable','CSV',current_timestamp())     -- TS_DUP pair B\n",
    "\"\"\")\n",
    "print(\"Inserted planted timesheets (TS_DEMO_001..007).\")\n",
    "\n",
    "# -- 12.6 Insert planted EXPENSES\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {TABLES['expense']} (expense_id, employee_id, project_id, expense_date, category, amount,\n",
    "                                  currency_code, merchant, receipt_attached, submission_ts)\n",
    "VALUES\n",
    "  ('EX_DEMO_001','E1010','P10012','2025-07-15','Travel',399.99,'USD','HotelCo',true,current_timestamp()), -- EXP_DUP A\n",
    "  ('EX_DEMO_002','E1010','P10012','2025-07-15','Travel',399.99,'USD','HotelCo',true,current_timestamp()), -- EXP_DUP B\n",
    "  ('EX_DEMO_003','E1011','P10012','2025-07-16','Meals',220.00,'USD','LocalCafe',false,current_timestamp()), -- EXP_NO_RECEIPT_OVER_T\n",
    "  ('EX_DEMO_004','E1012','P10012','2025-07-16','Supplies',42.00,'XXX','Staples',true,current_timestamp()),  -- EXP_BAD_CCY\n",
    "  ('EX_DEMO_005','E1099','P10012','2025-07-10','Other',50.00,'USD','Amazon',true,current_timestamp()),      -- EXP_EMP_STATUS_ERROR\n",
    "  ('EX_DEMO_006','E1013','P10012','2025-07-18','Travel',799.00,'USD','Delta',true,current_timestamp()),     -- EXP_OOP (warn)\n",
    "  ('EX_DEMO_007','E1014','P10037','2025-07-20','Software',123.45,'USD','SoftwareCo',true,current_timestamp()) -- EXP_OUTSIDE_PROJECT_WINDOW\n",
    "\"\"\")\n",
    "print(\"Inserted planted expenses (EX_DEMO_001..007).\")\n",
    "\n",
    "print(\"Planting complete.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 13 — Re-apply rules to reflect planted rows (quarantine + warnings)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"RE-APPLY: Apply rules to impacted tables after planting\")\n",
    "\n",
    "# Re-load RULES in case you edited rule classes\n",
    "RULES = load_rules()\n",
    "\n",
    "# Re-apply to the specific tables we touched\n",
    "for tbl in [TABLES[\"employee\"], TABLES[\"customer\"], TABLES[\"project\"], TABLES[\"timesheet\"], TABLES[\"expense\"]]:\n",
    "    apply_rules_to_table(tbl, RULES.get(tbl, []), run_id)\n",
    "\n",
    "print(\"Re-apply complete.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# =====================================================================================\n",
    "# CELL 14 — Dashboard Views (build once, then wire to Databricks SQL dashboard)\n",
    "# =====================================================================================\n",
    "\n",
    "banner(\"DASHBOARD: Create/refresh views for the demo\")\n",
    "\n",
    "# 14.1 Quarantine by rule (for this run)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_quarantine_by_rule AS\n",
    "SELECT _rule_id, COUNT(*) AS hits\n",
    "FROM {QUARANTINE_TABLE}\n",
    "WHERE _run_id = '{run_id}'\n",
    "GROUP BY _rule_id\n",
    "ORDER BY hits DESC\n",
    "\"\"\")\n",
    "\n",
    "# 14.2 Quarantine detail (drill)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_quarantine_detail AS\n",
    "SELECT\n",
    "  _source_table,\n",
    "  _rule_id,\n",
    "  _level,\n",
    "  _reason,\n",
    "  _run_id,\n",
    "  _event_ts,\n",
    "  _row_payload_json\n",
    "FROM {QUARANTINE_TABLE}\n",
    "WHERE _run_id = '{run_id}'\n",
    "ORDER BY _event_ts DESC\n",
    "\"\"\")\n",
    "\n",
    "# 14.3 Warnings by table (rows that passed but were marked)\n",
    "warn_counts = []\n",
    "for tbl in TABLES[\"sources\"]:\n",
    "    if \"warning\" in spark.table(tbl).columns:\n",
    "        cnt = spark.table(tbl).where(F.size(F.col(\"warning\")) > 0).count()\n",
    "        warn_counts.append((tbl, cnt))\n",
    "warn_df = spark.createDataFrame(warn_counts, \"table STRING, warn_rows BIGINT\")\n",
    "warn_df.createOrReplaceTempView(\"v_warning_counts\")\n",
    "\n",
    "# 14.4 Top entities to triage\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_top_employees_quarantined AS\n",
    "SELECT\n",
    "  payload.employee_id AS employee_id,\n",
    "  COUNT(*) AS error_hits\n",
    "FROM (\n",
    "  SELECT from_json(_row_payload_json, schema_of_json(_row_payload_json)) AS payload\n",
    "  FROM {QUARANTINE_TABLE}\n",
    "  WHERE _run_id = '{run_id}'\n",
    ") q\n",
    "GROUP BY payload.employee_id\n",
    "ORDER BY error_hits DESC\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_top_merchants_quarantined AS\n",
    "SELECT\n",
    "  payload.merchant AS merchant,\n",
    "  COUNT(*) AS error_hits\n",
    "FROM (\n",
    "  SELECT from_json(_row_payload_json, schema_of_json(_row_payload_json)) AS payload\n",
    "  FROM {QUARANTINE_TABLE}\n",
    "  WHERE _run_id = '{run_id}'\n",
    ") q\n",
    "WHERE payload.merchant IS NOT NULL\n",
    "GROUP BY payload.merchant\n",
    "ORDER BY error_hits DESC\n",
    "\"\"\")\n",
    "\n",
    "# 14.5 Handy spotlight views for the planted cases\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_demo_cases AS\n",
    "SELECT\n",
    "  _rule_id,\n",
    "  _source_table,\n",
    "  _reason,\n",
    "  _event_ts,\n",
    "  get_json_object(_row_payload_json, '$.employee_id')    AS employee_id,\n",
    "  get_json_object(_row_payload_json, '$.project_id')     AS project_id,\n",
    "  get_json_object(_row_payload_json, '$.work_date')      AS work_date,\n",
    "  get_json_object(_row_payload_json, '$.hours_worked')   AS hours_worked,\n",
    "  get_json_object(_row_payload_json, '$.expense_date')   AS expense_date,\n",
    "  get_json_object(_row_payload_json, '$.category')       AS category,\n",
    "  get_json_object(_row_payload_json, '$.merchant')       AS merchant,\n",
    "  get_json_object(_row_payload_json, '$.amount')         AS amount\n",
    "FROM {QUARANTINE_TABLE}\n",
    "WHERE _run_id = '{run_id}'\n",
    "  AND (\n",
    "        (_rule_id IN ('EXP_DUP','EXP_NO_RECEIPT_OVER_T','EXP_BAD_CCY','EXP_EMP_STATUS_ERROR','EXP_OUTSIDE_PROJECT_WINDOW'))\n",
    "     OR (_rule_id IN ('TS_NEG_OR_GT24','TS_FUTURE_DATE','TS_OUTSIDE_PROJECT_WINDOW','TS_EMP_STATUS_ERROR'))\n",
    "     OR (_rule_id = 'CUST_REG_DUP_ACTIVE')\n",
    "  )\n",
    "ORDER BY _event_ts DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Views created:\")\n",
    "print(\"  - v_quarantine_by_rule\")\n",
    "print(\"  - v_quarantine_detail\")\n",
    "print(\"  - v_warning_counts\")\n",
    "print(\"  - v_top_employees_quarantined\")\n",
    "print(\"  - v_top_merchants_quarantined\")\n",
    "print(\"  - v_demo_cases\")\n",
    "\n",
    "banner(\"DASHBOARD QUICK LOOK\")\n",
    "print(\"Quarantine by rule:\")\n",
    "display(spark.sql(\"SELECT * FROM v_quarantine_by_rule\"))\n",
    "\n",
    "print(\"Warnings by table:\")\n",
    "display(spark.sql(\"SELECT * FROM v_warning_counts ORDER BY warn_rows DESC\"))\n",
    "\n",
    "print(\"Spotlight on our planted cases:\")\n",
    "display(spark.sql(\"SELECT * FROM v_demo_cases LIMIT 20\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dqx_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
