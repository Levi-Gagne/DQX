{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166e836e-8021-41ff-adc1-a66290aa30cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Red background + logo + title (wider and taller)\n",
    "RED   = \"#FF0000\"\n",
    "WHITE = \"#FFFFFF\"\n",
    "title = \"DQX Demo\"\n",
    "\n",
    "# Load the logo (no SVG fallback)\n",
    "import base64, mimetypes, os\n",
    "\n",
    "logo_path = \"utils/cla_logo_white.png\"\n",
    "logo_html = \"\"\n",
    "try:\n",
    "    p = logo_path\n",
    "    if p.startswith(\"dbfs:/\"):\n",
    "        p = \"/dbfs\" + p[5:]\n",
    "    with open(p, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    mime = mimetypes.guess_type(p)[0] or \"image/png\"\n",
    "    logo_uri = f\"data:{mime};base64,{b64}\"\n",
    "    logo_html = f\"<img src='{logo_uri}' alt='Logo' style='height:48px; width:auto; display:block;'/>\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<div style=\"\n",
    "  background:{RED} !important;\n",
    "  padding: 28px 48px;  /* Increased padding for more height and width */\n",
    "  border-radius: 16px; /* Slightly larger rounded corners */\n",
    "  color: {WHITE};\n",
    "  text-align:center;\n",
    "  margin: 12px 0 20px 0;\n",
    "  max-width: 100%; /* Full width */\n",
    "  display: flex;\n",
    "  justify-content: center;\">\n",
    "  <span style=\"display:inline-flex; align-items:center; gap:20px;\">\n",
    "    {logo_html}\n",
    "    <span style=\"font-weight:800; font-size:36px; letter-spacing:.4px; font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;\">\n",
    "      {title}\n",
    "    </span>\n",
    "  </span>\n",
    "</div>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155eb0a8-86f0-48cd-ae4d-20f0dfa077f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0\n",
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2cdf8d-ffa0-48a3-bdfb-8198a7fd21f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97e3af7-63d7-4663-aba5-c3065dfe4f19",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo Config"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# CELL 1 — demo_spec.py  (Catalog/schema, table names, schemas, MEGASPEC)\n",
    "# =====================================================================================\n",
    "from utils.color import Color as C\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, BooleanType, DateType, TimestampType, DecimalType\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Globals / conventions (Unity Catalog: CATALOG.SCHEMA.TABLE)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "DQX_CATALOG        = \"dq_dev\"\n",
    "DQX_SCHEMA         = \"demo\"\n",
    "DQX_DB             = f\"{DQX_CATALOG}.{DQX_SCHEMA}\"           # catalog.schema\n",
    "QUARANTINE_TABLE   = f\"{DQX_DB}.demo_quarantine\"             # sink for ERROR rows\n",
    "# DQX adds _warning and _error by default; we'll stick with those.\n",
    "DQ_RESULT_WARN_COL = \"_warning\"\n",
    "DQ_RESULT_ERR_COL  = \"_error\"\n",
    "\n",
    "ROW_TARGETS = {\n",
    "    f\"{DQX_DB}.demo_employee\":   2_000,\n",
    "    f\"{DQX_DB}.demo_customer\":   1_000,\n",
    "    f\"{DQX_DB}.demo_project\":      600,\n",
    "    f\"{DQX_DB}.demo_timesheet\": 350_000,\n",
    "    f\"{DQX_DB}.demo_expense\":   120_000,\n",
    "}\n",
    "\n",
    "# Canonical table names (full paths) + groupings\n",
    "TABLES = {\n",
    "    \"catalog\":    DQX_CATALOG,\n",
    "    \"schema\":     DQX_DB,   # fully-qualified (catalog.schema)\n",
    "    \"employee\":   f\"{DQX_DB}.demo_employee\",\n",
    "    \"customer\":   f\"{DQX_DB}.demo_customer\",\n",
    "    \"project\":    f\"{DQX_DB}.demo_project\",\n",
    "    \"timesheet\":  f\"{DQX_DB}.demo_timesheet\",\n",
    "    \"expense\":    f\"{DQX_DB}.demo_expense\",\n",
    "    \"quarantine\": QUARANTINE_TABLE,\n",
    "    # Groupings\n",
    "    \"sources\": [\n",
    "        f\"{DQX_DB}.demo_employee\",\n",
    "        f\"{DQX_DB}.demo_customer\",\n",
    "        f\"{DQX_DB}.demo_project\",\n",
    "        f\"{DQX_DB}.demo_timesheet\",\n",
    "        f\"{DQX_DB}.demo_expense\",\n",
    "    ],\n",
    "    \"facts\": [\n",
    "        f\"{DQX_DB}.demo_timesheet\",\n",
    "        f\"{DQX_DB}.demo_expense\",\n",
    "    ],\n",
    "    \"dims\": [\n",
    "        f\"{DQX_DB}.demo_employee\",\n",
    "        f\"{DQX_DB}.demo_customer\",\n",
    "        f\"{DQX_DB}.demo_project\",\n",
    "    ],\n",
    "    \"all\": [\n",
    "        f\"{DQX_DB}.demo_employee\",\n",
    "        f\"{DQX_DB}.demo_customer\",\n",
    "        f\"{DQX_DB}.demo_project\",\n",
    "        f\"{DQX_DB}.demo_timesheet\",\n",
    "        f\"{DQX_DB}.demo_expense\",\n",
    "        QUARANTINE_TABLE,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Spark Structured Schemas\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "demo_employee_schema = StructType([\n",
    "    StructField(\"employee_id\",        StringType(),  False),\n",
    "    StructField(\"full_name\",          StringType(),  False),\n",
    "    StructField(\"department\",         StringType(),  False),  # Consulting, Audit, Tax, IT, Ops\n",
    "    StructField(\"role\",               StringType(),  False),  # Engineer, Analyst, Consultant, Manager, Support\n",
    "    StructField(\"cost_center\",        StringType(),  True),   # CC-####\n",
    "    StructField(\"employment_status\",  StringType(),  False),  # Active, Leave, Terminated\n",
    "    StructField(\"hire_date\",          DateType(),    False),\n",
    "    StructField(\"termination_date\",   DateType(),    True),\n",
    "    StructField(\"work_email\",         StringType(),  True),\n",
    "    StructField(\"country_code\",       StringType(),  True),   # US, CA, MX, GB, IN\n",
    "])\n",
    "\n",
    "demo_customer_schema = StructType([\n",
    "    StructField(\"customer_id\",            StringType(),  False),\n",
    "    StructField(\"customer_name\",          StringType(),  False),\n",
    "    StructField(\"industry\",               StringType(),  False),\n",
    "    StructField(\"country_code\",           StringType(),  True),   # ISO-3166 alpha-2\n",
    "    StructField(\"status\",                 StringType(),  False),  # Active, Prospect, Inactive\n",
    "    StructField(\"onboarding_date\",        DateType(),    False),\n",
    "    StructField(\"primary_contact_email\",  StringType(),  True),\n",
    "    StructField(\"registration_number\",    StringType(),  True),   # national reg / tax-like id\n",
    "])\n",
    "\n",
    "demo_project_schema = StructType([\n",
    "    StructField(\"project_id\",          StringType(),       False),\n",
    "    StructField(\"customer_id\",         StringType(),       False),  # FK -> customer\n",
    "    StructField(\"project_name\",        StringType(),       False),\n",
    "    StructField(\"status\",              StringType(),       False),  # Planned, Active, OnHold, Closed\n",
    "    StructField(\"start_date\",          DateType(),         False),\n",
    "    StructField(\"end_date\",            DateType(),         True),\n",
    "    StructField(\"manager_employee_id\", StringType(),       True),   # FK -> employee\n",
    "    StructField(\"budget_amount\",       DecimalType(18, 2), True),\n",
    "    StructField(\"billing_model\",       StringType(),       False),  # T&M, Fixed, Retainer\n",
    "])\n",
    "\n",
    "demo_timesheet_schema = StructType([\n",
    "    StructField(\"timesheet_id\",  StringType(),       False),\n",
    "    StructField(\"employee_id\",   StringType(),       False),  # FK -> employee\n",
    "    StructField(\"project_id\",    StringType(),       False),  # FK -> project\n",
    "    StructField(\"work_date\",     DateType(),         False),\n",
    "    StructField(\"hours_worked\",  DecimalType(5, 2),  False),  # 0–24\n",
    "    StructField(\"work_type\",     StringType(),       False),  # Billable, NonBillable, Admin\n",
    "    StructField(\"source_system\", StringType(),       False),  # Workday, Jira, CSV\n",
    "    StructField(\"created_ts\",    TimestampType(),    False),\n",
    "])\n",
    "\n",
    "demo_expense_schema = StructType([\n",
    "    StructField(\"expense_id\",       StringType(),       False),\n",
    "    StructField(\"employee_id\",      StringType(),       False),  # FK -> employee\n",
    "    StructField(\"project_id\",       StringType(),       True),   # FK -> project (nullable)\n",
    "    StructField(\"expense_date\",     DateType(),         False),\n",
    "    StructField(\"category\",         StringType(),       False),  # Meals, Travel, Supplies, Software, Other\n",
    "    StructField(\"amount\",           DecimalType(18, 2), False),\n",
    "    StructField(\"currency_code\",    StringType(),       False),  # ISO-4217\n",
    "    StructField(\"merchant\",         StringType(),       True),\n",
    "    StructField(\"receipt_attached\", BooleanType(),      False),\n",
    "    StructField(\"submission_ts\",    TimestampType(),    False),\n",
    "])\n",
    "\n",
    "SCHEMAS_BY_TABLE = {\n",
    "    TABLES[\"employee\"]:  demo_employee_schema,\n",
    "    TABLES[\"customer\"]:  demo_customer_schema,\n",
    "    TABLES[\"project\"]:   demo_project_schema,\n",
    "    TABLES[\"timesheet\"]: demo_timesheet_schema,\n",
    "    TABLES[\"expense\"]:   demo_expense_schema,\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# MEGASPEC (knobs & flags used by our checks)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "MEGASPEC = {\n",
    "    \"schema\": DQX_DB,\n",
    "    \"quarantine_table\": QUARANTINE_TABLE,\n",
    "    \"flags\": {\n",
    "        \"allow_weekend_billable\": \"warn\",            # 'warn' | 'error' | 'off'\n",
    "    },\n",
    "    \"knobs\": {\n",
    "        \"receipt_threshold\": 75.00,                  # >= threshold requires receipt\n",
    "        \"meal_limit\": 150.00,                        # warn if Meals > limit\n",
    "        \"travel_limit\": 500.00,                      # warn if Travel > limit\n",
    "        \"max_hours_per_day_error\": 24.0,             # error if > this\n",
    "        \"hi_hours_warn\": 12.0,                       # warn if > this\n",
    "        \"valid_country_codes\": [\"US\",\"CA\",\"MX\",\"GB\",\"IN\"],\n",
    "        \"valid_currency_codes\": [\"USD\",\"CAD\",\"MXN\",\"GBP\",\"INR\"],\n",
    "        \"valid_emp_status\": [\"Active\",\"Leave\",\"Terminated\"],\n",
    "        \"valid_proj_status\": [\"Planned\",\"Active\",\"OnHold\",\"Closed\"],\n",
    "        \"valid_billing_models\": [\"T&M\",\"Fixed\",\"Retainer\"],\n",
    "        \"timeliness_hours_expense_max\": 240,         # submission_ts within 10 days\n",
    "        \"timeliness_hours_timesheet_max\": 72,        # created_ts within 3 days\n",
    "        \"unusual_multiplier\": 1.5                    # 50% higher than avg → warn\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n[SPEC LOADED] schema={DQX_DB}  sources={len(TABLES['sources'])}  quarantine={TABLES['quarantine']}\")\n",
    "print(\"Row targets: \" + \", \".join([f\"{k.split('.')[-1]}={v:,}\" for k,v in ROW_TARGETS.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c828749-7835-4817-b8d2-b596c1540bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b7ce6a-8c98-459f-adfd-1fba5cc79554",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remove Tables"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all DQX tables with full catalog.schema.table names\n",
    "tables_to_drop = [\n",
    "    f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_employee\",\n",
    "    f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_customer\",\n",
    "    f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_project\",\n",
    "    f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_timesheet\",\n",
    "    f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_expense\",\n",
    "    f\"{DQX_CATALOG}.{DQX_SCHEMA}.demo_quarantine\"\n",
    "]\n",
    "\n",
    "for t in tables_to_drop:\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {t}\")\n",
    "        print(f\"✅ Dropped (if existed): {t}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped: {t} -> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb70a49-2f30-437f-b183-6cd3fd1f1ca9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Δ Quarantine Table"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# CELL 2 — Setup (UC catalog/schema, quarantine table, banner)\n",
    "# =====================================================================================\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from uuid import uuid4\n",
    "from datetime import date, datetime, timedelta\n",
    "from decimal import Decimal\n",
    "\n",
    "def banner(msg, color=\"neon_blue\"):\n",
    "    col = getattr(C, color, \"\")\n",
    "    print(\"\\n\" + \"═\"*92)\n",
    "    print(f\"{C.b}{col} {msg}{C.r}\")\n",
    "    print(\"═\"*92)\n",
    "\n",
    "banner(\"SETUP: Use catalog/schema and create quarantine table\", \"aqua_blue\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {DQX_CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {DQX_DB}\")\n",
    "spark.sql(f\"USE {DQX_DB}\")\n",
    "\n",
    "run_id = str(uuid4())\n",
    "print(f\"{C.ivory}Using: catalog={DQX_CATALOG}  schema={DQX_SCHEMA}  (db={DQX_DB}){C.r}\")\n",
    "print(f\"{C.ivory}Run ID: {C.bubblegum_pink}{run_id}{C.r}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {QUARANTINE_TABLE} (\n",
    "  _source_table     STRING,\n",
    "  _rule_id          STRING,\n",
    "  _level            STRING,\n",
    "  _reason           STRING,\n",
    "  _run_id           STRING,\n",
    "  _event_ts         TIMESTAMP,\n",
    "  _row_payload_json STRING\n",
    ") USING delta\n",
    "\"\"\")\n",
    "print(f\"{C.ivory}Quarantine table ready: {C.golden_yellow}{QUARANTINE_TABLE}{C.r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31bb8f6b-e71d-4a93-af7b-04466fc8a82e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Δ Source Tables"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# CELLS 3–7 — DataGen for all tables (deterministic demo rows included; avoid random \"bad\" noise)\n",
    "# =====================================================================================\n",
    "from dbldatagen import DataGenerator\n",
    "from utils.display import show_df  # << use your helper for nice display\n",
    "from datetime import date, datetime, timedelta\n",
    "from decimal import Decimal\n",
    "from uuid import uuid4\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# CELL 3 — Employees\n",
    "# -------------------------------------------------------------------------------------\n",
    "banner(f\"BUILD (Datagen): {TABLES['employee']}\", \"sky_blue\")\n",
    "n_emp = ROW_TARGETS[TABLES[\"employee\"]]\n",
    "\n",
    "emp_gen = (DataGenerator(spark, name=\"demo_employee_gen\", rows=n_emp, partitions=8)\n",
    "           .withIdOutput()\n",
    "           .withColumn(\"employee_id\", \"string\", expr=\"concat('E', lpad(cast(id + 1001 as string), 4, '0'))\")\n",
    "           .withColumn(\"full_name\", \"string\", expr=\"concat('Emp ', cast(id as string))\")\n",
    "           .withColumn(\"department\", values=[\"Consulting\",\"Audit\",\"Tax\",\"IT\",\"Ops\"], weights=[0.35,0.20,0.20,0.15,0.10])\n",
    "           .withColumn(\"role\", values=[\"Engineer\",\"Analyst\",\"Consultant\",\"Manager\",\"Support\"])\n",
    "           .withColumn(\"cost_center\", \"string\", expr=\"concat('CC-', lpad(cast(cast(rand()*10000 as int) as string), 4, '0'))\", percentNulls=0.02)\n",
    "           .withColumn(\"employment_status\", values=MEGASPEC[\"knobs\"][\"valid_emp_status\"], weights=[0.90,0.03,0.07])\n",
    "           .withColumn(\"hire_date\", \"date\", minValue=\"2018-01-01\", maxValue=\"2025-08-01\")\n",
    "           .withColumn(\"termination_date\", \"date\", minValue=\"2018-02-01\", maxValue=\"2030-12-31\", percentNulls=1.0)\n",
    "           .withColumn(\"work_email\", \"string\", expr=\"concat('emp', cast(id as string), '@company.com')\")\n",
    "           .withColumn(\"country_code\", values=MEGASPEC[\"knobs\"][\"valid_country_codes\"])\n",
    ")\n",
    "emp_df = emp_gen.build().drop(\"id\")\n",
    "\n",
    "# Deterministic demo employees (specific rule hits)\n",
    "demo_rows_emp = [\n",
    "    (\"E9999\", \"Terminated_NoEnd\", \"Consulting\", \"Analyst\", \"CC-0001\",\n",
    "     \"Terminated\", date(2023,1,15), None, \"emp9999@company.com\", \"US\"),\n",
    "    (\"E9998\", \"Terminated_BadEnd\", \"Audit\", \"Engineer\", \"CC-0002\",\n",
    "     \"Terminated\", date(2023,6,1), date(2023,5,15), \"emp9998@company.com\", \"US\"),\n",
    "    (\"E9997\", \"ExtEmail_InvalidCC\", \"IT\", \"Manager\", \"CC-0003\",\n",
    "     \"Active\", date(2021,3,1), None, \"someone@gmail.com\", \"XX\"),\n",
    "]\n",
    "emp_demo_df = spark.createDataFrame(demo_rows_emp, schema=SCHEMAS_BY_TABLE[TABLES[\"employee\"]])\n",
    "emp_df = emp_df.unionByName(emp_demo_df)\n",
    "\n",
    "emp_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"employee\"])\n",
    "print(f\"{C.ivory}Wrote: {C.golden_yellow}{TABLES['employee']}{C.r}  rows={spark.table(TABLES['employee']).count():,}\")\n",
    "show_df(spark.table(TABLES[\"employee\"]), n=20, truncate=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# CELL 4 — Customers\n",
    "# -------------------------------------------------------------------------------------\n",
    "banner(f\"BUILD (Datagen): {TABLES['customer']}\", \"sky_blue\")\n",
    "n_cust = ROW_TARGETS[TABLES[\"customer\"]]\n",
    "\n",
    "cust_gen = (DataGenerator(spark, name=\"demo_customer_gen\", rows=n_cust, partitions=8)\n",
    "            .withIdOutput()\n",
    "            .withColumn(\"customer_id\", \"string\", expr=\"concat('C', lpad(cast(id + 5001 as string), 4, '0'))\")\n",
    "            .withColumn(\"customer_name\", \"string\", expr=\"concat('Customer ', cast(id as string))\")\n",
    "            .withColumn(\"industry\", values=[\"Technology\",\"Healthcare\",\"Finance\",\"Retail\",\"Manufacturing\"], weights=[0.30,0.20,0.20,0.20,0.10])\n",
    "            .withColumn(\"country_code\", values=MEGASPEC[\"knobs\"][\"valid_country_codes\"])\n",
    "            .withColumn(\"status\", values=[\"Active\",\"Prospect\",\"Inactive\"], weights=[0.75,0.15,0.10])\n",
    "            .withColumn(\"onboarding_date\", \"date\", minValue=\"2019-01-01\", maxValue=\"2025-08-01\")\n",
    "            .withColumn(\"primary_contact_email\", \"string\", expr=\"concat('contact', cast(id as string), '@example.com')\")\n",
    "            .withColumn(\"registration_number\", \"string\", expr=\"concat('RN-', lpad(cast(cast(rand()*100000000 as int) as string), 8, '0'))\")\n",
    ")\n",
    "cust_df = cust_gen.build().drop(\"id\")\n",
    "\n",
    "# Deterministic dup registration among Active\n",
    "demo_rows_cust = [\n",
    "    (\"C9999\", \"DupReg A\", \"Technology\", \"US\", \"Active\",  date(2024,1,1), \"a@example.com\", \"RN-00001234\"),\n",
    "    (\"C9998\", \"DupReg B\", \"Technology\", \"US\", \"Active\",  date(2024,1,2), \"b@example.com\", \"RN-00001234\"),\n",
    "]\n",
    "cust_demo_df = spark.createDataFrame(demo_rows_cust, schema=SCHEMAS_BY_TABLE[TABLES[\"customer\"]])\n",
    "cust_df = cust_df.unionByName(cust_demo_df)\n",
    "\n",
    "cust_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"customer\"])\n",
    "print(f\"{C.ivory}Wrote: {C.golden_yellow}{TABLES['customer']}{C.r}  rows={spark.table(TABLES['customer']).count():,}\")\n",
    "show_df(spark.table(TABLES[\"customer\"]), n=20, truncate=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# CELL 5 — Projects\n",
    "# -------------------------------------------------------------------------------------\n",
    "banner(f\"BUILD (Datagen): {TABLES['project']}\", \"sky_blue\")\n",
    "n_proj = ROW_TARGETS[TABLES[\"project\"]]\n",
    "\n",
    "emp_ids  = [r[0] for r in spark.table(TABLES[\"employee\"]).select(\"employee_id\").collect()]\n",
    "cust_ids = [r[0] for r in spark.table(TABLES[\"customer\"]).select(\"customer_id\").collect()]\n",
    "mgr_id   = emp_ids[0] if emp_ids else None\n",
    "\n",
    "proj_gen = (DataGenerator(spark, name=\"demo_project_gen\", rows=n_proj, partitions=8)\n",
    "            .withIdOutput()\n",
    "            .withColumn(\"project_id\", \"string\", expr=\"concat('P', lpad(cast(id + 10001 as string), 5, '0'))\")\n",
    "            .withColumn(\"customer_id\", values=cust_ids)\n",
    "            .withColumn(\"project_name\", \"string\", expr=\"concat('Project ', cast(id as string))\")\n",
    "            .withColumn(\"status\", values=MEGASPEC[\"knobs\"][\"valid_proj_status\"], weights=[0.15,0.55,0.10,0.20])\n",
    "            .withColumn(\"start_date\", \"date\", minValue=\"2020-01-01\", maxValue=\"2025-03-01\")\n",
    "            .withColumn(\"end_date\", \"date\", minValue=\"2020-02-01\", maxValue=\"2028-12-31\", percentNulls=0.35)\n",
    "            .withColumn(\"manager_employee_id\", values=emp_ids, percentNulls=0.03)\n",
    "            .withColumn(\"budget_amount\", \"decimal(18,2)\", minValue=10000, maxValue=2000000)\n",
    "            .withColumn(\"billing_model\", values=MEGASPEC[\"knobs\"][\"valid_billing_models\"], weights=[0.55,0.35,0.10])\n",
    ")\n",
    "proj_df = proj_gen.build().drop(\"id\")\n",
    "\n",
    "# Deterministic demo projects (specific rule hits)\n",
    "demo_rows_proj = [\n",
    "    (\"P99998\", cust_ids[0] if cust_ids else \"C9999\", \"Demo_EndBeforeStart\", \"Active\",\n",
    "     date(2024,5,1), date(2024,4,15), mgr_id, Decimal(\"250000.00\"), \"T&M\"),\n",
    "    (\"P99999\", \"C0000\", \"Demo_MissingCustomer\", \"Active\",\n",
    "     date(2024,6,1), None, mgr_id, Decimal(\"150000.00\"), \"Fixed\"),\n",
    "    (\"P99997\", cust_ids[0] if cust_ids else \"C9999\", \"Demo_ClosedNoEnd\", \"Closed\",\n",
    "     date(2024,2,1), None, mgr_id, Decimal(\"100000.00\"), \"Retainer\"),\n",
    "]\n",
    "proj_demo_df = spark.createDataFrame(demo_rows_proj, schema=SCHEMAS_BY_TABLE[TABLES[\"project\"]])\n",
    "proj_df = proj_df.unionByName(proj_demo_df)\n",
    "\n",
    "proj_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"project\"])\n",
    "print(f\"{C.ivory}Wrote: {C.golden_yellow}{TABLES['project']}{C.r}  rows={spark.table(TABLES['project']).count():,}\")\n",
    "show_df(spark.table(TABLES[\"project\"]), n=20, truncate=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# CELL 6 — Timesheets  (enforce DECIMAL(5,2) + overwriteSchema)\n",
    "# -------------------------------------------------------------------------------------\n",
    "banner(f\"BUILD (Datagen): {TABLES['timesheet']}\", \"sky_blue\")\n",
    "n_ts = ROW_TARGETS[TABLES[\"timesheet\"]]\n",
    "\n",
    "emp_ids = [r[0] for r in spark.table(TABLES[\"employee\"]).select(\"employee_id\").collect()]\n",
    "proj_keys = (spark.table(TABLES[\"project\"]).select(\"project_id\",\"start_date\",\"end_date\").collect())\n",
    "project_id_list = [r[\"project_id\"] for r in proj_keys]\n",
    "proj_map = {r[\"project_id\"]: (r[\"start_date\"], r[\"end_date\"]) for r in proj_keys}\n",
    "\n",
    "ts_gen = (DataGenerator(spark, name=\"demo_timesheet_gen\", rows=n_ts, partitions=48)\n",
    "          .withColumn(\"timesheet_id\", \"string\", expr=\"uuid()\")\n",
    "          .withColumn(\"employee_id\", values=emp_ids)\n",
    "          .withColumn(\"project_id\", values=project_id_list)\n",
    "          .withColumn(\"work_date\", \"date\", minValue=\"2024-01-01\", maxValue=\"2025-08-10\")\n",
    "          .withColumn(\"hours_worked\", \"decimal(5,2)\", minValue=0.00, maxValue=12.00)\n",
    "          .withColumn(\"work_type\", values=[\"Billable\",\"NonBillable\",\"Admin\"], weights=[0.70,0.25,0.05])\n",
    "          .withColumn(\"source_system\", values=[\"Workday\",\"Jira\",\"CSV\"], weights=[0.70,0.20,0.10])\n",
    "          .withColumn(\"created_ts\", \"timestamp\")\n",
    ")\n",
    "ts_df = ts_gen.build()\n",
    "\n",
    "# created_ts near work_date (0–72 hours)\n",
    "ts_df = ts_df.withColumn(\n",
    "    \"created_ts\",\n",
    "    F.expr(\"timestamp(work_date) + make_interval(0,0,0,cast(rand()*3 as int), cast(rand()*24 as int), cast(rand()*60 as int), 0)\")\n",
    ")\n",
    "\n",
    "# Deterministic demo timesheets\n",
    "chosen_emp = emp_ids[0] if emp_ids else None\n",
    "chosen_proj = project_id_list[0] if project_id_list else None\n",
    "if chosen_emp and chosen_proj:\n",
    "    p_start, _ = proj_map.get(chosen_proj, (date(2024,1,1), None))\n",
    "    demo_rows_ts = [\n",
    "        (str(uuid4()), chosen_emp, chosen_proj, p_start - timedelta(days=3),\n",
    "         Decimal(\"8.00\"), \"Billable\", \"Workday\", datetime.utcnow()),\n",
    "        (str(uuid4()), chosen_emp, chosen_proj, date.today() - timedelta(days=10),\n",
    "         Decimal(\"8.00\"), \"Billable\", \"Workday\", datetime.utcnow()),\n",
    "        (str(uuid4()), chosen_emp, chosen_proj, date.today() - timedelta(days=10),\n",
    "         Decimal(\"8.00\"), \"Billable\", \"Workday\", datetime.utcnow()),\n",
    "    ]\n",
    "    ts_demo_df = spark.createDataFrame(demo_rows_ts, schema=SCHEMAS_BY_TABLE[TABLES[\"timesheet\"]])\n",
    "    ts_df = ts_df.unionByName(ts_demo_df)\n",
    "\n",
    "# Enforce canonical schema & write with overwriteSchema\n",
    "ts_schema = SCHEMAS_BY_TABLE[TABLES[\"timesheet\"]]\n",
    "for field in ts_schema:\n",
    "    ts_df = ts_df.withColumn(field.name, F.col(field.name).cast(field.dataType))\n",
    "\n",
    "ts_df.select([f.name for f in ts_schema]) \\\n",
    "     .write.format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .option(\"overwriteSchema\", \"true\") \\\n",
    "     .saveAsTable(TABLES[\"timesheet\"])\n",
    "\n",
    "print(f\"{C.ivory}Wrote: {C.golden_yellow}{TABLES['timesheet']}{C.r}  rows={spark.table(TABLES['timesheet']).count():,}\")\n",
    "show_df(spark.table(TABLES[\"timesheet\"]), n=20, truncate=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# CELL 7 — Expenses\n",
    "# -------------------------------------------------------------------------------------\n",
    "banner(f\"BUILD (Datagen): {TABLES['expense']}\", \"sky_blue\")\n",
    "n_exp = ROW_TARGETS[TABLES[\"expense\"]]\n",
    "emp_ids = [r[0] for r in spark.table(TABLES[\"employee\"]).select(\"employee_id\").collect()]\n",
    "proj_ids = [r[0] for r in spark.table(TABLES[\"project\"]).select(\"project_id\").collect()]\n",
    "\n",
    "exp_gen = (DataGenerator(spark, name=\"demo_expense_gen\", rows=n_exp, partitions=24)\n",
    "           .withColumn(\"expense_id\", \"string\", expr=\"uuid()\")\n",
    "           .withColumn(\"employee_id\", values=emp_ids)\n",
    "           .withColumn(\"project_id\", values=proj_ids, percentNulls=0.25)\n",
    "           .withColumn(\"expense_date\", \"date\", minValue=\"2024-01-01\", maxValue=\"2025-08-10\")\n",
    "           .withColumn(\"category\", values=[\"Meals\",\"Travel\",\"Supplies\",\"Software\",\"Other\"], weights=[0.35,0.25,0.20,0.10,0.10])\n",
    "           .withColumn(\"amount\", \"decimal(18,2)\", minValue=5.00, maxValue=5000.00)\n",
    "           .withColumn(\"currency_code\", values=MEGASPEC[\"knobs\"][\"valid_currency_codes\"])\n",
    "           .withColumn(\"merchant\", values=[\"Uber\",\"Lyft\",\"Delta\",\"AA\",\"Staples\",\"BestBuy\",\"Amazon\",\"LocalCafe\",\"HotelCo\",\"SoftwareCo\"])\n",
    "           .withColumn(\"receipt_attached\", \"boolean\", expr=\"rand() > 0.08\")\n",
    "           .withColumn(\"submission_ts\", \"timestamp\")\n",
    ")\n",
    "exp_df = exp_gen.build()\n",
    "\n",
    "# submission_ts near expense_date (0–240 hours)\n",
    "exp_df = exp_df.withColumn(\n",
    "    \"submission_ts\",\n",
    "    F.expr(\"timestamp(expense_date) + make_interval(0,0,0,cast(rand()*10 as int), cast(rand()*24 as int), cast(rand()*60 as int), 0)\")\n",
    ")\n",
    "\n",
    "# Deterministic demo expenses\n",
    "if emp_ids:\n",
    "    demo_emp = emp_ids[0]\n",
    "    emp_cc = (spark.table(TABLES[\"employee\"]).where(F.col(\"employee_id\")==demo_emp)\n",
    "              .select(\"country_code\").first())\n",
    "    home_cc = emp_cc[\"country_code\"] if emp_cc and emp_cc[\"country_code\"] else \"US\"\n",
    "    home_ccy = {\"US\":\"USD\",\"CA\":\"CAD\",\"MX\":\"MXN\",\"GB\":\"GBP\",\"IN\":\"INR\"}.get(home_cc, \"USD\")\n",
    "    away_ccy = \"GBP\" if home_ccy != \"GBP\" else \"USD\"\n",
    "else:\n",
    "    demo_emp = \"E9999\"; home_ccy = \"USD\"; away_ccy = \"GBP\"\n",
    "\n",
    "demo_proj = proj_ids[0] if proj_ids else None\n",
    "today = date.today()\n",
    "demo_rows_exp = [\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=20), \"Meals\", Decimal(\"15.00\"), home_ccy, \"LocalCafe\", False, datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=18), \"Meals\", Decimal(\"22.00\"), home_ccy, \"LocalCafe\", True,  datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=16), \"Meals\", Decimal(\"18.00\"), home_ccy, \"LocalCafe\", True,  datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=14), \"Meals\", Decimal(\"20.50\"), home_ccy, \"LocalCafe\", True,  datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=1),  \"Meals\", Decimal(\"600.00\"), away_ccy, \"LocalCafe\", True,  datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=2),  \"Meals\", Decimal(\"24.99\"), home_ccy, \"LocalCafe\", False, datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=2),  \"Meals\", Decimal(\"24.99\"), home_ccy, \"LocalCafe\", False, datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=2),  \"Meals\", Decimal(\"26.99\"), home_ccy, \"LocalCafe\", False, datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=5),  \"Supplies\", Decimal(\"129.00\"), home_ccy, \"Amazon\", True,  datetime.utcnow()),\n",
    "    (str(uuid4()), demo_emp, demo_proj, today - timedelta(days=5),  \"Supplies\", Decimal(\"129.00\"), home_ccy, \"Amazon\", True,  datetime.utcnow()),\n",
    "]\n",
    "exp_demo_df = spark.createDataFrame(demo_rows_exp, schema=SCHEMAS_BY_TABLE[TABLES[\"expense\"]])\n",
    "exp_df = exp_df.unionByName(exp_demo_df)\n",
    "\n",
    "exp_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLES[\"expense\"])\n",
    "print(f\"{C.ivory}Wrote: {C.golden_yellow}{TABLES['expense']}{C.r}  rows={spark.table(TABLES['expense']).count():,}\")\n",
    "show_df(spark.table(TABLES[\"expense\"]), n=20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e35623-ba8b-421b-bccf-40b184d9ee44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dqx_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
