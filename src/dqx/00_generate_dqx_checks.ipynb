{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4fe261-69b0-4d1a-a5a0-cb4d445b418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate DQX Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d960e86-3a79-4da3-b826-4dbf75713f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0267c88-84f7-4a75-bd6a-4f3b7643120c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DQProfiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679f49b2-d5c8-4923-8618-9d61914cf336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### [Profiling Options](https://databrickslabs.github.io/dqx/docs/reference/profiler/#profiling-options)[](url)        \n",
    "\n",
    "| Option            | Default Value | Description                                                                 |\n",
    "|-------------------|---------------|-----------------------------------------------------------------------------|\n",
    "| `round`           | `True`        | Round min/max values for cleaner rules                                     |\n",
    "| `max_in_count`    | `10`          | Generate `is_in` rule if distinct values < this count                      |\n",
    "| `distinct_ratio`  | `0.05`        | Generate `is_in` rule if distinct values < 5% of total                     |\n",
    "| `max_null_ratio`  | `0.01`        | Generate `is_not_null` rule if null values < 1% of total                   |\n",
    "| `remove_outliers` | `True`        | Enable outlier detection for min/max rules                                 |\n",
    "| `outlier_columns` | `[]`          | Specific columns for outlier detection (empty = all numeric)               |\n",
    "| `num_sigmas`      | `3`           | Number of standard deviations for outlier detection                        |\n",
    "| `trim_strings`    | `True`        | Trim whitespace from strings before analysis                               |\n",
    "| `max_empty_ratio` | `0.01`        | Generate `is_not_null_or_empty` rule if empty strings < 1% of total        |\n",
    "| `sample_fraction` | `0.3`         | Sample 30% of the data for profiling                                       |\n",
    "| `sample_seed`     | `None`        | Seed for sampling (`None` = random)                                        |\n",
    "| `limit`           | `1000`        | Maximum number of records to analyze                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abb6ab0a-4660-4704-9dbc-ac538c5dbbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### [Summary Statistics Reference](https://databrickslabs.github.io/dqx/docs/guide/data_profiling/#summary-statistics-reference)[](url) \n",
    "\n",
    "| Field            | Meaning                                                  | Notes                                                                 |\n",
    "|------------------|----------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| `count`          | Rows actually profiled (after sampling and limit)        | ≈ min(`limit`, `sample_fraction` × total_rows)                        |\n",
    "| `mean`           | Arithmetic average of non-null numeric values            | N/A for non-numeric                                                   |\n",
    "| `stddev`         | Sample standard deviation of non-null numeric values     | N/A for non-numeric                                                   |\n",
    "| `min`            | Smallest non-null value                                  | String = lexicographic; Date/Timestamp = earliest; Numeric = minimum |\n",
    "| `25 / 50 / 75`   | Approximate 25th/50th/75th percentiles of numeric values | Uses Spark approximate quantiles                                     |\n",
    "| `max`            | Largest non-null value                                   | String = lexicographic; Date/Timestamp = latest; Numeric = maximum   |\n",
    "| `count_non_null` | Number of non-null entries within the profiled rows      |                                                                       |\n",
    "| `count_null`     | Number of null entries within the profiled rows          | `count_non_null` + `count_null` = `count`                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8383f8f1-9db5-46eb-abc3-d472fbc64507",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Profiling options for a single table"
    }
   },
   "outputs": [],
   "source": [
    "#NOTE: https://databrickslabs.github.io/dqx/docs/guide/data_profiling/#profiling-options \n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Custom profiling options\n",
    "custom_options = {\n",
    "    # Sampling options\n",
    "    \"sample_fraction\": 0.2,       # Sample 20% of the data\n",
    "    \"sample_seed\": 42,            # Seed for reproducible sampling\n",
    "    \"limit\": 2000,                # Limit to 2000 records after sampling\n",
    "    \n",
    "    # Outlier detection options\n",
    "    \"remove_outliers\": True,      # Enable outlier detection for min/max rules\n",
    "    \"outlier_columns\": [\"price\", \"age\"],  # Only detect outliers in specific columns\n",
    "    \"num_sigmas\": 2.5,            # Use 2.5 standard deviations for outlier detection\n",
    "    \n",
    "    # Null value handling\n",
    "    \"max_null_ratio\": 0.05,       # Generate is_not_null rule if <5% nulls\n",
    "    \n",
    "    # String handling\n",
    "    \"trim_strings\": True,         # Trim whitespace from strings before analysis\n",
    "    \"max_empty_ratio\": 0.02,      # Generate is_not_null_or_empty rule if <2% empty strings\n",
    "    \n",
    "    # Distinct value analysis\n",
    "    \"distinct_ratio\": 0.01,       # Generate is_in rule if <1% distinct values\n",
    "    \"max_in_count\": 20,           # Maximum items in is_in rule list\n",
    "    \n",
    "    # Value rounding\n",
    "    \"round\": True,                # Round min/max values for cleaner rules\n",
    "}\n",
    "\n",
    "ws = WorkspaceClient()\n",
    "profiler = DQProfiler(ws)\n",
    "\n",
    "# Apply custom options for profiling a DataFrame\n",
    "summary_stats, profiles = profiler.profile(input_df, options=custom_options)\n",
    "\n",
    "# Apply custom options for profiling a table\n",
    "summary_stats, profiles = profiler.profile_table(\n",
    "    table=\"catalog1.schema1.table1\",\n",
    "    columns=[\"col1\", \"col2\", \"col3\"],\n",
    "    options=custom_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "636cf1bc-6d7e-4ead-a5a6-b048a2e35bf8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Profiling options for multiple table"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "ws = WorkspaceClient()\n",
    "profiler = DQProfiler(ws)\n",
    "\n",
    "tables = [\n",
    "  \"dqx.bronze.table_001\",\n",
    "  \"dqx.silver.table_001\",\n",
    "  \"dqx.silver.table_002\",\n",
    "]\n",
    "\n",
    "# Custom options with wildcard patterns\n",
    "custom_table_options = [\n",
    "  {\n",
    "    \"table\": \"*\",  # matches all tables by pattern\n",
    "    \"options\": {\"sample_fraction\": 0.5}\n",
    "  },\n",
    "  {\n",
    "    \"table\": \"dqx.silver.*\",  # matches tables in the 'dqx.silver' schema by pattern\n",
    "    \"options\": {\"num_sigmas\": 5}\n",
    "  },\n",
    "  {\n",
    "    \"table\": \"dqx.silver.table_*\",  # matches tables in 'dqx.silver' schema and having 'table_' prefix\n",
    "    \"options\": {\"num_sigmas\": 5}\n",
    "  },\n",
    "  {\n",
    "    \"table\": \"dqx.silver.table_002\",  # matches a specific table, overrides generic option\n",
    "    \"options\": {\"sample_fraction\": 0.1}\n",
    "  },\n",
    "]\n",
    "\n",
    "# Profile multiple tables using custom options\n",
    "results = profiler.profile_tables(tables=tables, options=custom_table_options)\n",
    "\n",
    "# Profile multiple tables by wildcard patterns using custom options\n",
    "results = profiler.profile_tables(\n",
    "  patterns=[\"dqx.*\"],\n",
    "  options=custom_table_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b53bf235-a02a-432e-8c92-3bd0012a22bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Future Enhancments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d093f9b-7bce-4ac5-ab6f-4699048df8aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sampling/Limits for Large Datasets"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: When profiling large datasets, use sampling or limits for best performance (https://databrickslabs.github.io/dqx/docs/guide/data_profiling/#performance-considerations)\n",
    "\n",
    "# For large datasets, use aggressive sampling\n",
    "large_dataset_opts = {\n",
    "    \"sample_fraction\": 0.01,  # Sample only 1% for very large datasets\n",
    "    \"limit\": 10000,          # Increase limit for better statistical accuracy\n",
    "    \"sample_seed\": 42,       # Use consistent seed for reproducible results\n",
    "}\n",
    "\n",
    "# For medium datasets, use moderate sampling\n",
    "medium_dataset_opts = {\n",
    "    \"sample_fraction\": 0.1,   # Sample 10%\n",
    "    \"limit\": 5000,           # Reasonable limit\n",
    "}\n",
    "\n",
    "# For small datasets, disable sampling\n",
    "small_dataset_opts = {\n",
    "    \"sample_fraction\": None,  # Use all data\n",
    "    \"limit\": None,           # No limit\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1869f8-668d-42d2-a627-67adcb645ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2168addb-58e3-4aae-853c-7e2f844cda7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8baa8a-c510-47e7-b107-3204ff7b2c9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75c5cb0e-8a7a-4f75-ad35-b66fe72b5e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30347346-cf42-4025-a9f4-bb1b364b246f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate & SaveDQX Checks - 'yaml', 'table', or 'both'"
    }
   },
   "outputs": [],
   "source": [
    "# generate_checks.py\n",
    "# Generate DQX checks from tables or YAML table-lists and write to YAML, table, or both.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import hashlib\n",
    "import datetime\n",
    "from typing import List, Optional, Dict, Any, Literal, Tuple\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql import DataFrame  # for type hints in show_df()\n",
    "\n",
    "# Notebook env helper (prints banner in the notebook and returns a dict we can reuse)\n",
    "from utils.print import print_notebook_env, get_notebook_path as _nb_path\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Documentation dictionary for the generated checks table (apply on first create)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "DQX_GENERATED_CHECKS_CONFIG_METADATA: Dict[str, Any] = {\n",
    "    \"table\": \"dq_dev.dqx.checks_generated_config\",  # will be overridden by the actual FQN you pass\n",
    "    \"table_comment\": (\n",
    "        \"## **DQX *Generated* Checks Configuration**\\n\"\n",
    "        \"- Stores flattened rules generated by the profiler.\\n\"\n",
    "        \"- Each row is a rule; `check_id` is a stable hash of the canonical payload.\\n\"\n",
    "        \"- `generator_meta` captures the profiler options and generator settings used to create these rows.\\n\"\n",
    "    ),\n",
    "    \"columns\": {\n",
    "        \"check_id\": \"SHA-256 **hash** of canonical payload (stable rule identity).\",\n",
    "        \"check_id_payload\": \"Canonical **JSON** used to compute `check_id`.\",\n",
    "        \"table_name\": \"Fully qualified **target table** (`catalog.schema.table`).\",\n",
    "\n",
    "        \"name\": \"Human-readable **rule name**.\",\n",
    "        \"criticality\": \"Rule severity: `warn|warning|error`.\",\n",
    "        \"check\": \"Structured **check** object: `{function, for_each_column, arguments}`.\",\n",
    "        \"filter\": \"Optional row-level **filter** expression.\",\n",
    "        \"run_config_name\": \"**Execution group/tag** for this rule.\",\n",
    "        \"user_metadata\": \"User-provided **metadata** `map<string,string>`.\",\n",
    "\n",
    "        \"yaml_path\": \"YAML **file path** that held this rule (or `<generated://...>` if ephemeral).\",\n",
    "        \"active\": \"If **false**, the rule is ignored by runners.\",\n",
    "\n",
    "        # NEW (placed after active, before audit)\n",
    "        \"generator_meta\": (\n",
    "            \"Array of two items: \"\n",
    "            \"`[{section:'profile_options', payload:map}, {section:'generator_settings', payload:map}]`.\"\n",
    "        ),\n",
    "\n",
    "        \"created_by\": \"Audit: **creator** of the row.\",\n",
    "        \"created_at\": \"Audit: **creation timestamp** (UTC ISO).\",\n",
    "        \"updated_by\": \"Audit: **last updater**.\",\n",
    "        \"updated_at\": \"Audit: **last update timestamp**.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "DQXGeneratedChecksConfig = DQX_GENERATED_CHECKS_CONFIG_METADATA\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Schema (unified) for the generated checks table (adds generator_meta)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "DQX_GENERATED_CHECKS_CONFIG_SCHEMA = T.StructType([\n",
    "    T.StructField(\"check_id\",            T.StringType(),  False),\n",
    "    T.StructField(\"check_id_payload\",    T.StringType(),  False),\n",
    "    T.StructField(\"table_name\",          T.StringType(),  False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",                T.StringType(),  False),\n",
    "    T.StructField(\"criticality\",         T.StringType(),  False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(),  False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",              T.StringType(),  True),\n",
    "    T.StructField(\"run_config_name\",     T.StringType(),  False),\n",
    "    T.StructField(\"user_metadata\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # Ops\n",
    "    T.StructField(\"yaml_path\",           T.StringType(),  False),\n",
    "    T.StructField(\"active\",              T.BooleanType(), False),\n",
    "\n",
    "    # NEW: meta goes right here (as requested, before audit)\n",
    "    T.StructField(\"generator_meta\", T.ArrayType(T.StructType([\n",
    "        T.StructField(\"section\", T.StringType(), False),  # \"profile_options\" | \"generator_settings\"\n",
    "        T.StructField(\"payload\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ])), True),\n",
    "\n",
    "    # Audit\n",
    "    T.StructField(\"created_by\",          T.StringType(),  False),\n",
    "    T.StructField(\"created_at\",          T.StringType(),  False),  # ISO string; cast downstream if needed\n",
    "    T.StructField(\"updated_by\",          T.StringType(),  True),\n",
    "    T.StructField(\"updated_at\",          T.StringType(),  True),\n",
    "])\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Constants / helpers (kept intact)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "DOC_SUPPORTED_KEYS = {\n",
    "    \"sample_fraction\", \"sample_seed\", \"limit\",\n",
    "    \"remove_outliers\", \"outlier_columns\", \"num_sigmas\",\n",
    "    \"max_null_ratio\", \"trim_strings\", \"max_empty_ratio\",\n",
    "    \"distinct_ratio\", \"max_in_count\", \"round\",\n",
    "}\n",
    "\n",
    "_YAML_PATH_RE = re.compile(r\"\\.(ya?ml)$\", re.IGNORECASE)\n",
    "_FROM_INFOSCHEMA = re.compile(r\"FROM\\s+([A-Za-z0-9_]+)\\.information_schema\\.tables\", re.IGNORECASE)\n",
    "_TABLE_SCHEMA_EQ = re.compile(r\"table_schema\\s*=\\s*'([^']+)'\", re.IGNORECASE)\n",
    "\n",
    "def _is_yaml_path(p: str) -> bool:\n",
    "    return bool(_YAML_PATH_RE.search(p))\n",
    "\n",
    "def _esc_sql_comment(s: str) -> str:\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "def _safe_json(obj: Any) -> str:\n",
    "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def _resolve_local_like_path(path: str) -> Optional[str]:\n",
    "    \"\"\"Resolve a repo/local-like path by walking up a few parents.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return os.path.abspath(path)\n",
    "    base = os.getcwd()\n",
    "    for _ in range(6):\n",
    "        cand = os.path.abspath(os.path.join(base, path))\n",
    "        if os.path.exists(cand):\n",
    "            return cand\n",
    "        parent = os.path.dirname(base)\n",
    "        if parent == base:\n",
    "            break\n",
    "        base = parent\n",
    "    return None\n",
    "\n",
    "def _read_text_any(path: str) -> str:\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"dbutils is required to read from DBFS/Volumes\") from e\n",
    "        target = path if path.startswith(\"dbfs:\") else (f\"dbfs:{path}\" if path.startswith(\"/\") else f\"dbfs:/{path}\")\n",
    "        return dbutils.fs.head(target, 10 * 1024 * 1024)\n",
    "\n",
    "    # Workspace Files (absolute)\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            data = wc.files.download(file_path=path).read()\n",
    "        except TypeError:\n",
    "            data = wc.files.download(path=path).read()\n",
    "        return data.decode(\"utf-8\")\n",
    "\n",
    "    # Local / repo-relative\n",
    "    resolved = _resolve_local_like_path(path)\n",
    "    if resolved and os.path.isfile(resolved):\n",
    "        with open(resolved, \"r\", encoding=\"utf-8\") as fh:\n",
    "            return fh.read()\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not read file: {path}\")\n",
    "\n",
    "def _ensure_parent_local(path: str) -> None:\n",
    "    parent = os.path.dirname(path)\n",
    "    if parent and not os.path.exists(parent):\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "def _to_dbfs_target(path: str) -> str:\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        return path\n",
    "    if path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        return \"dbfs:\" + path\n",
    "    return path\n",
    "\n",
    "def _write_text_any(path: str, payload: str) -> None:\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"dbutils is required to write to DBFS/Volumes.\")\n",
    "        target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "        parent = target.rsplit(\"/\", 1)[0]\n",
    "        if parent:\n",
    "            dbutils.fs.mkdirs(parent)\n",
    "        dbutils.fs.put(target, payload, True)\n",
    "        return\n",
    "\n",
    "    # Workspace Files\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            wc.files.upload(file_path=path, contents=payload.encode(\"utf-8\"), overwrite=True)\n",
    "        except TypeError:\n",
    "            wc.files.upload(path=path, contents=payload.encode(\"utf-8\"), overwrite=True)\n",
    "        return\n",
    "\n",
    "    # Local (Repos/driver)\n",
    "    full = os.path.abspath(path)\n",
    "    _ensure_parent_local(full)\n",
    "    with open(full, \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(payload)\n",
    "\n",
    "def _parse_global_hints_from_comments(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    m_cat = _FROM_INFOSCHEMA.search(text)\n",
    "    m_sch = _TABLE_SCHEMA_EQ.search(text)\n",
    "    return (m_cat.group(1) if m_cat else None, m_sch.group(1) if m_sch else None)\n",
    "\n",
    "def _ensure_fqns(names: List[str], hint_catalog: Optional[str], hint_schema: Optional[str]) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for n in names:\n",
    "        n = n.strip()\n",
    "        if not n:\n",
    "            continue\n",
    "        parts = n.split(\".\")\n",
    "        if len(parts) == 3:\n",
    "            out.append(n)\n",
    "        elif len(parts) == 2:\n",
    "            if not hint_catalog:\n",
    "                raise ValueError(f\"'{n}' lacks catalog; add it or provide a comment default.\")\n",
    "            out.append(f\"{hint_catalog}.{n}\")\n",
    "        elif len(parts) == 1:\n",
    "            if not (hint_catalog and hint_schema):\n",
    "                raise ValueError(f\"'{n}' needs catalog & schema; add comments or use dotted forms.\")\n",
    "            out.append(f\"{hint_catalog}.{hint_schema}.{n}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized table format: {n}\")\n",
    "    return sorted(set(out))\n",
    "\n",
    "def _discover_tables_from_yaml_file(yaml_path: str) -> List[str]:\n",
    "    \"\"\"YAML contains e.g. `table_name: [a, b, c]`. Comments can hint catalog/schema.\"\"\"\n",
    "    text = _read_text_any(yaml_path)\n",
    "    cat_hint, sch_hint = _parse_global_hints_from_comments(text)\n",
    "    obj = yaml.safe_load(io.StringIO(text))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(f\"YAML must contain a mapping with a list; got: {type(obj).__name__}\")\n",
    "    names = None\n",
    "    for key in (\"table_name\", \"tables\", \"table_names\", \"list\"):\n",
    "        if isinstance(obj.get(key), list):\n",
    "            names = [str(x).strip() for x in obj[key] if x]\n",
    "            break\n",
    "    if not names:\n",
    "        raise ValueError(f\"No table list found in YAML: {yaml_path}\")\n",
    "    return _ensure_fqns(names, cat_hint, sch_hint)\n",
    "\n",
    "def _prefix_of(table_fqn: str) -> str:\n",
    "    \"\"\"Prefix up to first underscore of the *table* portion.\"\"\"\n",
    "    base = table_fqn.split(\".\")[-1]\n",
    "    return base.split(\"_\", 1)[0].lower() if base else \"\"\n",
    "\n",
    "def _filter_by_prefix_regex(tables: List[str], exclude_prefix_regex: Optional[str]) -> List[str]:\n",
    "    if not exclude_prefix_regex:\n",
    "        return tables\n",
    "    pat = re.compile(exclude_prefix_regex, re.IGNORECASE)\n",
    "    keep: List[str] = []\n",
    "    for t in tables:\n",
    "        if not pat.search(_prefix_of(t)):\n",
    "            keep.append(t)\n",
    "    return keep\n",
    "\n",
    "def _display_table_preview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    rows = [(f, *f.split(\".\")) for f in fqns]\n",
    "    df = spark.createDataFrame(rows, \"fqn string, catalog string, schema string, table string\")\n",
    "    print(f\"\\n=== {title} ({len(fqns)}) ===\")\n",
    "    try:\n",
    "        display(df)\n",
    "    except NameError:\n",
    "        df.show(len(fqns), truncate=False)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "\n",
    "def _stringify_map_values(d: Optional[Dict[str, Any]]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = _safe_json(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def _compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    def _canon_filter(s: Optional[str]) -> str:\n",
    "        return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "    def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "        fec = chk.get(\"for_each_column\")\n",
    "        if isinstance(fec, list):\n",
    "            out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "        args = chk.get(\"arguments\") or {}\n",
    "        canon_args: Dict[str, str] = {}\n",
    "        for k, v in args.items():\n",
    "            sv = \"\" if v is None else str(v).strip()\n",
    "            if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "                try:\n",
    "                    sv = _safe_json(json.loads(sv))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            canon_args[str(k)] = sv\n",
    "        out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "        return out\n",
    "\n",
    "    payload_obj = {\n",
    "        \"table_name\": (table_name or \"\").lower(),\n",
    "        \"filter\": _canon_filter(filter_str),\n",
    "        \"check\": _canon_check(check_dict or {}),\n",
    "    }\n",
    "    return _safe_json(payload_obj)\n",
    "\n",
    "def _compute_check_id(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Public aliases (you asked to keep these helper names around)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def DisplayTablePreview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    return _display_table_preview(spark, fqns, title)\n",
    "\n",
    "def NowISO() -> str:\n",
    "    return _now_iso()\n",
    "\n",
    "def GetNotebookPath() -> str:\n",
    "    return _nb_path()\n",
    "\n",
    "def PathStartsWith(s: str, *prefixes: str) -> bool:\n",
    "    return any(s.startswith(p) for p in prefixes)\n",
    "\n",
    "def ResolveLocalLikePath(p: str) -> Optional[str]:\n",
    "    return _resolve_local_like_path(p)\n",
    "\n",
    "def DiscoverTablesFromYAML(yaml_path: str) -> List[str]:\n",
    "    return _discover_tables_from_yaml_file(yaml_path)\n",
    "\n",
    "def ListYAMLPathsInFolder(folder: str) -> List[str]:\n",
    "    # DBFS/Volumes\n",
    "    out: List[str] = []\n",
    "    if folder.startswith(\"dbfs:/\") or folder.startswith(\"/dbfs/\") or folder.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"dbutils is required to traverse DBFS/Volumes.\")\n",
    "        root = _to_dbfs_target(folder)\n",
    "        def _walk_dbfs(dirpath: str):\n",
    "            for fi in dbutils.fs.ls(dirpath):\n",
    "                p = fi.path\n",
    "                if p.endswith(\"/\"):\n",
    "                    _walk_dbfs(p)\n",
    "                elif _is_yaml_path(p):\n",
    "                    out.append(p)\n",
    "        _walk_dbfs(root)\n",
    "        return out\n",
    "\n",
    "    # Workspace Files -> best-effort local resolve for recursion\n",
    "    if folder.startswith(\"/\"):\n",
    "        resolved = _resolve_local_like_path(folder)\n",
    "        if resolved and os.path.isdir(resolved):\n",
    "            for r, _, files in os.walk(resolved):\n",
    "                for f in files:\n",
    "                    if _is_yaml_path(f):\n",
    "                        out.append(os.path.join(r, f))\n",
    "            return out\n",
    "        return out\n",
    "\n",
    "    # Local / repo\n",
    "    resolved = _resolve_local_like_path(folder)\n",
    "    if resolved and os.path.isdir(resolved):\n",
    "        for r, _, files in os.walk(resolved):\n",
    "            for f in files:\n",
    "                if _is_yaml_path(f):\n",
    "                    out.append(os.path.join(r, f))\n",
    "    return out\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Pretty display helpers (as requested)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _can_display() -> bool:\n",
    "    return \"display\" in globals()\n",
    "\n",
    "def show_df(df: DataFrame, n: int = 100, truncate: bool = False) -> None:\n",
    "    if _can_display():\n",
    "        display(df.limit(n))\n",
    "    else:\n",
    "        df.show(n, truncate=truncate)\n",
    "\n",
    "def display_section(title: str) -> None:\n",
    "    print(\"\\n\" + \"═\" * 80)\n",
    "    print(f\"║ {title}\")\n",
    "    print(\"═\" * 80)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Column comment helper with robust multi-syntax fallback\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _apply_column_comment_with_fallback(\n",
    "    spark: SparkSession,\n",
    "    cat: str,\n",
    "    sch: str,\n",
    "    tbl: str,\n",
    "    col_name: str,\n",
    "    comment_text: str,\n",
    "    col_types_lower: Dict[str, str],\n",
    ") -> bool:\n",
    "    \"\"\"Try COMMENT ON COLUMN, then ALTER ... ALTER COLUMN, then CHANGE COLUMN with type.\"\"\"\n",
    "    fqn_q = f\"`{cat}`.`{sch}`.`{tbl}`\"\n",
    "    col_q = f\"`{col_name}`\"\n",
    "    cmt = _esc_sql_comment(comment_text)\n",
    "\n",
    "    # 1) COMMENT ON COLUMN\n",
    "    try:\n",
    "        spark.sql(f\"COMMENT ON COLUMN {fqn_q}.{col_q} IS '{cmt}'\")\n",
    "        return True\n",
    "    except Exception as e1:\n",
    "        pass\n",
    "\n",
    "    # 2) ALTER TABLE ... ALTER COLUMN ... COMMENT\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {fqn_q} ALTER COLUMN {col_q} COMMENT '{cmt}'\")\n",
    "        return True\n",
    "    except Exception as e2:\n",
    "        pass\n",
    "\n",
    "    # 3) ALTER TABLE ... CHANGE COLUMN col col <type> COMMENT ...\n",
    "    dtype = col_types_lower.get(col_name.lower())\n",
    "    if not dtype:\n",
    "        print(f\"[WARN] Cannot determine data type for {cat}.{sch}.{tbl}.{col_name}; skipping column comment.\")\n",
    "        return False\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {fqn_q} CHANGE COLUMN {col_q} {col_q} {dtype} COMMENT '{cmt}'\")\n",
    "        return True\n",
    "    except Exception as e3:\n",
    "        print(f\"[WARN] Failed to set comment for {cat}.{sch}.{tbl}.{col_name}: {e3}\")\n",
    "        return False\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Table documentation application\n",
    "#  - Table comment applied when just-created\n",
    "#  - Column comments applied ALWAYS using robust fallback (fixes your error)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _apply_table_documentation_on_create(spark: SparkSession, table_fqn: str, doc: Dict[str, Any], just_created: bool):\n",
    "    try:\n",
    "        cat, sch, tbl = table_fqn.split(\".\")\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "    table_comment = (doc or {}).get(\"table_comment\") or \"\"\n",
    "    if table_comment and just_created:\n",
    "        spark.sql(\n",
    "            f\"COMMENT ON TABLE `{cat}`.`{sch}`.`{tbl}` IS '{_esc_sql_comment(table_comment)}'\"\n",
    "        )\n",
    "\n",
    "    cols_doc: Dict[str, str] = (doc or {}).get(\"columns\") or {}\n",
    "    if not cols_doc:\n",
    "        return\n",
    "\n",
    "    # Discover existing columns and their types (for fallback syntax)\n",
    "    desc_rows = spark.sql(f\"DESCRIBE TABLE `{cat}`.`{sch}`.`{tbl}`\").collect()\n",
    "    existing_cols = {}\n",
    "    col_types = {}\n",
    "    for r in desc_rows:\n",
    "        if r.col_name and not r.col_name.startswith(\"#\"):\n",
    "            existing_cols[r.col_name.lower()] = True\n",
    "            # Some rows may not have data_type; guard defensively\n",
    "            if hasattr(r, \"data_type\") and r.data_type:\n",
    "                col_types[r.col_name.lower()] = r.data_type\n",
    "\n",
    "    # Apply column comments when columns exist\n",
    "    for col_name, cmt in cols_doc.items():\n",
    "        if col_name.lower() not in existing_cols:\n",
    "            continue\n",
    "        _apply_column_comment_with_fallback(\n",
    "            spark, cat, sch, tbl, col_name, cmt, col_types_lower=col_types\n",
    "        )\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Main generator\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "class CheckGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: str,                         # \"pipeline\" | \"catalog\" | \"schema\" | \"table\" | \"file\" | \"folder\"\n",
    "        source: str,                        # depends on scope\n",
    "        output_format: str,                 # \"yaml\" | \"table\" | \"both\"\n",
    "        output_yaml: Optional[str],         # folder or /Volumes/... or dbfs:/... or workspace \"/...\"\n",
    "        output_table: Optional[str],        # fully-qualified table FQN\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_prefix_regex: Optional[str] = None,   # regex on table prefix (before first \"_\")\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,          # only valid when scope==\"table\"\n",
    "        run_config_name: str = \"default\",\n",
    "        criticality: str = \"warn\",\n",
    "        key_order: Literal[\"engine\", \"custom\"] = \"custom\",\n",
    "        include_table_name: bool = True,\n",
    "        yaml_metadata: bool = False,                  # add commented header on each YAML\n",
    "        table_doc: Optional[Dict[str, Any]] = None,   # documentation dict; defaults to DQX_GENERATED_CHECKS_CONFIG_METADATA\n",
    "    ):\n",
    "        self.scope = scope.lower().strip()\n",
    "        self.source = source\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_yaml = output_yaml\n",
    "        self.output_table = output_table\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_prefix_regex = exclude_prefix_regex\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.key_order = key_order\n",
    "        self.include_table_name = include_table_name\n",
    "        self.yaml_metadata = yaml_metadata\n",
    "        self.table_doc = table_doc or DQX_GENERATED_CHECKS_CONFIG_METADATA\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "        self._validate_top_level()\n",
    "\n",
    "    # -------------------\n",
    "    # Validation\n",
    "    # -------------------\n",
    "    def _validate_top_level(self):\n",
    "        allowed_scopes = {\"pipeline\", \"catalog\", \"schema\", \"table\", \"file\", \"folder\"}\n",
    "        if self.scope not in allowed_scopes:\n",
    "            raise ValueError(f\"Invalid scope '{self.scope}'. Must be one of: {sorted(allowed_scopes)}.\")\n",
    "\n",
    "        allowed_formats = {\"yaml\", \"table\", \"both\"}\n",
    "        if self.output_format not in allowed_formats:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table' or 'both'.\")\n",
    "\n",
    "        # Source expectations per scope (explicit notes)\n",
    "        # scope=\"pipeline\" -> source=\"pipeline_name1,pipeline_name2\"\n",
    "        # scope=\"catalog\"  -> source=\"catalog\"\n",
    "        # scope=\"schema\"   -> source=\"catalog.schema\"\n",
    "        # scope=\"table\"    -> source=\"catalog.schema.table[,catalog.schema.table]\"\n",
    "        # scope=\"file\"     -> source=\"<path to YAML file listing tables>\"\n",
    "        # scope=\"folder\"   -> source=\"<path to folder with YAML table lists>\"\n",
    "        if self.scope == \"catalog\":\n",
    "            if \".\" in self.source:\n",
    "                raise ValueError(\"For scope='catalog', pass just the catalog name (no dots).\")\n",
    "        if self.scope == \"schema\":\n",
    "            if self.source.count(\".\") != 1:\n",
    "                raise ValueError(\"For scope='schema', pass 'catalog.schema'.\")\n",
    "        if self.scope == \"table\":\n",
    "            for t in [x.strip() for x in self.source.split(\",\") if x.strip()]:\n",
    "                if t.count(\".\") != 2:\n",
    "                    raise ValueError(f\"Invalid table FQN '{t}'. Use catalog.schema.table\")\n",
    "        if self.scope == \"file\":\n",
    "            if not _is_yaml_path(self.source):\n",
    "                raise ValueError(\"For scope='file', source must be a YAML path.\")\n",
    "        # folder: any path is ok; we'll scan recursively\n",
    "\n",
    "        # Sinks\n",
    "        if self.output_format == \"yaml\" and not self.output_yaml:\n",
    "            raise ValueError(\"output_yaml is required when output_format='yaml'.\")\n",
    "        if self.output_format == \"table\" and not self.output_table:\n",
    "            raise ValueError(\"output_table is required when output_format='table'.\")\n",
    "        if self.output_format == \"both\":\n",
    "            if not self.output_yaml or not self.output_table:\n",
    "                raise ValueError(\"When output_format='both', both output_yaml and output_table are required.\")\n",
    "\n",
    "    # -------------------\n",
    "    # Discovery\n",
    "    # -------------------\n",
    "    def _walk_yaml_files(self, folder: str) -> List[str]:\n",
    "        return ListYAMLPathsInFolder(folder)\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"scope:            {self.scope}\")\n",
    "        print(f\"source:           {self.source}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_yaml:      {self.output_yaml}\")\n",
    "        print(f\"output_table:     {self.output_table}\")\n",
    "        print(f\"exclude_prefix_rx:{self.exclude_prefix_regex}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"key_order:        {self.key_order}\")\n",
    "        print(f\"include_table_name: {self.include_table_name}\")\n",
    "        print(f\"yaml_metadata:    {self.yaml_metadata}\")\n",
    "        print(\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "\n",
    "        if self.scope == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipeline_names = [p.strip() for p in self.source.split(\",\") if p.strip()]\n",
    "            for pipeline_name in pipeline_names:\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                discovered += [x for x in pipeline_tables if x]\n",
    "\n",
    "        elif self.scope == \"catalog\":\n",
    "            print(\"Discovering all tables in catalog...\")\n",
    "            catalog = self.source.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                discovered += [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.scope == \"schema\":\n",
    "            print(\"Discovering all tables in schema...\")\n",
    "            catalog, schema = self.source.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            discovered = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.scope == \"table\":\n",
    "            print(\"Using provided fully-qualified table(s)...\")\n",
    "            discovered = [t.strip() for t in self.source.split(\",\") if t.strip()]\n",
    "\n",
    "        elif self.scope == \"file\":\n",
    "            print(\"Reading table list from YAML file...\")\n",
    "            discovered = _discover_tables_from_yaml_file(self.source)\n",
    "\n",
    "        else:  # folder\n",
    "            print(\"Reading table lists from all YAML files in folder (recursive)...\")\n",
    "            yaml_files = self._walk_yaml_files(self.source)\n",
    "            agg: List[str] = []\n",
    "            for yp in yaml_files:\n",
    "                try:\n",
    "                    agg += _discover_tables_from_yaml_file(yp)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Skipping YAML '{yp}': {e}\")\n",
    "            discovered = sorted(set(agg))\n",
    "\n",
    "        discovered = _filter_by_prefix_regex(discovered, self.exclude_prefix_regex)\n",
    "        _display_table_preview(self.spark, discovered, title=\"Final table list to generate DQX rules for\")\n",
    "        print(\"==========================================\\n\")\n",
    "        return discovered\n",
    "\n",
    "    # -------------------\n",
    "    # Profiler call args\n",
    "    # -------------------\n",
    "    def _profile_call_kwargs(self) -> Dict[str, Any]:\n",
    "        kwargs: Dict[str, Any] = {}\n",
    "        if self.columns is not None:\n",
    "            kwargs[\"cols\"] = self.columns\n",
    "        if self.profile_options:\n",
    "            unknown = sorted(set(self.profile_options) - DOC_SUPPORTED_KEYS)\n",
    "            if unknown:\n",
    "                print(f\"[INFO] Profiling options not in current docs (passing through anyway): {unknown}\")\n",
    "            kwargs[\"options\"] = self.profile_options\n",
    "        return kwargs\n",
    "\n",
    "    # -------------------\n",
    "    # Rule shaping\n",
    "    # -------------------\n",
    "    def _dq_constraint_to_check(self, rule_name: str, constraint_sql: str, table_name: str) -> Dict[str, Any]:\n",
    "        d = {\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": self.criticality,\n",
    "            \"run_config_name\": self.run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    # remove duplicate 'name' from arguments per your preference\n",
    "                    \"expression\": constraint_sql,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if self.include_table_name:\n",
    "            d = {\"table_name\": table_name, **d}  # keep table_name first\n",
    "        return d\n",
    "\n",
    "    # -------------------\n",
    "    # YAML emission (header + list items with blank lines)\n",
    "    # -------------------\n",
    "    def _yaml_header_block(self, table_fqn: str, env_info: Dict[str, Any]) -> str:\n",
    "        dashed = \"-\" * 81\n",
    "        lines = [\n",
    "            \"#\" * 76,\n",
    "            f\"# GENERATED DQX CHECKS\",\n",
    "            f\"# Table: {table_fqn}\",\n",
    "            f\"# Generated at (UTC): {env_info.get('utc_time','')}\",\n",
    "            f\"# Notebook: {env_info.get('notebook_path','Unknown')}\",\n",
    "            f\"# Spark: {env_info.get('spark_version','')}  |  Python: {env_info.get('python_version','')}\",\n",
    "            f\"# Cluster: {env_info.get('cluster_name','')} ({env_info.get('cluster_id','')})  |  Executor memory: {env_info.get('executor_memory','')}\",\n",
    "            \"#\" * 76,\n",
    "            \"\",\n",
    "            f\"# {dashed}\",\n",
    "            \"# Profile options:\",\n",
    "            \"# \" + _safe_json(self.profile_options),\n",
    "            \"# Generator settings:\",\n",
    "            \"# \" + _safe_json({\n",
    "                \"scope\": self.scope,\n",
    "                \"source\": self.source,\n",
    "                \"output_format\": self.output_format,\n",
    "                \"output_yaml\": self.output_yaml,\n",
    "                \"output_table\": self.output_table,\n",
    "                \"criticality\": self.criticality,\n",
    "                \"run_config_name\": self.run_config_name,\n",
    "                \"include_table_name\": self.include_table_name,\n",
    "                \"key_order\": self.key_order,\n",
    "                \"exclude_prefix_regex\": self.exclude_prefix_regex,\n",
    "            }),\n",
    "        ]\n",
    "        return \"\\n\".join(lines) + \"\\n\\n\"\n",
    "\n",
    "    def _dump_rules_as_yaml_stream(self, rules: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Emit a single YAML document that is a list of rule objects:\n",
    "        - table_name: ...\n",
    "          name: ...\n",
    "          ...\n",
    "        (Blank line between items for readability.)\n",
    "        \"\"\"\n",
    "        pieces: List[str] = []\n",
    "        for r in rules:\n",
    "            block = yaml.safe_dump(r, sort_keys=False, default_flow_style=False).rstrip()\n",
    "            lines = block.splitlines()\n",
    "            if not lines:\n",
    "                continue\n",
    "            first = f\"- {lines[0]}\"\n",
    "            rest = \"\\n\".join((\"  \" + ln) for ln in lines[1:])\n",
    "            pieces.append(first + (\"\\n\" + rest if rest else \"\"))\n",
    "        return \"\\n\\n\".join(pieces) + \"\\n\"\n",
    "\n",
    "    # -------------------\n",
    "    # Table write helpers\n",
    "    # -------------------\n",
    "    def _ensure_schema_exists(self, fqn: str):\n",
    "        cat, sch, _ = fqn.split(\".\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "\n",
    "    def _write_rows_to_table(self, fqn: str, rows: List[Dict[str, Any]], mode: str = \"append\"):\n",
    "        self._ensure_schema_exists(fqn)\n",
    "        existed = self.spark.catalog.tableExists(fqn)\n",
    "        if not existed:\n",
    "            # create empty table with correct schema first, then apply docs\n",
    "            empty_df = self.spark.createDataFrame([], DQX_GENERATED_CHECKS_CONFIG_SCHEMA)\n",
    "            empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(fqn)\n",
    "        # Apply docs (table comment only on create; column comments always with fallback)\n",
    "        _apply_table_documentation_on_create(self.spark, fqn, {**self.table_doc, \"table\": fqn}, just_created=(not existed))\n",
    "\n",
    "        df = self.spark.createDataFrame(rows, schema=DQX_GENERATED_CHECKS_CONFIG_SCHEMA)\n",
    "        df.write.format(\"delta\").mode(mode).saveAsTable(fqn)\n",
    "        print(f\"[WRITE] {len(rows)} rows -> {fqn} ({mode})\")\n",
    "\n",
    "    # -------------------\n",
    "    # Summary display\n",
    "    # -------------------\n",
    "    def _show_summary_table(self, summary: Dict[str, Dict[str, Any]]):\n",
    "        if not summary:\n",
    "            display_section(\"Checks written per table\")\n",
    "            print(\"(no tables processed)\")\n",
    "            return\n",
    "        rows = []\n",
    "        for t, s in summary.items():\n",
    "            rows.append((\n",
    "                s.get(\"table_name\", t),\n",
    "                int(s.get(\"checks_generated\", 0)),\n",
    "                bool(s.get(\"wrote_yaml\", False)),\n",
    "                s.get(\"yaml_path\", None),\n",
    "                int(s.get(\"table_rows_written\", 0)),\n",
    "                self.output_table or \"\",\n",
    "            ))\n",
    "        schema = \"table_name string, checks_generated int, wrote_yaml boolean, yaml_path string, table_rows_written int, output_table string\"\n",
    "        df = self.spark.createDataFrame(rows, schema=schema)\n",
    "        display_section(\"Checks written per table\")\n",
    "        show_df(df.orderBy(\"table_name\"))\n",
    "\n",
    "    # -------------------\n",
    "    # Main\n",
    "    # -------------------\n",
    "    def run(self):\n",
    "        dq_engine = DQEngine(WorkspaceClient())\n",
    "        profiler = DQProfiler(WorkspaceClient())\n",
    "        generator = DQDltGenerator(WorkspaceClient())\n",
    "\n",
    "        env_info = print_notebook_env(self.spark)  # prints banner and returns dict\n",
    "        call_kwargs = self._profile_call_kwargs()\n",
    "        tables = self._discover_tables()\n",
    "\n",
    "        all_rows_for_table_sink: List[Dict[str, Any]] = []\n",
    "        written_yaml_paths: List[str] = []\n",
    "\n",
    "        # Per-table summary tracking\n",
    "        per_table_summary: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        for fq in tables:\n",
    "            if fq.count(\".\") != 2:\n",
    "                print(f\"[WARN] Skipping invalid table name: {fq}\")\n",
    "                continue\n",
    "\n",
    "            cat, sch, tab = fq.split(\".\")\n",
    "            per_table_summary.setdefault(fq, {\n",
    "                \"table_name\": fq,\n",
    "                \"checks_generated\": 0,\n",
    "                \"wrote_yaml\": False,\n",
    "                \"yaml_path\": None,\n",
    "                \"table_rows_written\": 0,\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                self.spark.table(fq).limit(1).collect()  # readability probe\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Table not readable: {fq} -> {e}\")\n",
    "                continue\n",
    "\n",
    "            # Profile & generate DLT rules\n",
    "            try:\n",
    "                df = self.spark.table(fq)\n",
    "                _, profiles = profiler.profile(df, **call_kwargs)\n",
    "                rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Profiling/rule-gen failed for {fq}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Shape checks\n",
    "            checks: List[Dict[str, Any]] = []\n",
    "            for rule_name, constraint_sql in (rules_dict or {}).items():\n",
    "                checks.append(self._dq_constraint_to_check(rule_name, constraint_sql, fq))\n",
    "\n",
    "            per_table_summary[fq][\"checks_generated\"] = len(checks)\n",
    "\n",
    "            if not checks:\n",
    "                print(f\"[INFO] No checks generated for {fq}.\")\n",
    "                continue\n",
    "\n",
    "            # YAML sink\n",
    "            yaml_path_for_rows: str = f\"<generated://{fq}>\"\n",
    "            if self.output_format in {\"yaml\", \"both\"}:\n",
    "                if self.output_yaml.endswith((\".yaml\", \".yml\")):\n",
    "                    path = self.output_yaml  # explicit file path (edge case)\n",
    "                else:\n",
    "                    path = f\"{self.output_yaml.rstrip('/')}/{tab}.yaml\"\n",
    "\n",
    "                if self.key_order == \"engine\":\n",
    "                    cfg = self._infer_file_storage_config(path)\n",
    "                    dq_engine.save_checks(checks, config=cfg)  # writes list-of-dicts\n",
    "                else:\n",
    "                    header = self._yaml_header_block(fq, env_info) if self.yaml_metadata else \"\"\n",
    "                    body = self._dump_rules_as_yaml_stream(checks)\n",
    "                    _write_text_any(path, header + body)\n",
    "\n",
    "                yaml_path_for_rows = path\n",
    "                written_yaml_paths.append(path)\n",
    "                per_table_summary[fq][\"wrote_yaml\"] = True\n",
    "                per_table_summary[fq][\"yaml_path\"] = path\n",
    "                print(f\"[RUN] Wrote {len(checks)} rule(s) to YAML: {path}\")\n",
    "\n",
    "            # Prepare table rows (for TABLE only path; BOTH path reloads YAMLs to avoid drift)\n",
    "            if self.output_format == \"table\":\n",
    "                gen_meta = [\n",
    "                    {\"section\": \"profile_options\", \"payload\": _stringify_map_values(self.profile_options)},\n",
    "                    {\"section\": \"generator_settings\", \"payload\": _stringify_map_values({\n",
    "                        \"scope\": self.scope, \"source\": self.source, \"output_format\": self.output_format,\n",
    "                        \"output_yaml\": self.output_yaml or \"\", \"output_table\": self.output_table or \"\",\n",
    "                        \"criticality\": self.criticality, \"run_config_name\": self.run_config_name,\n",
    "                        \"include_table_name\": self.include_table_name, \"key_order\": self.key_order,\n",
    "                        \"exclude_prefix_regex\": self.exclude_prefix_regex or \"\",\n",
    "                    })},\n",
    "                ]\n",
    "                for rule in checks:\n",
    "                    raw_check = rule[\"check\"]\n",
    "                    payload = _compute_check_id_payload(fq, raw_check, rule.get(\"filter\"))\n",
    "                    all_rows_for_table_sink.append({\n",
    "                        \"check_id\": _compute_check_id(payload),\n",
    "                        \"check_id_payload\": payload,\n",
    "                        \"table_name\": fq,\n",
    "                        \"name\": rule[\"name\"],\n",
    "                        \"criticality\": rule[\"criticality\"],\n",
    "                        \"check\": {\n",
    "                            \"function\": raw_check.get(\"function\"),\n",
    "                            \"for_each_column\": raw_check.get(\"for_each_column\"),\n",
    "                            \"arguments\": _stringify_map_values(raw_check.get(\"arguments\") or {}),\n",
    "                        },\n",
    "                        \"filter\": rule.get(\"filter\"),\n",
    "                        \"run_config_name\": rule[\"run_config_name\"],\n",
    "                        \"user_metadata\": _stringify_map_values(rule.get(\"user_metadata\") or None) or None,\n",
    "                        \"yaml_path\": yaml_path_for_rows,\n",
    "                        \"active\": True,\n",
    "                        \"generator_meta\": gen_meta,\n",
    "                        \"created_by\": self.created_by,\n",
    "                        \"created_at\": _now_iso(),\n",
    "                        \"updated_by\": None,\n",
    "                        \"updated_at\": None,\n",
    "                    })\n",
    "\n",
    "        # BOTH → reload the exact YAMLs we wrote and write those rows (canonical)\n",
    "        if self.output_format == \"both\":\n",
    "            rows_from_yaml: List[Dict[str, Any]] = []\n",
    "            for yp in written_yaml_paths:\n",
    "                try:\n",
    "                    txt = _read_text_any(yp)\n",
    "                    docs = list(yaml.safe_load_all(io.StringIO(txt)))\n",
    "                    rules: List[dict] = []\n",
    "                    for d in docs:\n",
    "                        if not d:\n",
    "                            continue\n",
    "                        if isinstance(d, dict):\n",
    "                            rules.append(d)\n",
    "                        elif isinstance(d, list):\n",
    "                            rules.extend([x for x in d if isinstance(x, dict)])\n",
    "\n",
    "                    # If file is a single list (our custom format), docs will be [list]; handled above.\n",
    "                    for r in rules:\n",
    "                        fq = r.get(\"table_name\")\n",
    "                        raw_check = r.get(\"check\") or {}\n",
    "                        payload = _compute_check_id_payload(fq, raw_check, r.get(\"filter\"))\n",
    "                        row_obj = {\n",
    "                            \"check_id\": _compute_check_id(payload),\n",
    "                            \"check_id_payload\": payload,\n",
    "                            \"table_name\": fq,\n",
    "                            \"name\": r.get(\"name\"),\n",
    "                            \"criticality\": r.get(\"criticality\"),\n",
    "                            \"check\": {\n",
    "                                \"function\": raw_check.get(\"function\"),\n",
    "                                \"for_each_column\": raw_check.get(\"for_each_column\"),\n",
    "                                \"arguments\": _stringify_map_values(raw_check.get(\"arguments\") or {}),\n",
    "                            },\n",
    "                            \"filter\": r.get(\"filter\"),\n",
    "                            \"run_config_name\": r.get(\"run_config_name\", self.run_config_name),\n",
    "                            \"user_metadata\": _stringify_map_values(r.get(\"user_metadata\") or None) or None,\n",
    "                            \"yaml_path\": yp,\n",
    "                            \"active\": True,\n",
    "                            \"generator_meta\": [\n",
    "                                {\"section\": \"profile_options\", \"payload\": _stringify_map_values(self.profile_options)},\n",
    "                                {\"section\": \"generator_settings\", \"payload\": _stringify_map_values({\n",
    "                                    \"scope\": self.scope, \"source\": self.source, \"output_format\": self.output_format,\n",
    "                                    \"output_yaml\": self.output_yaml or \"\", \"output_table\": self.output_table or \"\",\n",
    "                                    \"criticality\": self.criticality,\n",
    "                                    \"run_config_name\": r.get(\"run_config_name\", self.run_config_name),\n",
    "                                    \"include_table_name\": self.include_table_name, \"key_order\": self.key_order,\n",
    "                                    \"exclude_prefix_regex\": self.exclude_prefix_regex or \"\",\n",
    "                                })},\n",
    "                            ],\n",
    "                            \"created_by\": self.created_by,\n",
    "                            \"created_at\": _now_iso(),\n",
    "                            \"updated_by\": None,\n",
    "                            \"updated_at\": None,\n",
    "                        }\n",
    "                        rows_from_yaml.append(row_obj)\n",
    "                        # count per-table table_rows_written (we'll add to summary after the write too)\n",
    "                        if fq in per_table_summary:\n",
    "                            per_table_summary[fq][\"table_rows_written\"] = per_table_summary[fq].get(\"table_rows_written\", 0) + 1\n",
    "                        else:\n",
    "                            per_table_summary[fq] = {\n",
    "                                \"table_name\": fq,\n",
    "                                \"checks_generated\": 0,\n",
    "                                \"wrote_yaml\": True,\n",
    "                                \"yaml_path\": yp,\n",
    "                                \"table_rows_written\": 1,\n",
    "                            }\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Could not load back YAML '{yp}' for table sink: {e}\")\n",
    "\n",
    "            if self.output_table and rows_from_yaml:\n",
    "                self._write_rows_to_table(self.output_table, rows_from_yaml, mode=\"append\")\n",
    "            print(f\"[DONE] Wrote YAML files ({len(written_yaml_paths)}). Then loaded {len(rows_from_yaml)} rows into {self.output_table}.\")\n",
    "\n",
    "        elif self.output_format == \"table\":\n",
    "            if self.output_table and all_rows_for_table_sink:\n",
    "                # update per-table counts before write\n",
    "                for r in all_rows_for_table_sink:\n",
    "                    fq = r[\"table_name\"]\n",
    "                    per_table_summary.setdefault(fq, {\n",
    "                        \"table_name\": fq,\n",
    "                        \"checks_generated\": 0,\n",
    "                        \"wrote_yaml\": False,\n",
    "                        \"yaml_path\": None,\n",
    "                        \"table_rows_written\": 0,\n",
    "                    })\n",
    "                    per_table_summary[fq][\"table_rows_written\"] = per_table_summary[fq].get(\"table_rows_written\", 0) + 1\n",
    "\n",
    "                self._write_rows_to_table(self.output_table, all_rows_for_table_sink, mode=\"append\")\n",
    "            print(f\"[DONE] Wrote {len(all_rows_for_table_sink)} rows into {self.output_table}.\")\n",
    "        else:\n",
    "            print(f\"[DONE] Wrote YAML files ({len(written_yaml_paths)}).\")\n",
    "\n",
    "        # Print a nice per-table summary\n",
    "        self._show_summary_table(per_table_summary)\n",
    "\n",
    "    # Storage config passthrough (kept)\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "\n",
    "# -------------------- Usage examples --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": 42,\n",
    "        \"limit\": 1000,\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_in_count\": 10,\n",
    "        \"round\": True,\n",
    "        # other passthrough keys are fine\n",
    "    }\n",
    "\n",
    "    # Example A — scope=\"table\": write BOTH (YAMLs first, then load exactly those YAMLs into the table)\n",
    "    CheckGenerator(\n",
    "        scope=\"table\",                                  # \"pipeline\" | \"catalog\" | \"schema\" | \"table\" | \"file\" | \"folder\"\n",
    "        source=\"dq_prd.monitoring.job_run_audit\",\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\" | \"both\"\n",
    "        output_yaml=\"/Volumes/dq_dev/dqx/generated_checks/\",  # Workspace Folder or Volume\n",
    "        yaml_metadata=True,      #(yaml_modifier)      # True = Add run metadata to file | False = Don't add metadata to file\n",
    "        key_order=\"custom\",      #(yaml_modifier)      # \"custom\" = our ordered YAML with list items; \"engine\" = DQX default writer\n",
    "        include_table_name=True, #(yaml_modifier)\n",
    "        output_table=\"dq_dev.dqx.generated_checks_config\",  # 'Table_fqn'  | 'None' if not writing to table\n",
    "        profile_options=profile_options,\n",
    "        exclude_prefix_regex=r\"^tama\",               # exclude tables whose prefix (before _) matches\n",
    "        created_by=\"LMG\",          #(table_modifier)      # populates 'created_by' column\n",
    "        columns=None,              #(check_modifier)      # only valid when scope==\"table\"\n",
    "        run_config_name=\"default\", #(check_modifier)\n",
    "        criticality=\"error\",       #(check_modifier)       # \"error\" | \"warn\"\n",
    "        table_doc=DQX_GENERATED_CHECKS_CONFIG_METADATA,   # used if/when we create the output table\n",
    "    ).run()\n",
    "\n",
    "    \"\"\"\n",
    "    # Example B — scope=\"catalog\": write BOTH (YAMLs first, then load those YAMLs into table)\n",
    "    CheckGenerator(\n",
    "        scope=\"catalog\",\n",
    "        source=\"de_prd\",\n",
    "        output_format=\"both\",\n",
    "        output_yaml=\"dbfs:/mnt/dqx/generated_checks/de_prd\",\n",
    "        output_table=\"dq_dev.dqx.checks_generated_config\",\n",
    "        profile_options=profile_options,\n",
    "        exclude_prefix_regex=r\"^tamarack$\",\n",
    "        created_by=\"LMG\",\n",
    "        columns=None,\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",\n",
    "        include_table_name=True,\n",
    "        yaml_metadata=True,\n",
    "        table_doc=DQX_GENERATED_CHECKS_CONFIG_METADATA,\n",
    "    ).run()\n",
    "\n",
    "    # Example C — scope=\"table\": write directly to table (no YAML)\n",
    "    CheckGenerator(\n",
    "        scope=\"table\",\n",
    "        source=\"dq_prd.monitoring.job_run_audit\",\n",
    "        output_format=\"table\",\n",
    "        output_yaml=None,\n",
    "        output_table=\"dq_dev.dqx.checks_generated_config\",\n",
    "        profile_options=profile_options,\n",
    "        exclude_prefix_regex=None,\n",
    "        created_by=\"LMG\",\n",
    "        columns=None,\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",\n",
    "        include_table_name=True,\n",
    "        yaml_metadata=False,\n",
    "        table_doc=DQX_GENERATED_CHECKS_CONFIG_METADATA,\n",
    "    ).run()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a6fe1d-db73-4f35-8836-3a6699be6fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56df5f91-8280-4e7d-a475-38f194784f1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Generated Checks - 'generated_checks_config' --> 'generated_checks_log'"
    }
   },
   "outputs": [],
   "source": [
    "# === DQX runner with table prefix filtering (glob) ===\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import json, fnmatch\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from pyspark.sql import SparkSession, DataFrame, Row, functions as F, types as T\n",
    "\n",
    "from utils.color import Color\n",
    "\n",
    "# -------------------\n",
    "# Display helpers\n",
    "# -------------------\n",
    "def _can_display() -> bool:\n",
    "    return \"display\" in globals()\n",
    "\n",
    "def show_df(df: DataFrame, n: int = 100, truncate: bool = False) -> None:\n",
    "    if _can_display():\n",
    "        display(df.limit(n))\n",
    "    else:\n",
    "        df.show(n, truncate=truncate)\n",
    "\n",
    "def display_section(title: str) -> None:\n",
    "    print(\"\\n\" + f\"{Color.b}{Color.light_seafoam}═{Color.r}\" *80)\n",
    "    print(f\"{Color.b}{Color.light_seafoam}║{Color.r} {Color.b}{Color.ghost_white}{title}{Color.r}\")\n",
    "    print(f\"{Color.b}{Color.light_seafoam}═{Color.r}\"* 80)\n",
    "\n",
    "# -------------\n",
    "# Result schema (row-level hits)\n",
    "# -------------\n",
    "ROW_LOG_SCHEMA = T.StructType([\n",
    "    T.StructField(\"log_id\",                      T.StringType(),  False),\n",
    "    T.StructField(\"check_id\",                    T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"table_name\",                  T.StringType(),  False),\n",
    "    T.StructField(\"run_config_name\",             T.StringType(),  False),\n",
    "\n",
    "    T.StructField(\"_errors\", T.ArrayType(T.StructType([\n",
    "        T.StructField(\"name\",                   T.StringType(), True),\n",
    "        T.StructField(\"message\",                T.StringType(), True),\n",
    "        T.StructField(\"columns\",                T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"filter\",                 T.StringType(), True),\n",
    "        T.StructField(\"function\",               T.StringType(), True),\n",
    "        T.StructField(\"run_time\",               T.TimestampType(), True),\n",
    "        T.StructField(\"user_metadata\",          T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ])), False),\n",
    "    T.StructField(\"_errors_fingerprint\",         T.StringType(),  False),\n",
    "\n",
    "    T.StructField(\"_warnings\", T.ArrayType(T.StructType([\n",
    "        T.StructField(\"name\",                   T.StringType(), True),\n",
    "        T.StructField(\"message\",                T.StringType(), True),\n",
    "        T.StructField(\"columns\",                T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"filter\",                 T.StringType(), True),\n",
    "        T.StructField(\"function\",               T.StringType(), True),\n",
    "        T.StructField(\"run_time\",               T.TimestampType(), True),\n",
    "        T.StructField(\"user_metadata\",          T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ])), False),\n",
    "    T.StructField(\"_warnings_fingerprint\",       T.StringType(),  False),\n",
    "\n",
    "    T.StructField(\"row_snapshot\", T.ArrayType(T.StructType([\n",
    "        T.StructField(\"column\",                 T.StringType(), False),\n",
    "        T.StructField(\"value\",                  T.StringType(), True),\n",
    "    ])), False),\n",
    "    T.StructField(\"row_snapshot_fingerprint\",    T.StringType(),  False),\n",
    "\n",
    "    T.StructField(\"created_by\",                  T.StringType(),  False),\n",
    "    T.StructField(\"created_at\",                  T.TimestampType(), False),\n",
    "    T.StructField(\"updated_by\",                  T.StringType(),  True),\n",
    "    T.StructField(\"updated_at\",                  T.TimestampType(), True),\n",
    "])\n",
    "\n",
    "DQX_CHECKS_LOG_METADATA: Dict[str, Any] = {\n",
    "    \"table\": \"<override at create time>\",\n",
    "    \"table_comment\": (\n",
    "        \"## **DQX Row-level Check Results Log**\\n\"\n",
    "        \"- One row per source row that triggered at least one rule (error or warn).\\n\"\n",
    "        \"- `check_id` contains the originating rule IDs from the config table.\\n\"\n",
    "        \"- Fingerprint columns are deterministic digests to aid de-duplication & rollups.\\n\"\n",
    "    ),\n",
    "    \"columns\": {\n",
    "        \"log_id\": \"Deterministic SHA-256 over table/run_config/row_snapshot/_errors/_warnings.\",\n",
    "        \"check_id\": \"Originating rule IDs attached post-hoc via join.\",\n",
    "        \"table_name\": \"Fully qualified source table (`catalog.schema.table`).\",\n",
    "        \"run_config_name\": \"Run configuration tag/group under which checks were applied.\",\n",
    "        \"_errors\": \"Array<struct> of error issues.\",\n",
    "        \"_errors_fingerprint\": \"SHA-256 of a normalized view of `_errors`.\",\n",
    "        \"_warnings\": \"Array<struct> of warning issues.\",\n",
    "        \"_warnings_fingerprint\": \"SHA-256 of a normalized view of `_warnings`.\",\n",
    "        \"row_snapshot\": \"Array<struct{column:string, value:string}> for non-reserved columns.\",\n",
    "        \"row_snapshot_fingerprint\": \"SHA-256 of JSON(row_snapshot).\",\n",
    "        \"created_by\": \"Audit: user/process that wrote this record.\",\n",
    "        \"created_at\": \"Audit: creation timestamp (UTC).\",\n",
    "        \"updated_by\": \"Audit: last updater (nullable).\",\n",
    "        \"updated_at\": \"Audit: last update timestamp (UTC, nullable).\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# -------------------\n",
    "# Comment + DDL helpers\n",
    "# -------------------\n",
    "def _esc_sql_comment(s: str) -> str:\n",
    "    return (s or \"\").replace(\"'\", \"''\")\n",
    "\n",
    "def _comment_on_table(spark: SparkSession, fqn: str, text: Optional[str]):\n",
    "    if not text:\n",
    "        return\n",
    "    cat, sch, tbl = fqn.split(\".\")\n",
    "    spark.sql(f\"COMMENT ON TABLE `{cat}`.`{sch}`.`{tbl}` IS '{_esc_sql_comment(text)}'\")\n",
    "\n",
    "def _set_column_comment_safe(spark: SparkSession, fqn: str, col_name: str, comment: str):\n",
    "    cat, sch, tbl = fqn.split(\".\")\n",
    "    escaped = _esc_sql_comment(comment)\n",
    "    try:\n",
    "        spark.sql(f\"COMMENT ON COLUMN `{cat}`.`{sch}`.`{tbl}`.`{col_name}` IS '{escaped}'\")\n",
    "        return\n",
    "    except Exception:\n",
    "        info = spark.sql(f\"DESCRIBE TABLE `{cat}`.`{sch}`.`{tbl}`\").collect()\n",
    "        types_map = {r.col_name: r.data_type for r in info if r.col_name and not r.col_name.startswith(\"#\")}\n",
    "        dt = types_map.get(col_name)\n",
    "        if not dt:\n",
    "            return\n",
    "        spark.sql(\n",
    "            f\"ALTER TABLE `{cat}`.`{sch}`.`{tbl}` CHANGE COLUMN `{col_name}` `{col_name}` {dt} COMMENT '{escaped}'\"\n",
    "        )\n",
    "\n",
    "def _apply_table_documentation_on_create(spark: SparkSession, table_fqn: str,\n",
    "                                         doc: Dict[str, Any], just_created: bool):\n",
    "    if not doc:\n",
    "        return\n",
    "    try:\n",
    "        if just_created:\n",
    "            _comment_on_table(spark, table_fqn, doc.get(\"table_comment\"))\n",
    "        cat, sch, tbl = table_fqn.split(\".\")\n",
    "        existing = {\n",
    "            r.col_name for r in spark.sql(f\"DESCRIBE TABLE `{cat}`.`{sch}`.`{tbl}`\").collect()\n",
    "            if r.col_name and not str(r.col_name).startswith(\"#\")\n",
    "        }\n",
    "        for col_name, cmt in (doc.get(\"columns\") or {}).items():\n",
    "            if col_name in existing:\n",
    "                _set_column_comment_safe(spark, table_fqn, col_name, cmt)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not apply table/column comments to {table_fqn}: {e}\")\n",
    "\n",
    "def ensure_table(full_name: str, doc: Optional[Dict[str, Any]] = None):\n",
    "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "    existed = spark.catalog.tableExists(full_name)\n",
    "    if not existed:\n",
    "        cat, sch, _ = full_name.split(\".\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "        spark.createDataFrame([], ROW_LOG_SCHEMA).write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    _apply_table_documentation_on_create(spark, full_name, doc or {}, just_created=(not existed))\n",
    "\n",
    "def _ensure_schema_exists_for_fqn(fqn: str) -> None:\n",
    "    if not fqn:\n",
    "        return\n",
    "    try:\n",
    "        cat, sch, _ = fqn.split(\".\")\n",
    "    except ValueError:\n",
    "        return\n",
    "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "\n",
    "# --------------------------\n",
    "# Issue array helpers\n",
    "# --------------------------\n",
    "def _pick_col(df: DataFrame, *candidates: str) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _empty_issues_array() -> F.Column:\n",
    "    elem = T.StructType([\n",
    "        T.StructField(\"name\",          T.StringType(), True),\n",
    "        T.StructField(\"message\",       T.StringType(), True),\n",
    "        T.StructField(\"columns\",       T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"filter\",        T.StringType(), True),\n",
    "        T.StructField(\"function\",      T.StringType(), True),\n",
    "        T.StructField(\"run_time\",      T.TimestampType(), True),\n",
    "        T.StructField(\"user_metadata\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ])\n",
    "    return F.from_json(F.lit(\"[]\"), T.ArrayType(elem))\n",
    "\n",
    "def _normalize_issues_for_fp(arr_col: F.Column) -> F.Column:\n",
    "    return F.transform(\n",
    "        arr_col,\n",
    "        lambda r: F.struct(\n",
    "            r[\"name\"].alias(\"name\"),\n",
    "            r[\"message\"].alias(\"message\"),\n",
    "            F.coalesce(F.to_json(F.array_sort(r[\"columns\"])), F.lit(\"[]\")).alias(\"columns_json\"),\n",
    "            r[\"filter\"].alias(\"filter\"),\n",
    "            r[\"function\"].alias(\"function\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# JIT argument coercion (exec-time only)\n",
    "# --------------------------\n",
    "_EXPECTED: Dict[str, Dict[str, str]] = {\n",
    "    \"is_unique\": {\"columns\": \"list\"},\n",
    "    \"is_in_list\": {\"column\": \"str\", \"allowed\": \"list\"},\n",
    "    \"is_in_range\": {\"column\": \"str\", \"min_limit\": \"num\", \"max_limit\": \"num\",\n",
    "                    \"inclusive_min\": \"bool\", \"inclusive_max\": \"bool\"},\n",
    "    \"regex_match\": {\"column\": \"str\", \"regex\": \"str\"},\n",
    "    \"sql_expression\": {\"expression\": \"str\"},\n",
    "    \"sql_query\": {\"query\": \"str\", \"limit\": \"num\"},\n",
    "    \"is_not_null\": {\"column\": \"str\"},\n",
    "    \"is_not_null_and_not_empty\": {\"column\": \"str\"},\n",
    "}\n",
    "\n",
    "def _parse_scalar(s: Optional[str]):\n",
    "    if s is None: return None\n",
    "    s = s.strip()\n",
    "    sl = s.lower()\n",
    "    if sl in (\"null\", \"none\", \"\"): return None\n",
    "    if sl == \"true\": return True\n",
    "    if sl == \"false\": return False\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"{\") and s.endswith(\"}\")):\n",
    "        try: return json.loads(s)\n",
    "        except Exception: return s\n",
    "    try:\n",
    "        return int(s) if s.lstrip(\"+-\").isdigit() else float(s)\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def _to_list(v):\n",
    "    if v is None: return []\n",
    "    if isinstance(v, list): return v\n",
    "    if isinstance(v, str) and v.strip().startswith(\"[\"):\n",
    "        try: return json.loads(v)\n",
    "        except Exception: return [v]\n",
    "    return [v]\n",
    "\n",
    "def _to_num(v):\n",
    "    if v is None: return None\n",
    "    if isinstance(v, (int, float)): return v\n",
    "    try: return int(v) if str(v).lstrip(\"+-\").isdigit() else float(v)\n",
    "    except Exception: return v\n",
    "\n",
    "def _to_bool(v):\n",
    "    if isinstance(v, bool): return v\n",
    "    if isinstance(v, str):\n",
    "        vl = v.strip().lower()\n",
    "        if vl in (\"true\", \"t\", \"1\"): return True\n",
    "        if vl in (\"false\", \"f\", \"0\"): return False\n",
    "    return v\n",
    "\n",
    "def _coerce_arguments(args_map: Optional[Dict[str, str]],\n",
    "                      function_name: Optional[str],\n",
    "                      mode: str = \"permissive\") -> Tuple[Dict[str, Any], List[str]]:\n",
    "    if not args_map: return {}, []\n",
    "    raw = {k: _parse_scalar(v) for k, v in args_map.items()}\n",
    "    spec = _EXPECTED.get((function_name or \"\").strip(), {})\n",
    "    out: Dict[str, Any] = {}\n",
    "    errs: List[str] = []\n",
    "    for k, v in raw.items():\n",
    "        want = spec.get(k)\n",
    "        if want == \"list\":\n",
    "            out[k] = _to_list(v)\n",
    "            if not isinstance(out[k], list):\n",
    "                errs.append(f\"key '{k}' expected list, got {type(out[k]).__name__}\")\n",
    "        elif want == \"num\":\n",
    "            out[k] = _to_num(v)\n",
    "        elif want == \"bool\":\n",
    "            out[k] = _to_bool(v)\n",
    "        elif want == \"str\":\n",
    "            out[k] = \"\" if v is None else str(v)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    if (function_name or \"\").strip() == \"sql_query\" and (\"limit\" not in out or out.get(\"limit\") in (None, 0)):\n",
    "        errs.append(\"sql_query requires a positive 'limit'\")\n",
    "    if mode == \"strict\" and errs:\n",
    "        raise ValueError(f\"Argument coercion failed for '{function_name}': {errs}\")\n",
    "    return out, errs\n",
    "\n",
    "# --------------------------\n",
    "# Table-name filtering helpers (glob -> Python & SQL LIKE)\n",
    "# --------------------------\n",
    "def _normalize_glob(p: str) -> str:\n",
    "    # Accept bare table patterns like \"crm_*\" => match any catalog/schema\n",
    "    if \".\" not in p:\n",
    "        return f\"*.*.{p}\"\n",
    "    return p\n",
    "\n",
    "def _glob_to_sql_like(p: str) -> str:\n",
    "    # naive: '*' -> '%', '?' -> '_' ; if you need literal %/_ you can add ESCAPE later\n",
    "    p = _normalize_glob(p)\n",
    "    return p.replace(\"*\", \"%\").replace(\"?\", \"_\")\n",
    "\n",
    "def _matches_any_glob(name: str, patterns: Optional[List[str]]) -> bool:\n",
    "    if not patterns:\n",
    "        return True\n",
    "    for g in patterns:\n",
    "        if fnmatch.fnmatchcase(name, _normalize_glob(g)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# --------------------------\n",
    "# Load rules from table (with optional table LIKE filters)\n",
    "# --------------------------\n",
    "def _group_by_table(rules: List[dict]) -> Dict[str, List[dict]]:\n",
    "    out: Dict[str, List[dict]] = {}\n",
    "    for r in rules:\n",
    "        out.setdefault(r[\"table_name\"], []).append(r)\n",
    "    return out\n",
    "\n",
    "def _load_checks_from_table_as_dicts(\n",
    "    spark: SparkSession,\n",
    "    checks_table: str,\n",
    "    run_config_name: str,\n",
    "    coercion_mode: str = \"permissive\",\n",
    "    include_tables_glob: Optional[List[str]] = None,   # NEW: pushdown filter\n",
    ") -> Tuple[Dict[str, List[dict]], int, int]:\n",
    "    df = (\n",
    "        spark.table(checks_table)\n",
    "        .where((F.col(\"run_config_name\") == run_config_name) & (F.col(\"active\") == True))\n",
    "        .select(\"table_name\", \"name\", \"criticality\", \"filter\",\n",
    "                \"run_config_name\", \"user_metadata\", \"check\")\n",
    "    )\n",
    "\n",
    "    if include_tables_glob:\n",
    "        likes = [_glob_to_sql_like(g) for g in include_tables_glob]\n",
    "        cond = \" OR \".join([f\"table_name LIKE '{lk}'\" for lk in likes])\n",
    "        df = df.where(F.expr(cond))\n",
    "\n",
    "    rows = [r.asDict(recursive=True) for r in df.collect()]\n",
    "    raw_rules: List[dict] = []\n",
    "    coerced: int = 0\n",
    "\n",
    "    for r in rows:\n",
    "        chk = r.get(\"check\") or {}\n",
    "        fn  = chk.get(\"function\")\n",
    "        fec = chk.get(\"for_each_column\")\n",
    "        args, _errs = _coerce_arguments(chk.get(\"arguments\"), fn, mode=coercion_mode)\n",
    "        coerced += 1\n",
    "        raw_rules.append({\n",
    "            \"table_name\":       r[\"table_name\"],\n",
    "            \"name\":             r[\"name\"],\n",
    "            \"criticality\":      r[\"criticality\"],\n",
    "            \"run_config_name\":  r[\"run_config_name\"],\n",
    "            \"filter\":           r.get(\"filter\"),\n",
    "            \"user_metadata\":    r.get(\"user_metadata\"),\n",
    "            \"check\": {\n",
    "                \"function\":        fn,\n",
    "                \"for_each_column\": fec if fec else None,\n",
    "                \"arguments\":       args,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    status = DQEngine.validate_checks(raw_rules)\n",
    "    if getattr(status, \"has_errors\", False):\n",
    "        keep: List[dict] = []\n",
    "        for r in raw_rules:\n",
    "            st = DQEngine.validate_checks([r])\n",
    "            if not getattr(st, \"has_errors\", False):\n",
    "                keep.append(r)\n",
    "        return _group_by_table(keep), coerced, len(raw_rules) - len(keep)\n",
    "    else:\n",
    "        return _group_by_table(raw_rules), coerced, 0\n",
    "\n",
    "# --------------------------\n",
    "# Apply with isolation and diagnostics\n",
    "# --------------------------\n",
    "def _force_eval(df: DataFrame) -> None:\n",
    "    cols = []\n",
    "    if \"_errors\" in df.columns:\n",
    "        cols.append(F.size(F.coalesce(F.col(\"_errors\"), F.array())).alias(\"e\"))\n",
    "    if \"_warnings\" in df.columns:\n",
    "        cols.append(F.size(F.coalesce(F.col(\"_warnings\"), F.array())).alias(\"w\"))\n",
    "    if not cols:\n",
    "        cols = [F.lit(1).alias(\"one\")]\n",
    "    df.select(*cols).limit(1).collect()\n",
    "\n",
    "def _apply_rules_isolating_failures(dq: DQEngine,\n",
    "                                    src: DataFrame,\n",
    "                                    table_name: str,\n",
    "                                    tbl_rules: List[dict]) -> Tuple[Optional[DataFrame], List[Tuple[str, str]]]:\n",
    "    try:\n",
    "        df_all = dq.apply_checks_by_metadata(src, tbl_rules)\n",
    "        _force_eval(df_all)\n",
    "        return df_all, []\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    bad: List[Tuple[str, str]] = []\n",
    "    good: List[dict] = []\n",
    "    for r in tbl_rules:\n",
    "        try:\n",
    "            df_one = dq.apply_checks_by_metadata(src, [r])\n",
    "            _force_eval(df_one)\n",
    "            good.append(r)\n",
    "        except Exception as ex:\n",
    "            bad.append((r.get(\"name\") or \"<unnamed>\", str(ex)))\n",
    "            try:\n",
    "                print(f\"    offending rule JSON: {json.dumps(r, indent=2)}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if bad:\n",
    "        print(f\"[{table_name}] Skipping {len(bad)} bad rule(s).\")\n",
    "    if not good:\n",
    "        return None, bad\n",
    "\n",
    "    try:\n",
    "        df_good = dq.apply_checks_by_metadata(src, good)\n",
    "        _force_eval(df_good)\n",
    "        return df_good, bad\n",
    "    except Exception as ex2:\n",
    "        print(f\"[{table_name}] Still failing after pruning bad rules: {ex2}\")\n",
    "        return None, bad\n",
    "\n",
    "# --------------------------\n",
    "# Projection & enrichment\n",
    "# --------------------------\n",
    "def _empty_issues_array_struct() -> T.ArrayType:\n",
    "    return T.ArrayType(T.StructType([\n",
    "        T.StructField(\"name\",          T.StringType(), True),\n",
    "        T.StructField(\"message\",       T.StringType(), True),\n",
    "        T.StructField(\"columns\",       T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"filter\",        T.StringType(), True),\n",
    "        T.StructField(\"function\",      T.StringType(), True),\n",
    "        T.StructField(\"run_time\",      T.TimestampType(), True),\n",
    "        T.StructField(\"user_metadata\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]))\n",
    "\n",
    "def _project_row_hits(df_annot: DataFrame,\n",
    "                      table_name: str,\n",
    "                      run_config_name: str,\n",
    "                      created_by: str,\n",
    "                      exclude_cols: Optional[List[str]] = None) -> DataFrame:\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    e_name = _pick_col(df_annot, \"_errors\", \"_error\")\n",
    "    w_name = _pick_col(df_annot, \"_warnings\", \"_warning\")\n",
    "    errors_col   = F.col(e_name) if e_name else _empty_issues_array()\n",
    "    warnings_col = F.col(w_name) if w_name else _empty_issues_array()\n",
    "\n",
    "    df = (df_annot\n",
    "          .withColumn(\"_errs\", errors_col)\n",
    "          .withColumn(\"_warns\", warnings_col)\n",
    "          .where((F.size(\"_errs\") > 0) | (F.size(\"_warns\") > 0)))\n",
    "\n",
    "    reserved = {e_name, w_name, \"_errs\", \"_warns\"} - {None} | exclude_cols\n",
    "    cols = [c for c in df.columns if c not in reserved]\n",
    "    row_snapshot = F.array(*[F.struct(F.lit(c).alias(\"column\"), F.col(c).cast(\"string\").alias(\"value\")) for c in sorted(cols)])\n",
    "    row_snapshot_fp = F.sha2(F.to_json(row_snapshot), 256)\n",
    "\n",
    "    _errors_fp   = F.sha2(F.to_json(F.array_sort(_normalize_issues_for_fp(F.col(\"_errs\")))), 256)\n",
    "    _warnings_fp = F.sha2(F.to_json(F.array_sort(_normalize_issues_for_fp(F.col(\"_warns\")))), 256)\n",
    "\n",
    "    log_id = F.sha2(F.concat_ws(\"||\",\n",
    "                                F.lit(table_name),\n",
    "                                F.lit(run_config_name),\n",
    "                                row_snapshot_fp,\n",
    "                                _errors_fp,\n",
    "                                _warnings_fp), 256)\n",
    "\n",
    "    return df.select(\n",
    "        log_id.alias(\"log_id\"),\n",
    "        F.lit(None).cast(T.ArrayType(T.StringType())).alias(\"check_id\"),\n",
    "        F.lit(table_name).alias(\"table_name\"),\n",
    "        F.lit(run_config_name).alias(\"run_config_name\"),\n",
    "        F.col(\"_errs\").alias(\"_errors\"),\n",
    "        _errors_fp.alias(\"_errors_fingerprint\"),\n",
    "        F.col(\"_warns\").alias(\"_warnings\"),\n",
    "        _warnings_fp.alias(\"_warnings_fingerprint\"),\n",
    "        row_snapshot.alias(\"row_snapshot\"),\n",
    "        row_snapshot_fp.alias(\"row_snapshot_fingerprint\"),\n",
    "        F.lit(created_by).alias(\"created_by\"),\n",
    "        F.current_timestamp().alias(\"created_at\"),\n",
    "        F.lit(None).cast(T.StringType()).alias(\"updated_by\"),\n",
    "        F.lit(None).cast(T.TimestampType()).alias(\"updated_at\"),\n",
    "    )\n",
    "\n",
    "def _enrich_check_ids(row_log_df: DataFrame, checks_table: str) -> DataFrame:\n",
    "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "    cfg = (\n",
    "        spark.table(checks_table)\n",
    "        .select(\n",
    "            F.lower(F.col(\"table_name\")).alias(\"t_tbl_norm\"),\n",
    "            F.col(\"run_config_name\").alias(\"t_rc\"),\n",
    "            F.lower(F.col(\"name\")).alias(\"t_name_norm\"),\n",
    "            F.col(\"check_id\").alias(\"cfg_check_id\"),\n",
    "            F.col(\"active\").alias(\"t_active\")\n",
    "        )\n",
    "        .where(F.col(\"t_active\") == True)\n",
    "        .drop(\"t_active\")\n",
    "        .dropDuplicates([\"t_tbl_norm\", \"t_rc\", \"t_name_norm\"])\n",
    "    )\n",
    "    names = (\n",
    "        row_log_df\n",
    "        .select(\n",
    "            \"log_id\",\n",
    "            F.lower(F.col(\"table_name\")).alias(\"tbl_norm\"),\n",
    "            F.col(\"run_config_name\").alias(\"rc\"),\n",
    "            F.expr(\"transform(_errors, x -> x.name)\").alias(\"e_names\"),\n",
    "            F.expr(\"transform(_warnings, x -> x.name)\").alias(\"w_names\"),\n",
    "        )\n",
    "        .withColumn(\"all_names\", F.array_union(\"e_names\", \"w_names\"))\n",
    "        .withColumn(\"name\", F.explode_outer(\"all_names\"))\n",
    "        .where(F.col(\"name\").isNotNull())\n",
    "        .withColumn(\"name_norm\", F.lower(F.trim(F.col(\"name\"))))\n",
    "        .select(\"log_id\",\"tbl_norm\",\"rc\",\"name_norm\")\n",
    "    )\n",
    "    j = (\n",
    "        names.join(\n",
    "            cfg,\n",
    "            (names.tbl_norm == cfg.t_tbl_norm) &\n",
    "            (names.rc == cfg.t_rc) &\n",
    "            (names.name_norm == cfg.t_name_norm),\n",
    "            \"left\"\n",
    "        )\n",
    "        .groupBy(\"log_id\")\n",
    "        .agg(F.array_sort(F.array_distinct(F.collect_list(\"cfg_check_id\"))).alias(\"check_id\"))\n",
    "    )\n",
    "    out = (\n",
    "        row_log_df.drop(\"check_id\")\n",
    "        .join(j, \"log_id\", \"left\")\n",
    "        .withColumn(\"check_id\", F.coalesce(F.col(\"check_id\"), F.array().cast(T.ArrayType(T.StringType()))))\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# --------------------------\n",
    "# Summaries\n",
    "# --------------------------\n",
    "def _summarize_table(annot: DataFrame, table_name: str) -> Row:\n",
    "    err = \"_errors\" if \"_errors\" in annot.columns else \"_error\"\n",
    "    wrn = \"_warnings\" if \"_warnings\" in annot.columns else \"_warning\"\n",
    "    error_rows   = annot.where(F.size(F.col(err)) > 0).count()\n",
    "    warning_rows = annot.where(F.size(F.col(wrn)) > 0).count()\n",
    "    total_rows   = annot.count()\n",
    "    total_flagged_rows = annot.where((F.size(F.col(err)) > 0) | (F.size(F.col(wrn)) > 0)).count()\n",
    "    rules_fired = (\n",
    "        annot.select(\n",
    "            F.explode_outer(\n",
    "                F.array_union(\n",
    "                    F.expr(f\"transform({err}, x -> x.name)\"),\n",
    "                    F.expr(f\"transform({wrn}, x -> x.name)\")\n",
    "                )\n",
    "            ).alias(\"nm\")\n",
    "        )\n",
    "        .where(F.col(\"nm\").isNotNull())\n",
    "        .agg(F.countDistinct(\"nm\").alias(\"rules\"))\n",
    "        .collect()[0][\"rules\"]\n",
    "    )\n",
    "    return Row(table_name=table_name,\n",
    "               table_total_rows=int(total_rows),\n",
    "               table_total_error_rows=int(error_rows),\n",
    "               table_total_warning_rows=int(warning_rows),\n",
    "               total_flagged_rows=int(total_flagged_rows),\n",
    "               distinct_rules_fired=int(rules_fired))\n",
    "\n",
    "def _rules_hits_for_table(annot: DataFrame, table_name: str) -> DataFrame:\n",
    "    err = \"_errors\" if \"_errors\" in annot.columns else \"_error\"\n",
    "    wrn = \"_warnings\" if \"_warnings\" in annot.columns else \"_warning\"\n",
    "    errs = (\n",
    "        annot\n",
    "        .select(F.explode_outer(F.expr(f\"transform({err}, x -> x.name)\")).alias(\"name\"))\n",
    "        .where(F.col(\"name\").isNotNull())\n",
    "        .withColumn(\"severity\", F.lit(\"error\"))\n",
    "    )\n",
    "    warns = (\n",
    "        annot\n",
    "        .select(F.explode_outer(F.expr(f\"transform({wrn}, x -> x.name)\")).alias(\"name\"))\n",
    "        .where(F.col(\"name\").isNotNull())\n",
    "        .withColumn(\"severity\", F.lit(\"warning\"))\n",
    "    )\n",
    "    both = errs.unionByName(warns, allowMissingColumns=True)\n",
    "    return (\n",
    "        both.groupBy(\"name\", \"severity\")\n",
    "        .agg(F.count(F.lit(1)).alias(\"rows_flagged\"))\n",
    "        .withColumn(\"table_name\", F.lit(table_name))\n",
    "    )\n",
    "\n",
    "# ---------------\n",
    "# Main entry point\n",
    "# ---------------\n",
    "def run_checks(\n",
    "    generated_dqx_checks_config_table_name: str,\n",
    "    generated_dqx_checks_log_table_name: str,\n",
    "    *,\n",
    "    created_by: str = \"AdminUser\",\n",
    "    exclude_cols: Optional[List[str]] = None,\n",
    "    coercion_mode: str = \"permissive\",  # or \"strict\"\n",
    "    table_summary_output: Optional[str] = \"dq_dev.dqx.generated_checks_log_summary_by_table\",\n",
    "    row_summary_output: Optional[str]   = \"dq_dev.dqx.generated_checks_log_summary_by_rule\",\n",
    "    write_mode: str = \"overwrite\",               # <-- applies to ALL outputs\n",
    "    write_options: Optional[Dict[str, str]] = None,\n",
    "    run_configs: Optional[List[str]] = None,     # if None, auto-detect from checks table\n",
    "    include_tables_glob: Optional[List[str]] = None,  # only process matching tables\n",
    "):\n",
    "    \"\"\"\n",
    "    Process only tables whose fully-qualified names match any glob in `include_tables_glob`.\n",
    "      - Examples: [\"de_prd.gold.crm_*\"], [\"dq_prd.monitoring.*\"], [\"crm_*\"]  (# => *.*.crm_*)\n",
    "    The `write_mode` applies to:\n",
    "      - detailed row log (generated_checks_log),\n",
    "      - per-rule summary (generated_checks_log_summary_by_rule),\n",
    "      - per-table summary (generated_checks_log_summary_by_table).\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    dq = DQEngine(WorkspaceClient())\n",
    "\n",
    "    checks_table  = generated_dqx_checks_config_table_name\n",
    "    results_table = generated_dqx_checks_log_table_name\n",
    "    write_opts = write_options or {}\n",
    "\n",
    "    # Ensure results table exists (row-log)\n",
    "    ensure_table(results_table, {**DQX_CHECKS_LOG_METADATA, \"table\": results_table})\n",
    "\n",
    "    # Discover run_configs if not provided\n",
    "    if run_configs is None:\n",
    "        run_configs = [r[0] for r in\n",
    "                       spark.table(checks_table)\n",
    "                            .where(F.col(\"active\") == True)\n",
    "                            .select(\"run_config_name\").distinct()\n",
    "                            .collect()]\n",
    "        run_configs = sorted(str(rc) for rc in run_configs if rc is not None)\n",
    "\n",
    "    # Count total active checks once (for logging only)\n",
    "    try:\n",
    "        checks_table_total = spark.table(checks_table).where(F.col(\"active\") == True).count()\n",
    "    except Exception:\n",
    "        checks_table_total = -1\n",
    "\n",
    "    grand_total = 0\n",
    "    all_tbl_summaries: List[Row] = []\n",
    "    printed_grand_once = False\n",
    "\n",
    "    # normalize patterns once\n",
    "    globs_norm = [ _normalize_glob(g) for g in (include_tables_glob or []) ]\n",
    "\n",
    "    for rc_name in run_configs:\n",
    "        if rc_name is None or str(rc_name).lower() == \"none\":\n",
    "            continue\n",
    "\n",
    "        display_section(f\"Run config: {rc_name}\")\n",
    "        by_tbl, coerced, skipped = _load_checks_from_table_as_dicts(\n",
    "            spark,\n",
    "            checks_table,\n",
    "            rc_name,\n",
    "            coercion_mode=coercion_mode,\n",
    "            include_tables_glob=globs_norm,     # SQL pushdown\n",
    "        )\n",
    "\n",
    "        # Python-side filter (belt & suspenders)\n",
    "        if globs_norm:\n",
    "            by_tbl = {tbl: rules for tbl, rules in by_tbl.items() if _matches_any_glob(tbl, globs_norm)}\n",
    "\n",
    "        checks_loaded = sum(len(v) for v in by_tbl.values())\n",
    "        print(f\"[{rc_name}] checks_in_table_total={checks_table_total}, loaded={checks_loaded}, coerced={coerced}, skipped_invalid={skipped}\")\n",
    "        if globs_norm:\n",
    "            print(f\"[{rc_name}] include_tables_glob={globs_norm}\")\n",
    "            if not by_tbl:\n",
    "                print(f\"[{rc_name}] No tables matched globs; skipping.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"[{rc_name}] Tables to process ({len(by_tbl)}): {', '.join(sorted(by_tbl.keys()))}\")\n",
    "\n",
    "        if not checks_loaded:\n",
    "            print(f\"[{rc_name}] no checks loaded (active=TRUE & run_config_name='{rc_name}').\")\n",
    "            continue\n",
    "\n",
    "        out_batches: List[DataFrame] = []\n",
    "        rc_tbl_summaries: List[Row] = []\n",
    "        rc_rule_hit_parts: List[DataFrame] = []\n",
    "        table_row_counts: Dict[str, int] = {}\n",
    "        processed_tables: List[str] = []\n",
    "\n",
    "        for tbl, tbl_rules in by_tbl.items():\n",
    "            try:\n",
    "                src = spark.read.table(tbl)\n",
    "                annot, bad = _apply_rules_isolating_failures(dq, src, tbl, tbl_rules)\n",
    "                if annot is None:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"[{rc_name}] {tbl} failed: {e}\")\n",
    "                continue\n",
    "\n",
    "            processed_tables.append(tbl)\n",
    "\n",
    "            total_rows = annot.count()\n",
    "            table_row_counts[tbl] = total_rows\n",
    "\n",
    "            summary_row = _summarize_table(annot, tbl)\n",
    "            rc_tbl_summaries.append(summary_row)\n",
    "            all_tbl_summaries.append(Row(run_config_name=rc_name, **summary_row.asDict()))\n",
    "\n",
    "            rc_rule_hit_parts.append(_rules_hits_for_table(annot, tbl))\n",
    "\n",
    "            row_hits = _project_row_hits(annot, tbl, rc_name, created_by, exclude_cols=exclude_cols)\n",
    "            if row_hits.limit(1).count() > 0:\n",
    "                out_batches.append(row_hits)\n",
    "\n",
    "        if rc_tbl_summaries:\n",
    "            summary_df = spark.createDataFrame(rc_tbl_summaries).orderBy(\"table_name\")\n",
    "            display_section(f\"Row-hit summary by table (run_config={rc_name})\")\n",
    "            show_df(summary_df, n=200, truncate=False)\n",
    "\n",
    "        if rc_rule_hit_parts:\n",
    "            rules_all = rc_rule_hit_parts[0]\n",
    "            for part in rc_rule_hit_parts[1:]:\n",
    "                rules_all = rules_all.unionByName(part, allowMissingColumns=True)\n",
    "\n",
    "            cfg_rules = (\n",
    "                spark.table(checks_table)\n",
    "                .where((F.col(\"run_config_name\") == rc_name) & (F.col(\"active\") == True))\n",
    "                .where(F.col(\"table_name\").isin(processed_tables))\n",
    "                .select(\n",
    "                    F.col(\"table_name\"),\n",
    "                    F.col(\"name\").alias(\"rule_name\"),\n",
    "                    F.when(F.lower(\"criticality\").isin(\"warn\", \"warning\"), F.lit(\"warning\"))\n",
    "                     .otherwise(F.lit(\"error\")).alias(\"severity\")\n",
    "                )\n",
    "                .dropDuplicates([\"table_name\",\"rule_name\",\"severity\"])\n",
    "            )\n",
    "\n",
    "            counts = (\n",
    "                rules_all\n",
    "                .groupBy(\"table_name\", \"name\", \"severity\")\n",
    "                .agg(F.sum(\"rows_flagged\").alias(\"rows_flagged\"))\n",
    "                .withColumnRenamed(\"name\", \"rule_name\")\n",
    "            )\n",
    "\n",
    "            full_rules = (\n",
    "                cfg_rules.join(counts, on=[\"table_name\",\"rule_name\",\"severity\"], how=\"left\")\n",
    "                .withColumn(\"rows_flagged\", F.coalesce(F.col(\"rows_flagged\"), F.lit(0)))\n",
    "            )\n",
    "\n",
    "            totals_df = spark.createDataFrame(\n",
    "                [Row(table_name=k, table_total_rows=v) for k, v in table_row_counts.items()]\n",
    "            )\n",
    "            full_rules = (\n",
    "                full_rules.join(totals_df, \"table_name\", \"left\")\n",
    "                .withColumn(\n",
    "                    \"pct_of_table_rows\",\n",
    "                    F.when(F.col(\"table_total_rows\") > 0,\n",
    "                           F.col(\"rows_flagged\") / F.col(\"table_total_rows\"))\n",
    "                     .otherwise(F.lit(0.0))\n",
    "                )\n",
    "                .select(\"table_name\", \"rule_name\", \"severity\", \"rows_flagged\",\n",
    "                        \"table_total_rows\", \"pct_of_table_rows\")\n",
    "                .orderBy(F.desc(\"rows_flagged\"), F.asc(\"table_name\"), F.asc(\"rule_name\"))\n",
    "            )\n",
    "\n",
    "            display_section(f\"Row-hit summary by rule (run_config={rc_name})\")\n",
    "            show_df(full_rules, n=2000, truncate=False)\n",
    "\n",
    "            if row_summary_output:\n",
    "                _ensure_schema_exists_for_fqn(row_summary_output)\n",
    "                (full_rules\n",
    "                 .withColumn(\"run_config_name\", F.lit(rc_name))\n",
    "                 .select(\"run_config_name\",\"table_name\",\"rule_name\",\"severity\",\n",
    "                         \"rows_flagged\",\"table_total_rows\",\"pct_of_table_rows\")\n",
    "                 .write.format(\"delta\")\n",
    "                 .mode(write_mode)                  # <--- unified write mode\n",
    "                 .options(**write_opts)\n",
    "                 .saveAsTable(row_summary_output))\n",
    "                print(f\"[{rc_name}] per-rule summary ({write_mode}) → {row_summary_output}\")\n",
    "\n",
    "        if not out_batches:\n",
    "            print(f\"[{rc_name}] no row-level hits.\")\n",
    "            continue\n",
    "\n",
    "        out = out_batches[0]\n",
    "        for b in out_batches[1:]:\n",
    "            out = out.unionByName(b, allowMissingColumns=True)\n",
    "\n",
    "        out = _enrich_check_ids(out, checks_table)\n",
    "\n",
    "        out = out.select([f.name for f in ROW_LOG_SCHEMA.fields])\n",
    "        rows = out.count()\n",
    "        out.write.format(\"delta\").mode(write_mode).options(**write_opts).saveAsTable(results_table)\n",
    "        grand_total += rows\n",
    "        print(f\"[{rc_name}] wrote {rows} rows ({write_mode}) → {results_table}\")\n",
    "\n",
    "    if all_tbl_summaries and not printed_grand_once:\n",
    "        grand_df = (\n",
    "            SparkSession.getActiveSession().createDataFrame(all_tbl_summaries)\n",
    "            .select(\n",
    "                F.col(\"run_config_name\"),\n",
    "                F.col(\"table_name\"),\n",
    "                F.col(\"table_total_rows\"),\n",
    "                F.col(\"table_total_error_rows\"),\n",
    "                F.col(\"table_total_warning_rows\"),\n",
    "                F.col(\"total_flagged_rows\"),\n",
    "                F.col(\"distinct_rules_fired\"),\n",
    "            )\n",
    "            .orderBy(\"run_config_name\", \"table_name\")\n",
    "        )\n",
    "        display_section(\"Row-hit summary by table (ALL run_configs)\")\n",
    "        show_df(grand_df, n=500, truncate=False)\n",
    "\n",
    "        if table_summary_output:\n",
    "            _ensure_schema_exists_for_fqn(table_summary_output)\n",
    "            grand_df.write.format(\"delta\").mode(write_mode).options(**write_opts).saveAsTable(table_summary_output)\n",
    "            print(f\"[ALL RCs] table summary ({write_mode}) → {table_summary_output}\")\n",
    "\n",
    "    display_section(\"Grand total\")\n",
    "    print(f\"TOTAL rows written: {grand_total}\")\n",
    "\n",
    "# ---- example run: ONLY outlaw_* tables under de_prd.gold, APPEND everywhere ----\n",
    "run_checks(\n",
    "    generated_dqx_checks_config_table_name=\"dq_dev.dqx.generated_checks_config\",\n",
    "    generated_dqx_checks_log_table_name=\"dq_dev.dqx.generated_checks_log\",\n",
    "    created_by=\"AdminUser\",\n",
    "    coercion_mode=\"strict\",\n",
    "    table_summary_output=\"dq_dev.dqx.generated_checks_log_summary_by_table\",\n",
    "    row_summary_output=\"dq_dev.dqx.generated_checks_log_summary_by_rule\",\n",
    "    write_mode=\"append\",                           # <-- append across ALL outputs\n",
    "    write_options={\"mergeSchema\": \"true\"},\n",
    "    # run_configs=[\"default\"],  # optional\n",
    "    include_tables_glob=[\"de_prd.gold.audit_*\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_generate_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
