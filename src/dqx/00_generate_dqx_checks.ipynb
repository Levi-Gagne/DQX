{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4fe261-69b0-4d1a-a5a0-cb4d445b418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8383f8f1-9db5-46eb-abc3-d472fbc64507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- DQX profiling options: all known keys --\n",
    "profile_options = {\n",
    "    # \"round\": True,                 # (Removed - not valid for this profiler version)\n",
    "    \"max_in_count\": 10,            # Max distinct values for is_in rule\n",
    "    \"distinct_ratio\": 0.05,        # Max unique/total ratio for is_in rule\n",
    "    \"max_null_ratio\": 0.01,        # Max null fraction to allow is_not_null rule\n",
    "    \"remove_outliers\": True,       # Remove outliers for min/max\n",
    "    \"outlier_columns\": [],         # Only these columns get outlier removal (empty=all numerics)\n",
    "    \"num_sigmas\": 3,               # Stddev for outlier removal (z-score cutoff)\n",
    "    \"trim_strings\": True,          # Strip whitespace before profiling strings\n",
    "    \"max_empty_ratio\": 0.01,       # Max empty string ratio for is_not_null_or_empty\n",
    "    \"sample_fraction\": 0.3,        # Row fraction to sample\n",
    "    \"sample_seed\": None,           # Seed for reproducibility (set int for deterministic)\n",
    "    \"limit\": 1000,                 # Max number of rows to profile\n",
    "    \"profile_types\": None,         # List of rule types (e.g. [\"is_in\", \"is_not_null\"]); None=default\n",
    "    \"min_length\": None,            # Min string length to consider (None disables)\n",
    "    \"max_length\": None,            # Max string length to consider (None disables)\n",
    "    \"include_histograms\": False,   # Compute histograms as part of profiling\n",
    "    \"min_value\": None,             # Numeric min override (None disables)\n",
    "    \"max_value\": None,             # Numeric max override (None disables)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2168addb-58e3-4aae-853c-7e2f844cda7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8baa8a-c510-47e7-b107-3204ff7b2c9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29833b1c-0236-46f9-a6f9-adb15dd55ba0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main"
    }
   },
   "outputs": [],
   "source": [
    "# dqx_rule_generator.py\n",
    "# Full file — adds YAML list support via header-comment parsing + display() preview.\n",
    "# NOTHING REMOVED: all params/options stay. Renamed yaml_key_order -> key_order.\n",
    "# Single-table & CSV FQNs still work. One YAML file per table is written in output_location.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from typing import List, Optional, Dict, Any, Literal, Tuple\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "\n",
    "# Keys currently shown in DQX docs for profiler \"options\" (kept permissive; we only warn on extras)\n",
    "DOC_SUPPORTED_KEYS = {\n",
    "    \"sample_fraction\",\n",
    "    \"sample_seed\",\n",
    "    \"limit\",\n",
    "    \"remove_outliers\",\n",
    "    \"outlier_columns\",\n",
    "    \"num_sigmas\",\n",
    "    \"max_null_ratio\",\n",
    "    \"trim_strings\",\n",
    "    \"max_empty_ratio\",\n",
    "    \"distinct_ratio\",\n",
    "    \"max_in_count\",\n",
    "    \"round\",\n",
    "}\n",
    "\n",
    "# ---------- YAML file support (header-comment parsing for defaults) ----------\n",
    "_YAML_PATH_RE = re.compile(r\"\\.(ya?ml)$\", re.IGNORECASE)\n",
    "_FROM_INFOSCHEMA = re.compile(r\"FROM\\s+([A-Za-z0-9_]+)\\.information_schema\\.tables\", re.IGNORECASE)\n",
    "_TABLE_SCHEMA_EQ = re.compile(r\"table_schema\\s*=\\s*'([^']+)'\", re.IGNORECASE)\n",
    "\n",
    "def _is_yaml_path(path: str) -> bool:\n",
    "    return bool(_YAML_PATH_RE.search(path))\n",
    "\n",
    "def _resolve_local_like_path(path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try hard to resolve a repo-relative file path from a notebook:\n",
    "    - as-given\n",
    "    - join with cwd\n",
    "    - walk up 6 parents and join\n",
    "    Returns a filesystem path if found, else None.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    cwd = os.getcwd()\n",
    "    base = cwd\n",
    "    for _ in range(6):\n",
    "        cand = os.path.abspath(os.path.join(base, path))\n",
    "        candidates.append(cand)\n",
    "        if os.path.exists(cand):\n",
    "            return cand\n",
    "        parent = os.path.dirname(base)\n",
    "        if parent == base:\n",
    "            break\n",
    "        base = parent\n",
    "    return None\n",
    "\n",
    "def _read_text_any(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read small text files from:\n",
    "      - repo/local filesystem (relative or absolute)\n",
    "      - dbfs:/, /dbfs/, /Volumes/\n",
    "      - workspace files path (starts with '/'), via Files API\n",
    "    \"\"\"\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"dbutils is required to read from DBFS/Volumes\") from e\n",
    "        target = path if path.startswith(\"dbfs:\") else (f\"dbfs:{path}\" if path.startswith(\"/\") else f\"dbfs:/{path}\")\n",
    "        return dbutils.fs.head(target, 10 * 1024 * 1024)\n",
    "\n",
    "    # Absolute workspace file path (e.g. /Workspace/Repos/.../file.yaml)\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            # newer SDK signature\n",
    "            data = wc.files.download(file_path=path).read()\n",
    "        except TypeError:\n",
    "            # older SDK signature\n",
    "            data = wc.files.download(path=path).read()\n",
    "        return data.decode(\"utf-8\")\n",
    "\n",
    "    # Relative/local resolution (repo-friendly)\n",
    "    resolved = _resolve_local_like_path(path)\n",
    "    if resolved and os.path.isfile(resolved):\n",
    "        with open(resolved, \"r\", encoding=\"utf-8\") as fh:\n",
    "            return fh.read()\n",
    "\n",
    "    # Nothing worked → helpful error\n",
    "    msg = [\n",
    "        f\"Could not find YAML at '{path}'.\",\n",
    "        f\"cwd={os.getcwd()}\",\n",
    "        \"Hints:\",\n",
    "        \"  - If this file is in your repo, pass a path relative to the notebook or the repo root.\",\n",
    "        \"  - Or pass an absolute workspace path like '/Workspace/Repos/.../file.yaml'.\",\n",
    "        \"  - Or use a DBFS/Volumes path like 'dbfs:/...' or '/Volumes/...'.\",\n",
    "    ]\n",
    "    raise FileNotFoundError(\"\\n\".join(msg))\n",
    "\n",
    "def _parse_global_hints_from_comments(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Pull defaults from header comments only (no YAML keys):\n",
    "      - catalog from: 'FROM <catalog>.information_schema.tables'\n",
    "      - schema  from: \"table_schema = '<schema>'\"\n",
    "    \"\"\"\n",
    "    m_cat = _FROM_INFOSCHEMA.search(text)\n",
    "    m_sch = _TABLE_SCHEMA_EQ.search(text)\n",
    "    cat = m_cat.group(1) if m_cat else None\n",
    "    sch = m_sch.group(1) if m_sch else None\n",
    "    return cat, sch\n",
    "\n",
    "def _ensure_fqns(names: List[str], hint_catalog: Optional[str], hint_schema: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Accept: catalog.schema.table | schema.table | table\n",
    "    Use comment-derived defaults to fill missing parts. Dotted entries override per item.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    for n in names:\n",
    "        n = n.strip()\n",
    "        if not n:\n",
    "            continue\n",
    "        parts = n.split(\".\")\n",
    "        if len(parts) == 3:\n",
    "            out.append(n)\n",
    "        elif len(parts) == 2:\n",
    "            if not hint_catalog:\n",
    "                raise ValueError(f\"'{n}' lacks catalog; add it to the item or provide a comment default.\")\n",
    "            out.append(f\"{hint_catalog}.{n}\")\n",
    "        elif len(parts) == 1:\n",
    "            if not (hint_catalog and hint_schema):\n",
    "                raise ValueError(f\"'{n}' needs catalog & schema; set via comments or use dotted forms.\")\n",
    "            out.append(f\"{hint_catalog}.{hint_schema}.{n}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized table format: {n}\")\n",
    "    bad = [t for t in out if t.count(\".\") != 2]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Invalid FQN(s) after resolution: {bad}\")\n",
    "    return sorted(set(out))\n",
    "\n",
    "def _discover_tables_from_yaml_by_comments(yaml_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load the YAML file (with e.g. 'table_name:' list) and use ONLY header comments\n",
    "    to infer catalog/schema defaults. Dotted items override per entry.\n",
    "    \"\"\"\n",
    "    text = _read_text_any(yaml_path)\n",
    "    cat_hint, sch_hint = _parse_global_hints_from_comments(text)\n",
    "\n",
    "    obj = yaml.safe_load(io.StringIO(text))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(f\"YAML must contain a mapping with a list; got: {type(obj).__name__}\")\n",
    "\n",
    "    names = None\n",
    "    for key in (\"table_name\", \"tables\", \"table_names\", \"list\"):\n",
    "        if isinstance(obj.get(key), list):\n",
    "            names = [str(x).strip() for x in obj[key] if x]\n",
    "            break\n",
    "    if not names:\n",
    "        raise ValueError(f\"No table list found in YAML: {yaml_path}\")\n",
    "\n",
    "    fqns = _ensure_fqns(names, cat_hint, sch_hint)\n",
    "    print(f\"[INFO] Parsed defaults from comments: catalog={cat_hint} schema={sch_hint}\")\n",
    "    return fqns\n",
    "\n",
    "def _display_table_preview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    \"\"\"\n",
    "    Use display() (Databricks) for a clean, interactive preview instead of printing.\n",
    "    \"\"\"\n",
    "    rows = [(f, *f.split(\".\")) for f in fqns]\n",
    "    df = spark.createDataFrame(rows, \"fqn string, catalog string, schema string, table string\")\n",
    "    print(f\"\\n=== {title} ({len(fqns)}) ===\")\n",
    "    try:\n",
    "        display(df)\n",
    "    except NameError:\n",
    "        df.show(len(fqns), truncate=False)\n",
    "\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,                       # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param: str,                 # pipeline CSV | catalog | catalog.schema | table FQN CSV | YAML path\n",
    "        output_format: str,              # \"yaml\" | \"table\"\n",
    "        output_location: str,            # yaml: folder or file; table: catalog.schema.table\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,     # e.g. \".tmp_*\"\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,       # None => whole table (only if mode==\"table\")\n",
    "        run_config_name: str = \"default\",          # DQX run group tag\n",
    "        criticality: str = \"warn\",                 # \"warn\" | \"error\"\n",
    "        key_order: Literal[\"engine\", \"custom\"] = \"custom\",  # \"engine\" uses DQX save; \"custom\" enforces key order\n",
    "        include_table_name: bool = True,           # include table_name in each rule dict\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_location = output_location\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.key_order = key_order\n",
    "        self.include_table_name = include_table_name\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "        if self.output_format not in {\"yaml\", \"table\"}:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table'.\")\n",
    "        if self.output_format == \"yaml\" and not self.output_location:\n",
    "            raise ValueError(\"When output_format='yaml', provide output_location (folder or file).\")\n",
    "\n",
    "    # ---------- profile options: pass via 'options' kwarg, warn on unknowns ----------\n",
    "    def _profile_call_kwargs(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Build kwargs for DQProfiler.profile / profile_table.\n",
    "        We pass:\n",
    "          - cols=self.columns (when provided)\n",
    "          - options=self.profile_options (dict; profiler reads keys internally)\n",
    "        We only WARN on keys not in the current documented set; we do not drop them.\n",
    "        \"\"\"\n",
    "        kwargs: Dict[str, Any] = {}\n",
    "        if self.columns is not None:\n",
    "            kwargs[\"cols\"] = self.columns\n",
    "        if self.profile_options:\n",
    "            unknown = sorted(set(self.profile_options) - DOC_SUPPORTED_KEYS)\n",
    "            if unknown:\n",
    "                print(f\"[INFO] Profiling options not in current docs (passing through anyway): {unknown}\")\n",
    "            kwargs[\"options\"] = self.profile_options\n",
    "        return kwargs\n",
    "\n",
    "    # ---------- discovery ----------\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:             {self.mode}\")\n",
    "        print(f\"name_param:       {self.name_param}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_location:  {self.output_location}\")\n",
    "        print(f\"exclude_pattern:  {self.exclude_pattern}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"key_order:        {self.key_order}\")\n",
    "        print(f\"include_table_name: {self.include_table_name}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if self.mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{self.mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "        if self.columns is not None and self.mode != \"table\":\n",
    "            raise ValueError(\"The 'columns' parameter can only be used in mode='table'.\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "\n",
    "        if self.mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in self.name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                discovered += [x for x in pipeline_tables if x]\n",
    "\n",
    "        elif self.mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = self.name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                discovered += [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if self.name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = self.name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            discovered = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        else:  # table\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            if _is_yaml_path(self.name_param):\n",
    "                print(f\"[INFO] name_param is a YAML list → {self.name_param}\")\n",
    "                discovered = _discover_tables_from_yaml_by_comments(self.name_param)\n",
    "            else:\n",
    "                tables = [t.strip() for t in self.name_param.split(\",\") if t.strip()]\n",
    "                for t in tables:\n",
    "                    if t.count(\".\") != 2:\n",
    "                        raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "                discovered = tables\n",
    "\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "\n",
    "        # Interactive preview instead of plain prints\n",
    "        _display_table_preview(self.spark, discovered, title=\"Final table list to generate DQX rules for\")\n",
    "\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    # ---------- storage config helpers ----------\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def _workspace_files_upload(path: str, payload: bytes) -> None:\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            wc.files.upload(file_path=path, contents=payload, overwrite=True)  # newer SDK\n",
    "        except TypeError:\n",
    "            wc.files.upload(path=path, contents=payload, overwrite=True)       # older SDK\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_parent(path: str) -> None:\n",
    "        \"\"\"Create parent dir for local paths.\"\"\"\n",
    "        parent = os.path.dirname(path)\n",
    "        if parent and not os.path.exists(parent):\n",
    "            os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dbfs_parent(dbutils, path: str) -> None:\n",
    "        parent = path.rsplit(\"/\", 1)[0] if \"/\" in path else path\n",
    "        if parent:\n",
    "            dbutils.fs.mkdirs(parent)\n",
    "\n",
    "    # ---------- DQX check shaping ----------\n",
    "    def _dq_constraint_to_check(\n",
    "        self,\n",
    "        rule_name: str,\n",
    "        constraint_sql: str,\n",
    "        table_name: str,\n",
    "        criticality: str,\n",
    "        run_config_name: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert a profiler constraint (SQL) into a DQX check dict.\n",
    "        Key order: table_name, name, criticality, run_config_name, check (insertion order).\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": criticality,\n",
    "            \"run_config_name\": run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    \"expression\": constraint_sql,\n",
    "                    \"name\": rule_name,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if self.include_table_name:\n",
    "            d = {\"table_name\": table_name, **d}\n",
    "        return d\n",
    "\n",
    "    # ---------- YAML writers ----------\n",
    "    def _write_yaml_ordered(self, checks: List[Dict[str, Any]], path: str) -> None:\n",
    "        \"\"\"\n",
    "        Dump YAML preserving key order and upload:\n",
    "          - /Volumes/... | dbfs:/... | /dbfs/... -> dbutils.fs.put (mkdirs parent)\n",
    "          - /Shared/... (workspace files) -> Files API\n",
    "          - relative/local path -> os.makedirs + open(...)\n",
    "        \"\"\"\n",
    "        yaml_str = yaml.safe_dump(checks, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "        # DBFS / Volumes\n",
    "        if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "            try:\n",
    "                from databricks.sdk.runtime import dbutils\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"dbutils is required to write to DBFS/Volumes.\")\n",
    "            target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "            self._ensure_dbfs_parent(dbutils, target.rsplit(\"/\", 1)[0])\n",
    "            dbutils.fs.put(target, yaml_str, True)\n",
    "            print(f\"[RUN] Wrote ordered YAML to {path}\")\n",
    "            return\n",
    "\n",
    "        # Workspace files\n",
    "        if path.startswith(\"/\"):\n",
    "            self._workspace_files_upload(path, yaml_str.encode(\"utf-8\"))\n",
    "            print(f\"[RUN] Wrote ordered YAML to workspace file: {path}\")\n",
    "            return\n",
    "\n",
    "        # Local (driver) relative/absolute filesystem (Repos-friendly)\n",
    "        full_path = os.path.abspath(path)\n",
    "        self._ensure_parent(full_path)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(yaml_str)\n",
    "        print(f\"[RUN] Wrote ordered YAML to local path: {full_path}\")\n",
    "\n",
    "    # ---------- main ----------\n",
    "    def run(self):\n",
    "        try:\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "            call_kwargs = self._profile_call_kwargs()\n",
    "            print(\"[RUN] Profiler call kwargs:\")\n",
    "            print(f\"  cols:    {call_kwargs.get('cols')}\")\n",
    "            print(f\"  options: {json.dumps(call_kwargs.get('options', {}), indent=2)}\")\n",
    "\n",
    "            dq_engine = DQEngine(WorkspaceClient())\n",
    "            total_checks = 0\n",
    "\n",
    "            for fq_table in tables:\n",
    "                if fq_table.count(\".\") != 2:\n",
    "                    print(f\"[WARN] Skipping invalid table name: {fq_table}\")\n",
    "                    continue\n",
    "                cat, sch, tab = fq_table.split(\".\")\n",
    "\n",
    "                # Verify readability\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table readability: {fq_table}\")\n",
    "                    self.spark.table(fq_table).limit(1).collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    # DataFrame profiling with options and optional cols\n",
    "                    summary_stats, profiles = profiler.profile(df, **call_kwargs)\n",
    "                    # If you prefer table-based API:\n",
    "                    # summary_stats, profiles = profiler.profile_table(table=fq_table, **call_kwargs)\n",
    "\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                checks: List[Dict[str, Any]] = []\n",
    "                for rule_name, constraint in (rules_dict or {}).items():\n",
    "                    checks.append(\n",
    "                        self._dq_constraint_to_check(\n",
    "                            rule_name=rule_name,\n",
    "                            constraint_sql=constraint,\n",
    "                            table_name=fq_table,\n",
    "                            criticality=self.criticality,\n",
    "                            run_config_name=self.run_config_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if not checks:\n",
    "                    print(f\"[INFO] No checks generated for {fq_table}.\")\n",
    "                    continue\n",
    "\n",
    "                # Destination selection\n",
    "                if self.output_format == \"yaml\":            # \"yaml\" | \"table\"\n",
    "                    # Directory -> {table}.yaml ; or exact file path\n",
    "                    if self.output_location.endswith((\".yaml\", \".yml\")):\n",
    "                        path = self.output_location\n",
    "                    else:\n",
    "                        path = f\"{self.output_location.rstrip('/')}/{tab}.yaml\"\n",
    "\n",
    "                    if self.key_order == \"engine\":\n",
    "                        cfg = self._infer_file_storage_config(path)\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks via DQX to: {path}\")\n",
    "                        dq_engine.save_checks(checks, config=cfg)\n",
    "                    else:  # \"custom\"\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks with strict key order to: {path}\")\n",
    "                        self._write_yaml_ordered(checks, path)\n",
    "\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "                else:  # table sink\n",
    "                    cfg = self._table_storage_config(\n",
    "                        table_fqn=self.output_location,\n",
    "                        run_config_name=self.run_config_name,\n",
    "                        mode=\"append\"\n",
    "                    )\n",
    "                    print(f\"[RUN] Appending {len(checks)} checks to table: {self.output_location} (run_config_name={self.run_config_name})\")\n",
    "                    dq_engine.save_checks(checks, config=cfg)\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "            print(f\"[RUN] {'Successfully saved' if total_checks else 'No'} checks. Count: {total_checks}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "\n",
    "# -------------------- Usage examples --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        # Sampling\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": 42,\n",
    "        \"limit\": 1000,\n",
    "        # Outliers\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        # Nulls / empties\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        # Distincts → is_in\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_in_count\": 10,\n",
    "        # Rounding\n",
    "        \"round\": True,\n",
    "        # (Keys not in current docs will still pass through; you’ll see a one-time INFO warning)\n",
    "        # \"include_histograms\": False,\n",
    "        # \"min_length\": None,\n",
    "        # \"max_length\": None,\n",
    "        # \"min_value\": None,\n",
    "        # \"max_value\": None,\n",
    "        # \"profile_types\": None,\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Example A: Single table (unchanged)\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"dq_prd.monitoring.job_run_audit\",   # depends on mode\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"dqx_checks\",                   # yaml dir (local) OR \"/Shared/...\" OR \"dbfs:/...\" OR \"/Volumes/...\"\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",                             # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()\n",
    "    \"\"\"\n",
    "    # Example B: YAML list of tables + header comments (wkdy_* example)\n",
    "    #   Notebook path: src/dqx/00_generate_dqx_checks\n",
    "    #   YAML file path (relative to repo): info/wkdy_table/wkdy_table_info/wkdy_table_list-gold.yaml\n",
    "    #   Output folder:                      info/wkdy_table/wkdy_table_info/wkdy_generated_dqx_checks\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"resources/dqx_checks_generated/audit/audit_table_list-gold.yaml\",\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"resources/dqx_checks_generated/audit/audit_generated_dqx_checks\",  # one YAML per table\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",                             # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "782e0fee-9430-456b-8771-e3d0ec1d9527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30347346-cf42-4025-a9f4-bb1b364b246f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate_checks.py\n",
    "# Generate DQX checks from tables or YAML table-lists and write to YAML, table, or both.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import hashlib\n",
    "import datetime\n",
    "from typing import List, Optional, Dict, Any, Literal, Tuple\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql import DataFrame  # for type hints in show_df()\n",
    "\n",
    "# Notebook env helper (prints banner in the notebook and returns a dict we can reuse)\n",
    "from utils.print import print_notebook_env, get_notebook_path as _nb_path\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Documentation dictionary for the generated checks table (apply on first create)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "DQX_GENERATED_CHECKS_CONFIG_METADATA: Dict[str, Any] = {\n",
    "    \"table\": \"dq_dev.dqx.checks_generated_config\",  # will be overridden by the actual FQN you pass\n",
    "    \"table_comment\": (\n",
    "        \"# DQX *Generated* Checks Configuration\\n\"\n",
    "        \"- Stores flattened rules generated by the profiler.\\n\"\n",
    "        \"- Each row is a rule; `check_id` is a stable hash of the canonical payload.\\n\"\n",
    "        \"- `generator_meta` captures the profiler options and generator settings used to create these rows.\\n\"\n",
    "    ),\n",
    "    \"columns\": {\n",
    "        \"check_id\": \"SHA-256 **hash** of canonical payload (stable rule identity).\",\n",
    "        \"check_id_payload\": \"Canonical **JSON** used to compute `check_id`.\",\n",
    "        \"table_name\": \"Fully qualified **target table** (`catalog.schema.table`).\",\n",
    "\n",
    "        \"name\": \"Human-readable **rule name**.\",\n",
    "        \"criticality\": \"Rule severity: `warn|warning|error`.\",\n",
    "        \"check\": \"Structured **check** object: `{function, for_each_column, arguments}`.\",\n",
    "        \"filter\": \"Optional row-level **filter** expression.\",\n",
    "        \"run_config_name\": \"**Execution group/tag** for this rule.\",\n",
    "        \"user_metadata\": \"User-provided **metadata** `map<string,string>`.\",\n",
    "\n",
    "        \"yaml_path\": \"YAML **file path** that held this rule (or `<generated://...>` if ephemeral).\",\n",
    "        \"active\": \"If **false**, the rule is ignored by runners.\",\n",
    "\n",
    "        # NEW (placed after active, before audit)\n",
    "        \"generator_meta\": (\n",
    "            \"Array of two items: \"\n",
    "            \"`[{section:'profile_options', payload:map}, {section:'generator_settings', payload:map}]`.\"\n",
    "        ),\n",
    "\n",
    "        \"created_by\": \"Audit: **creator** of the row.\",\n",
    "        \"created_at\": \"Audit: **creation timestamp** (UTC ISO).\",\n",
    "        \"updated_by\": \"Audit: **last updater**.\",\n",
    "        \"updated_at\": \"Audit: **last update timestamp**.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "DQXGeneratedChecksConfig = DQX_GENERATED_CHECKS_CONFIG_METADATA\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Schema (unified) for the generated checks table (adds generator_meta)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "DQX_GENERATED_CHECKS_CONFIG_SCHEMA = T.StructType([\n",
    "    T.StructField(\"check_id\",            T.StringType(),  False),\n",
    "    T.StructField(\"check_id_payload\",    T.StringType(),  False),\n",
    "    T.StructField(\"table_name\",          T.StringType(),  False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",                T.StringType(),  False),\n",
    "    T.StructField(\"criticality\",         T.StringType(),  False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(),  False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",              T.StringType(),  True),\n",
    "    T.StructField(\"run_config_name\",     T.StringType(),  False),\n",
    "    T.StructField(\"user_metadata\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # Ops\n",
    "    T.StructField(\"yaml_path\",           T.StringType(),  False),\n",
    "    T.StructField(\"active\",              T.BooleanType(), False),\n",
    "\n",
    "    # NEW: meta goes right here (as requested, before audit)\n",
    "    T.StructField(\"generator_meta\", T.ArrayType(T.StructType([\n",
    "        T.StructField(\"section\", T.StringType(), False),  # \"profile_options\" | \"generator_settings\"\n",
    "        T.StructField(\"payload\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ])), True),\n",
    "\n",
    "    # Audit\n",
    "    T.StructField(\"created_by\",          T.StringType(),  False),\n",
    "    T.StructField(\"created_at\",          T.StringType(),  False),  # ISO string; cast downstream if needed\n",
    "    T.StructField(\"updated_by\",          T.StringType(),  True),\n",
    "    T.StructField(\"updated_at\",          T.StringType(),  True),\n",
    "])\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Constants / helpers (kept intact)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "DOC_SUPPORTED_KEYS = {\n",
    "    \"sample_fraction\", \"sample_seed\", \"limit\",\n",
    "    \"remove_outliers\", \"outlier_columns\", \"num_sigmas\",\n",
    "    \"max_null_ratio\", \"trim_strings\", \"max_empty_ratio\",\n",
    "    \"distinct_ratio\", \"max_in_count\", \"round\",\n",
    "}\n",
    "\n",
    "_YAML_PATH_RE = re.compile(r\"\\.(ya?ml)$\", re.IGNORECASE)\n",
    "_FROM_INFOSCHEMA = re.compile(r\"FROM\\s+([A-Za-z0-9_]+)\\.information_schema\\.tables\", re.IGNORECASE)\n",
    "_TABLE_SCHEMA_EQ = re.compile(r\"table_schema\\s*=\\s*'([^']+)'\", re.IGNORECASE)\n",
    "\n",
    "def _is_yaml_path(p: str) -> bool:\n",
    "    return bool(_YAML_PATH_RE.search(p))\n",
    "\n",
    "def _esc_sql_comment(s: str) -> str:\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "def _safe_json(obj: Any) -> str:\n",
    "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def _resolve_local_like_path(path: str) -> Optional[str]:\n",
    "    \"\"\"Resolve a repo/local-like path by walking up a few parents.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return os.path.abspath(path)\n",
    "    base = os.getcwd()\n",
    "    for _ in range(6):\n",
    "        cand = os.path.abspath(os.path.join(base, path))\n",
    "        if os.path.exists(cand):\n",
    "            return cand\n",
    "        parent = os.path.dirname(base)\n",
    "        if parent == base:\n",
    "            break\n",
    "        base = parent\n",
    "    return None\n",
    "\n",
    "def _read_text_any(path: str) -> str:\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"dbutils is required to read from DBFS/Volumes\") from e\n",
    "        target = path if path.startswith(\"dbfs:\") else (f\"dbfs:{path}\" if path.startswith(\"/\") else f\"dbfs:/{path}\")\n",
    "        return dbutils.fs.head(target, 10 * 1024 * 1024)\n",
    "\n",
    "    # Workspace Files (absolute)\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            data = wc.files.download(file_path=path).read()\n",
    "        except TypeError:\n",
    "            data = wc.files.download(path=path).read()\n",
    "        return data.decode(\"utf-8\")\n",
    "\n",
    "    # Local / repo-relative\n",
    "    resolved = _resolve_local_like_path(path)\n",
    "    if resolved and os.path.isfile(resolved):\n",
    "        with open(resolved, \"r\", encoding=\"utf-8\") as fh:\n",
    "            return fh.read()\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not read file: {path}\")\n",
    "\n",
    "def _ensure_parent_local(path: str) -> None:\n",
    "    parent = os.path.dirname(path)\n",
    "    if parent and not os.path.exists(parent):\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "def _to_dbfs_target(path: str) -> str:\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        return path\n",
    "    if path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        return \"dbfs:\" + path\n",
    "    return path\n",
    "\n",
    "def _write_text_any(path: str, payload: str) -> None:\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"dbutils is required to write to DBFS/Volumes.\")\n",
    "        target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "        parent = target.rsplit(\"/\", 1)[0]\n",
    "        if parent:\n",
    "            dbutils.fs.mkdirs(parent)\n",
    "        dbutils.fs.put(target, payload, True)\n",
    "        return\n",
    "\n",
    "    # Workspace Files\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            wc.files.upload(file_path=path, contents=payload.encode(\"utf-8\"), overwrite=True)\n",
    "        except TypeError:\n",
    "            wc.files.upload(path=path, contents=payload.encode(\"utf-8\"), overwrite=True)\n",
    "        return\n",
    "\n",
    "    # Local (Repos/driver)\n",
    "    full = os.path.abspath(path)\n",
    "    _ensure_parent_local(full)\n",
    "    with open(full, \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(payload)\n",
    "\n",
    "def _parse_global_hints_from_comments(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    m_cat = _FROM_INFOSCHEMA.search(text)\n",
    "    m_sch = _TABLE_SCHEMA_EQ.search(text)\n",
    "    return (m_cat.group(1) if m_cat else None, m_sch.group(1) if m_sch else None)\n",
    "\n",
    "def _ensure_fqns(names: List[str], hint_catalog: Optional[str], hint_schema: Optional[str]) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for n in names:\n",
    "        n = n.strip()\n",
    "        if not n:\n",
    "            continue\n",
    "        parts = n.split(\".\")\n",
    "        if len(parts) == 3:\n",
    "            out.append(n)\n",
    "        elif len(parts) == 2:\n",
    "            if not hint_catalog:\n",
    "                raise ValueError(f\"'{n}' lacks catalog; add it or provide a comment default.\")\n",
    "            out.append(f\"{hint_catalog}.{n}\")\n",
    "        elif len(parts) == 1:\n",
    "            if not (hint_catalog and hint_schema):\n",
    "                raise ValueError(f\"'{n}' needs catalog & schema; add comments or use dotted forms.\")\n",
    "            out.append(f\"{hint_catalog}.{hint_schema}.{n}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized table format: {n}\")\n",
    "    return sorted(set(out))\n",
    "\n",
    "def _discover_tables_from_yaml_file(yaml_path: str) -> List[str]:\n",
    "    \"\"\"YAML contains e.g. `table_name: [a, b, c]`. Comments can hint catalog/schema.\"\"\"\n",
    "    text = _read_text_any(yaml_path)\n",
    "    cat_hint, sch_hint = _parse_global_hints_from_comments(text)\n",
    "    obj = yaml.safe_load(io.StringIO(text))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(f\"YAML must contain a mapping with a list; got: {type(obj).__name__}\")\n",
    "    names = None\n",
    "    for key in (\"table_name\", \"tables\", \"table_names\", \"list\"):\n",
    "        if isinstance(obj.get(key), list):\n",
    "            names = [str(x).strip() for x in obj[key] if x]\n",
    "            break\n",
    "    if not names:\n",
    "        raise ValueError(f\"No table list found in YAML: {yaml_path}\")\n",
    "    return _ensure_fqns(names, cat_hint, sch_hint)\n",
    "\n",
    "def _prefix_of(table_fqn: str) -> str:\n",
    "    \"\"\"Prefix up to first underscore of the *table* portion.\"\"\"\n",
    "    base = table_fqn.split(\".\")[-1]\n",
    "    return base.split(\"_\", 1)[0].lower() if base else \"\"\n",
    "\n",
    "def _filter_by_prefix_regex(tables: List[str], exclude_prefix_regex: Optional[str]) -> List[str]:\n",
    "    if not exclude_prefix_regex:\n",
    "        return tables\n",
    "    pat = re.compile(exclude_prefix_regex, re.IGNORECASE)\n",
    "    keep: List[str] = []\n",
    "    for t in tables:\n",
    "        if not pat.search(_prefix_of(t)):\n",
    "            keep.append(t)\n",
    "    return keep\n",
    "\n",
    "def _display_table_preview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    rows = [(f, *f.split(\".\")) for f in fqns]\n",
    "    df = spark.createDataFrame(rows, \"fqn string, catalog string, schema string, table string\")\n",
    "    print(f\"\\n=== {title} ({len(fqns)}) ===\")\n",
    "    try:\n",
    "        display(df)\n",
    "    except NameError:\n",
    "        df.show(len(fqns), truncate=False)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "\n",
    "def _stringify_map_values(d: Optional[Dict[str, Any]]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = _safe_json(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def _compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    def _canon_filter(s: Optional[str]) -> str:\n",
    "        return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "    def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "        fec = chk.get(\"for_each_column\")\n",
    "        if isinstance(fec, list):\n",
    "            out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "        args = chk.get(\"arguments\") or {}\n",
    "        canon_args: Dict[str, str] = {}\n",
    "        for k, v in args.items():\n",
    "            sv = \"\" if v is None else str(v).strip()\n",
    "            if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "                try:\n",
    "                    sv = _safe_json(json.loads(sv))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            canon_args[str(k)] = sv\n",
    "        out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "        return out\n",
    "\n",
    "    payload_obj = {\n",
    "        \"table_name\": (table_name or \"\").lower(),\n",
    "        \"filter\": _canon_filter(filter_str),\n",
    "        \"check\": _canon_check(check_dict or {}),\n",
    "    }\n",
    "    return _safe_json(payload_obj)\n",
    "\n",
    "def _compute_check_id(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Public aliases (you asked to keep these helper names around)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def DisplayTablePreview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    return _display_table_preview(spark, fqns, title)\n",
    "\n",
    "def NowISO() -> str:\n",
    "    return _now_iso()\n",
    "\n",
    "def GetNotebookPath() -> str:\n",
    "    return _nb_path()\n",
    "\n",
    "def PathStartsWith(s: str, *prefixes: str) -> bool:\n",
    "    return any(s.startswith(p) for p in prefixes)\n",
    "\n",
    "def ResolveLocalLikePath(p: str) -> Optional[str]:\n",
    "    return _resolve_local_like_path(p)\n",
    "\n",
    "def DiscoverTablesFromYAML(yaml_path: str) -> List[str]:\n",
    "    return _discover_tables_from_yaml_file(yaml_path)\n",
    "\n",
    "def ListYAMLPathsInFolder(folder: str) -> List[str]:\n",
    "    # DBFS/Volumes\n",
    "    out: List[str] = []\n",
    "    if folder.startswith(\"dbfs:/\") or folder.startswith(\"/dbfs/\") or folder.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"dbutils is required to traverse DBFS/Volumes.\")\n",
    "        root = _to_dbfs_target(folder)\n",
    "        def _walk_dbfs(dirpath: str):\n",
    "            for fi in dbutils.fs.ls(dirpath):\n",
    "                p = fi.path\n",
    "                if p.endswith(\"/\"):\n",
    "                    _walk_dbfs(p)\n",
    "                elif _is_yaml_path(p):\n",
    "                    out.append(p)\n",
    "        _walk_dbfs(root)\n",
    "        return out\n",
    "\n",
    "    # Workspace Files -> best-effort local resolve for recursion\n",
    "    if folder.startswith(\"/\"):\n",
    "        resolved = _resolve_local_like_path(folder)\n",
    "        if resolved and os.path.isdir(resolved):\n",
    "            for r, _, files in os.walk(resolved):\n",
    "                for f in files:\n",
    "                    if _is_yaml_path(f):\n",
    "                        out.append(os.path.join(r, f))\n",
    "            return out\n",
    "        return out\n",
    "\n",
    "    # Local / repo\n",
    "    resolved = _resolve_local_like_path(folder)\n",
    "    if resolved and os.path.isdir(resolved):\n",
    "        for r, _, files in os.walk(resolved):\n",
    "            for f in files:\n",
    "                if _is_yaml_path(f):\n",
    "                    out.append(os.path.join(r, f))\n",
    "    return out\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Pretty display helpers (as requested)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _can_display() -> bool:\n",
    "    return \"display\" in globals()\n",
    "\n",
    "def show_df(df: DataFrame, n: int = 100, truncate: bool = False) -> None:\n",
    "    if _can_display():\n",
    "        display(df.limit(n))\n",
    "    else:\n",
    "        df.show(n, truncate=truncate)\n",
    "\n",
    "def display_section(title: str) -> None:\n",
    "    print(\"\\n\" + \"═\" * 80)\n",
    "    print(f\"║ {title}\")\n",
    "    print(\"═\" * 80)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Column comment helper with robust multi-syntax fallback\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _apply_column_comment_with_fallback(\n",
    "    spark: SparkSession,\n",
    "    cat: str,\n",
    "    sch: str,\n",
    "    tbl: str,\n",
    "    col_name: str,\n",
    "    comment_text: str,\n",
    "    col_types_lower: Dict[str, str],\n",
    ") -> bool:\n",
    "    \"\"\"Try COMMENT ON COLUMN, then ALTER ... ALTER COLUMN, then CHANGE COLUMN with type.\"\"\"\n",
    "    fqn_q = f\"`{cat}`.`{sch}`.`{tbl}`\"\n",
    "    col_q = f\"`{col_name}`\"\n",
    "    cmt = _esc_sql_comment(comment_text)\n",
    "\n",
    "    # 1) COMMENT ON COLUMN\n",
    "    try:\n",
    "        spark.sql(f\"COMMENT ON COLUMN {fqn_q}.{col_q} IS '{cmt}'\")\n",
    "        return True\n",
    "    except Exception as e1:\n",
    "        pass\n",
    "\n",
    "    # 2) ALTER TABLE ... ALTER COLUMN ... COMMENT\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {fqn_q} ALTER COLUMN {col_q} COMMENT '{cmt}'\")\n",
    "        return True\n",
    "    except Exception as e2:\n",
    "        pass\n",
    "\n",
    "    # 3) ALTER TABLE ... CHANGE COLUMN col col <type> COMMENT ...\n",
    "    dtype = col_types_lower.get(col_name.lower())\n",
    "    if not dtype:\n",
    "        print(f\"[WARN] Cannot determine data type for {cat}.{sch}.{tbl}.{col_name}; skipping column comment.\")\n",
    "        return False\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {fqn_q} CHANGE COLUMN {col_q} {col_q} {dtype} COMMENT '{cmt}'\")\n",
    "        return True\n",
    "    except Exception as e3:\n",
    "        print(f\"[WARN] Failed to set comment for {cat}.{sch}.{tbl}.{col_name}: {e3}\")\n",
    "        return False\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Table documentation application\n",
    "#  - Table comment applied when just-created\n",
    "#  - Column comments applied ALWAYS using robust fallback (fixes your error)\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def _apply_table_documentation_on_create(spark: SparkSession, table_fqn: str, doc: Dict[str, Any], just_created: bool):\n",
    "    try:\n",
    "        cat, sch, tbl = table_fqn.split(\".\")\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "    table_comment = (doc or {}).get(\"table_comment\") or \"\"\n",
    "    if table_comment and just_created:\n",
    "        spark.sql(\n",
    "            f\"COMMENT ON TABLE `{cat}`.`{sch}`.`{tbl}` IS '{_esc_sql_comment(table_comment)}'\"\n",
    "        )\n",
    "\n",
    "    cols_doc: Dict[str, str] = (doc or {}).get(\"columns\") or {}\n",
    "    if not cols_doc:\n",
    "        return\n",
    "\n",
    "    # Discover existing columns and their types (for fallback syntax)\n",
    "    desc_rows = spark.sql(f\"DESCRIBE TABLE `{cat}`.`{sch}`.`{tbl}`\").collect()\n",
    "    existing_cols = {}\n",
    "    col_types = {}\n",
    "    for r in desc_rows:\n",
    "        if r.col_name and not r.col_name.startswith(\"#\"):\n",
    "            existing_cols[r.col_name.lower()] = True\n",
    "            # Some rows may not have data_type; guard defensively\n",
    "            if hasattr(r, \"data_type\") and r.data_type:\n",
    "                col_types[r.col_name.lower()] = r.data_type\n",
    "\n",
    "    # Apply column comments when columns exist\n",
    "    for col_name, cmt in cols_doc.items():\n",
    "        if col_name.lower() not in existing_cols:\n",
    "            continue\n",
    "        _apply_column_comment_with_fallback(\n",
    "            spark, cat, sch, tbl, col_name, cmt, col_types_lower=col_types\n",
    "        )\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Main generator\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "class CheckGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: str,                         # \"pipeline\" | \"catalog\" | \"schema\" | \"table\" | \"file\" | \"folder\"\n",
    "        source: str,                        # depends on scope\n",
    "        output_format: str,                 # \"yaml\" | \"table\" | \"both\"\n",
    "        output_yaml: Optional[str],         # folder or /Volumes/... or dbfs:/... or workspace \"/...\"\n",
    "        output_table: Optional[str],        # fully-qualified table FQN\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_prefix_regex: Optional[str] = None,   # regex on table prefix (before first \"_\")\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,          # only valid when scope==\"table\"\n",
    "        run_config_name: str = \"default\",\n",
    "        criticality: str = \"warn\",\n",
    "        key_order: Literal[\"engine\", \"custom\"] = \"custom\",\n",
    "        include_table_name: bool = True,\n",
    "        yaml_metadata: bool = False,                  # add commented header on each YAML\n",
    "        table_doc: Optional[Dict[str, Any]] = None,   # documentation dict; defaults to DQX_GENERATED_CHECKS_CONFIG_METADATA\n",
    "    ):\n",
    "        self.scope = scope.lower().strip()\n",
    "        self.source = source\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_yaml = output_yaml\n",
    "        self.output_table = output_table\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_prefix_regex = exclude_prefix_regex\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.key_order = key_order\n",
    "        self.include_table_name = include_table_name\n",
    "        self.yaml_metadata = yaml_metadata\n",
    "        self.table_doc = table_doc or DQX_GENERATED_CHECKS_CONFIG_METADATA\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "        self._validate_top_level()\n",
    "\n",
    "    # -------------------\n",
    "    # Validation\n",
    "    # -------------------\n",
    "    def _validate_top_level(self):\n",
    "        allowed_scopes = {\"pipeline\", \"catalog\", \"schema\", \"table\", \"file\", \"folder\"}\n",
    "        if self.scope not in allowed_scopes:\n",
    "            raise ValueError(f\"Invalid scope '{self.scope}'. Must be one of: {sorted(allowed_scopes)}.\")\n",
    "\n",
    "        allowed_formats = {\"yaml\", \"table\", \"both\"}\n",
    "        if self.output_format not in allowed_formats:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table' or 'both'.\")\n",
    "\n",
    "        # Source expectations per scope (explicit notes)\n",
    "        # scope=\"pipeline\" -> source=\"pipeline_name1,pipeline_name2\"\n",
    "        # scope=\"catalog\"  -> source=\"catalog\"\n",
    "        # scope=\"schema\"   -> source=\"catalog.schema\"\n",
    "        # scope=\"table\"    -> source=\"catalog.schema.table[,catalog.schema.table]\"\n",
    "        # scope=\"file\"     -> source=\"<path to YAML file listing tables>\"\n",
    "        # scope=\"folder\"   -> source=\"<path to folder with YAML table lists>\"\n",
    "        if self.scope == \"catalog\":\n",
    "            if \".\" in self.source:\n",
    "                raise ValueError(\"For scope='catalog', pass just the catalog name (no dots).\")\n",
    "        if self.scope == \"schema\":\n",
    "            if self.source.count(\".\") != 1:\n",
    "                raise ValueError(\"For scope='schema', pass 'catalog.schema'.\")\n",
    "        if self.scope == \"table\":\n",
    "            for t in [x.strip() for x in self.source.split(\",\") if x.strip()]:\n",
    "                if t.count(\".\") != 2:\n",
    "                    raise ValueError(f\"Invalid table FQN '{t}'. Use catalog.schema.table\")\n",
    "        if self.scope == \"file\":\n",
    "            if not _is_yaml_path(self.source):\n",
    "                raise ValueError(\"For scope='file', source must be a YAML path.\")\n",
    "        # folder: any path is ok; we'll scan recursively\n",
    "\n",
    "        # Sinks\n",
    "        if self.output_format == \"yaml\" and not self.output_yaml:\n",
    "            raise ValueError(\"output_yaml is required when output_format='yaml'.\")\n",
    "        if self.output_format == \"table\" and not self.output_table:\n",
    "            raise ValueError(\"output_table is required when output_format='table'.\")\n",
    "        if self.output_format == \"both\":\n",
    "            if not self.output_yaml or not self.output_table:\n",
    "                raise ValueError(\"When output_format='both', both output_yaml and output_table are required.\")\n",
    "\n",
    "    # -------------------\n",
    "    # Discovery\n",
    "    # -------------------\n",
    "    def _walk_yaml_files(self, folder: str) -> List[str]:\n",
    "        return ListYAMLPathsInFolder(folder)\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"scope:            {self.scope}\")\n",
    "        print(f\"source:           {self.source}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_yaml:      {self.output_yaml}\")\n",
    "        print(f\"output_table:     {self.output_table}\")\n",
    "        print(f\"exclude_prefix_rx:{self.exclude_prefix_regex}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"key_order:        {self.key_order}\")\n",
    "        print(f\"include_table_name: {self.include_table_name}\")\n",
    "        print(f\"yaml_metadata:    {self.yaml_metadata}\")\n",
    "        print(\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "\n",
    "        if self.scope == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipeline_names = [p.strip() for p in self.source.split(\",\") if p.strip()]\n",
    "            for pipeline_name in pipeline_names:\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                discovered += [x for x in pipeline_tables if x]\n",
    "\n",
    "        elif self.scope == \"catalog\":\n",
    "            print(\"Discovering all tables in catalog...\")\n",
    "            catalog = self.source.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                discovered += [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.scope == \"schema\":\n",
    "            print(\"Discovering all tables in schema...\")\n",
    "            catalog, schema = self.source.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            discovered = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.scope == \"table\":\n",
    "            print(\"Using provided fully-qualified table(s)...\")\n",
    "            discovered = [t.strip() for t in self.source.split(\",\") if t.strip()]\n",
    "\n",
    "        elif self.scope == \"file\":\n",
    "            print(\"Reading table list from YAML file...\")\n",
    "            discovered = _discover_tables_from_yaml_file(self.source)\n",
    "\n",
    "        else:  # folder\n",
    "            print(\"Reading table lists from all YAML files in folder (recursive)...\")\n",
    "            yaml_files = self._walk_yaml_files(self.source)\n",
    "            agg: List[str] = []\n",
    "            for yp in yaml_files:\n",
    "                try:\n",
    "                    agg += _discover_tables_from_yaml_file(yp)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Skipping YAML '{yp}': {e}\")\n",
    "            discovered = sorted(set(agg))\n",
    "\n",
    "        discovered = _filter_by_prefix_regex(discovered, self.exclude_prefix_regex)\n",
    "        _display_table_preview(self.spark, discovered, title=\"Final table list to generate DQX rules for\")\n",
    "        print(\"==========================================\\n\")\n",
    "        return discovered\n",
    "\n",
    "    # -------------------\n",
    "    # Profiler call args\n",
    "    # -------------------\n",
    "    def _profile_call_kwargs(self) -> Dict[str, Any]:\n",
    "        kwargs: Dict[str, Any] = {}\n",
    "        if self.columns is not None:\n",
    "            kwargs[\"cols\"] = self.columns\n",
    "        if self.profile_options:\n",
    "            unknown = sorted(set(self.profile_options) - DOC_SUPPORTED_KEYS)\n",
    "            if unknown:\n",
    "                print(f\"[INFO] Profiling options not in current docs (passing through anyway): {unknown}\")\n",
    "            kwargs[\"options\"] = self.profile_options\n",
    "        return kwargs\n",
    "\n",
    "    # -------------------\n",
    "    # Rule shaping\n",
    "    # -------------------\n",
    "    def _dq_constraint_to_check(self, rule_name: str, constraint_sql: str, table_name: str) -> Dict[str, Any]:\n",
    "        d = {\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": self.criticality,\n",
    "            \"run_config_name\": self.run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    # remove duplicate 'name' from arguments per your preference\n",
    "                    \"expression\": constraint_sql,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if self.include_table_name:\n",
    "            d = {\"table_name\": table_name, **d}  # keep table_name first\n",
    "        return d\n",
    "\n",
    "    # -------------------\n",
    "    # YAML emission (header + list items with blank lines)\n",
    "    # -------------------\n",
    "    def _yaml_header_block(self, table_fqn: str, env_info: Dict[str, Any]) -> str:\n",
    "        dashed = \"-\" * 81\n",
    "        lines = [\n",
    "            \"#\" * 76,\n",
    "            f\"# GENERATED DQX CHECKS\",\n",
    "            f\"# Table: {table_fqn}\",\n",
    "            f\"# Generated at (UTC): {env_info.get('utc_time','')}\",\n",
    "            f\"# Notebook: {env_info.get('notebook_path','Unknown')}\",\n",
    "            f\"# Spark: {env_info.get('spark_version','')}  |  Python: {env_info.get('python_version','')}\",\n",
    "            f\"# Cluster: {env_info.get('cluster_name','')} ({env_info.get('cluster_id','')})  |  Executor memory: {env_info.get('executor_memory','')}\",\n",
    "            \"#\" * 76,\n",
    "            \"\",\n",
    "            f\"# {dashed}\",\n",
    "            \"# Profile options:\",\n",
    "            \"# \" + _safe_json(self.profile_options),\n",
    "            \"# Generator settings:\",\n",
    "            \"# \" + _safe_json({\n",
    "                \"scope\": self.scope,\n",
    "                \"source\": self.source,\n",
    "                \"output_format\": self.output_format,\n",
    "                \"output_yaml\": self.output_yaml,\n",
    "                \"output_table\": self.output_table,\n",
    "                \"criticality\": self.criticality,\n",
    "                \"run_config_name\": self.run_config_name,\n",
    "                \"include_table_name\": self.include_table_name,\n",
    "                \"key_order\": self.key_order,\n",
    "                \"exclude_prefix_regex\": self.exclude_prefix_regex,\n",
    "            }),\n",
    "        ]\n",
    "        return \"\\n\".join(lines) + \"\\n\\n\"\n",
    "\n",
    "    def _dump_rules_as_yaml_stream(self, rules: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Emit a single YAML document that is a list of rule objects:\n",
    "        - table_name: ...\n",
    "          name: ...\n",
    "          ...\n",
    "        (Blank line between items for readability.)\n",
    "        \"\"\"\n",
    "        pieces: List[str] = []\n",
    "        for r in rules:\n",
    "            block = yaml.safe_dump(r, sort_keys=False, default_flow_style=False).rstrip()\n",
    "            lines = block.splitlines()\n",
    "            if not lines:\n",
    "                continue\n",
    "            first = f\"- {lines[0]}\"\n",
    "            rest = \"\\n\".join((\"  \" + ln) for ln in lines[1:])\n",
    "            pieces.append(first + (\"\\n\" + rest if rest else \"\"))\n",
    "        return \"\\n\\n\".join(pieces) + \"\\n\"\n",
    "\n",
    "    # -------------------\n",
    "    # Table write helpers\n",
    "    # -------------------\n",
    "    def _ensure_schema_exists(self, fqn: str):\n",
    "        cat, sch, _ = fqn.split(\".\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "\n",
    "    def _write_rows_to_table(self, fqn: str, rows: List[Dict[str, Any]], mode: str = \"append\"):\n",
    "        self._ensure_schema_exists(fqn)\n",
    "        existed = self.spark.catalog.tableExists(fqn)\n",
    "        if not existed:\n",
    "            # create empty table with correct schema first, then apply docs\n",
    "            empty_df = self.spark.createDataFrame([], DQX_GENERATED_CHECKS_CONFIG_SCHEMA)\n",
    "            empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(fqn)\n",
    "        # Apply docs (table comment only on create; column comments always with fallback)\n",
    "        _apply_table_documentation_on_create(self.spark, fqn, {**self.table_doc, \"table\": fqn}, just_created=(not existed))\n",
    "\n",
    "        df = self.spark.createDataFrame(rows, schema=DQX_GENERATED_CHECKS_CONFIG_SCHEMA)\n",
    "        df.write.format(\"delta\").mode(mode).saveAsTable(fqn)\n",
    "        print(f\"[WRITE] {len(rows)} rows -> {fqn} ({mode})\")\n",
    "\n",
    "    # -------------------\n",
    "    # Summary display\n",
    "    # -------------------\n",
    "    def _show_summary_table(self, summary: Dict[str, Dict[str, Any]]):\n",
    "        if not summary:\n",
    "            display_section(\"Checks written per table\")\n",
    "            print(\"(no tables processed)\")\n",
    "            return\n",
    "        rows = []\n",
    "        for t, s in summary.items():\n",
    "            rows.append((\n",
    "                s.get(\"table_name\", t),\n",
    "                int(s.get(\"checks_generated\", 0)),\n",
    "                bool(s.get(\"wrote_yaml\", False)),\n",
    "                s.get(\"yaml_path\", None),\n",
    "                int(s.get(\"table_rows_written\", 0)),\n",
    "                self.output_table or \"\",\n",
    "            ))\n",
    "        schema = \"table_name string, checks_generated int, wrote_yaml boolean, yaml_path string, table_rows_written int, output_table string\"\n",
    "        df = self.spark.createDataFrame(rows, schema=schema)\n",
    "        display_section(\"Checks written per table\")\n",
    "        show_df(df.orderBy(\"table_name\"))\n",
    "\n",
    "    # -------------------\n",
    "    # Main\n",
    "    # -------------------\n",
    "    def run(self):\n",
    "        dq_engine = DQEngine(WorkspaceClient())\n",
    "        profiler = DQProfiler(WorkspaceClient())\n",
    "        generator = DQDltGenerator(WorkspaceClient())\n",
    "\n",
    "        env_info = print_notebook_env(self.spark)  # prints banner and returns dict\n",
    "        call_kwargs = self._profile_call_kwargs()\n",
    "        tables = self._discover_tables()\n",
    "\n",
    "        all_rows_for_table_sink: List[Dict[str, Any]] = []\n",
    "        written_yaml_paths: List[str] = []\n",
    "\n",
    "        # Per-table summary tracking\n",
    "        per_table_summary: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        for fq in tables:\n",
    "            if fq.count(\".\") != 2:\n",
    "                print(f\"[WARN] Skipping invalid table name: {fq}\")\n",
    "                continue\n",
    "\n",
    "            cat, sch, tab = fq.split(\".\")\n",
    "            per_table_summary.setdefault(fq, {\n",
    "                \"table_name\": fq,\n",
    "                \"checks_generated\": 0,\n",
    "                \"wrote_yaml\": False,\n",
    "                \"yaml_path\": None,\n",
    "                \"table_rows_written\": 0,\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                self.spark.table(fq).limit(1).collect()  # readability probe\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Table not readable: {fq} -> {e}\")\n",
    "                continue\n",
    "\n",
    "            # Profile & generate DLT rules\n",
    "            try:\n",
    "                df = self.spark.table(fq)\n",
    "                _, profiles = profiler.profile(df, **call_kwargs)\n",
    "                rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Profiling/rule-gen failed for {fq}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Shape checks\n",
    "            checks: List[Dict[str, Any]] = []\n",
    "            for rule_name, constraint_sql in (rules_dict or {}).items():\n",
    "                checks.append(self._dq_constraint_to_check(rule_name, constraint_sql, fq))\n",
    "\n",
    "            per_table_summary[fq][\"checks_generated\"] = len(checks)\n",
    "\n",
    "            if not checks:\n",
    "                print(f\"[INFO] No checks generated for {fq}.\")\n",
    "                continue\n",
    "\n",
    "            # YAML sink\n",
    "            yaml_path_for_rows: str = f\"<generated://{fq}>\"\n",
    "            if self.output_format in {\"yaml\", \"both\"}:\n",
    "                if self.output_yaml.endswith((\".yaml\", \".yml\")):\n",
    "                    path = self.output_yaml  # explicit file path (edge case)\n",
    "                else:\n",
    "                    path = f\"{self.output_yaml.rstrip('/')}/{tab}.yaml\"\n",
    "\n",
    "                if self.key_order == \"engine\":\n",
    "                    cfg = self._infer_file_storage_config(path)\n",
    "                    dq_engine.save_checks(checks, config=cfg)  # writes list-of-dicts\n",
    "                else:\n",
    "                    header = self._yaml_header_block(fq, env_info) if self.yaml_metadata else \"\"\n",
    "                    body = self._dump_rules_as_yaml_stream(checks)\n",
    "                    _write_text_any(path, header + body)\n",
    "\n",
    "                yaml_path_for_rows = path\n",
    "                written_yaml_paths.append(path)\n",
    "                per_table_summary[fq][\"wrote_yaml\"] = True\n",
    "                per_table_summary[fq][\"yaml_path\"] = path\n",
    "                print(f\"[RUN] Wrote {len(checks)} rule(s) to YAML: {path}\")\n",
    "\n",
    "            # Prepare table rows (for TABLE only path; BOTH path reloads YAMLs to avoid drift)\n",
    "            if self.output_format == \"table\":\n",
    "                gen_meta = [\n",
    "                    {\"section\": \"profile_options\", \"payload\": _stringify_map_values(self.profile_options)},\n",
    "                    {\"section\": \"generator_settings\", \"payload\": _stringify_map_values({\n",
    "                        \"scope\": self.scope, \"source\": self.source, \"output_format\": self.output_format,\n",
    "                        \"output_yaml\": self.output_yaml or \"\", \"output_table\": self.output_table or \"\",\n",
    "                        \"criticality\": self.criticality, \"run_config_name\": self.run_config_name,\n",
    "                        \"include_table_name\": self.include_table_name, \"key_order\": self.key_order,\n",
    "                        \"exclude_prefix_regex\": self.exclude_prefix_regex or \"\",\n",
    "                    })},\n",
    "                ]\n",
    "                for rule in checks:\n",
    "                    raw_check = rule[\"check\"]\n",
    "                    payload = _compute_check_id_payload(fq, raw_check, rule.get(\"filter\"))\n",
    "                    all_rows_for_table_sink.append({\n",
    "                        \"check_id\": _compute_check_id(payload),\n",
    "                        \"check_id_payload\": payload,\n",
    "                        \"table_name\": fq,\n",
    "                        \"name\": rule[\"name\"],\n",
    "                        \"criticality\": rule[\"criticality\"],\n",
    "                        \"check\": {\n",
    "                            \"function\": raw_check.get(\"function\"),\n",
    "                            \"for_each_column\": raw_check.get(\"for_each_column\"),\n",
    "                            \"arguments\": _stringify_map_values(raw_check.get(\"arguments\") or {}),\n",
    "                        },\n",
    "                        \"filter\": rule.get(\"filter\"),\n",
    "                        \"run_config_name\": rule[\"run_config_name\"],\n",
    "                        \"user_metadata\": _stringify_map_values(rule.get(\"user_metadata\") or None) or None,\n",
    "                        \"yaml_path\": yaml_path_for_rows,\n",
    "                        \"active\": True,\n",
    "                        \"generator_meta\": gen_meta,\n",
    "                        \"created_by\": self.created_by,\n",
    "                        \"created_at\": _now_iso(),\n",
    "                        \"updated_by\": None,\n",
    "                        \"updated_at\": None,\n",
    "                    })\n",
    "\n",
    "        # BOTH → reload the exact YAMLs we wrote and write those rows (canonical)\n",
    "        if self.output_format == \"both\":\n",
    "            rows_from_yaml: List[Dict[str, Any]] = []\n",
    "            for yp in written_yaml_paths:\n",
    "                try:\n",
    "                    txt = _read_text_any(yp)\n",
    "                    docs = list(yaml.safe_load_all(io.StringIO(txt)))\n",
    "                    rules: List[dict] = []\n",
    "                    for d in docs:\n",
    "                        if not d:\n",
    "                            continue\n",
    "                        if isinstance(d, dict):\n",
    "                            rules.append(d)\n",
    "                        elif isinstance(d, list):\n",
    "                            rules.extend([x for x in d if isinstance(x, dict)])\n",
    "\n",
    "                    # If file is a single list (our custom format), docs will be [list]; handled above.\n",
    "                    for r in rules:\n",
    "                        fq = r.get(\"table_name\")\n",
    "                        raw_check = r.get(\"check\") or {}\n",
    "                        payload = _compute_check_id_payload(fq, raw_check, r.get(\"filter\"))\n",
    "                        row_obj = {\n",
    "                            \"check_id\": _compute_check_id(payload),\n",
    "                            \"check_id_payload\": payload,\n",
    "                            \"table_name\": fq,\n",
    "                            \"name\": r.get(\"name\"),\n",
    "                            \"criticality\": r.get(\"criticality\"),\n",
    "                            \"check\": {\n",
    "                                \"function\": raw_check.get(\"function\"),\n",
    "                                \"for_each_column\": raw_check.get(\"for_each_column\"),\n",
    "                                \"arguments\": _stringify_map_values(raw_check.get(\"arguments\") or {}),\n",
    "                            },\n",
    "                            \"filter\": r.get(\"filter\"),\n",
    "                            \"run_config_name\": r.get(\"run_config_name\", self.run_config_name),\n",
    "                            \"user_metadata\": _stringify_map_values(r.get(\"user_metadata\") or None) or None,\n",
    "                            \"yaml_path\": yp,\n",
    "                            \"active\": True,\n",
    "                            \"generator_meta\": [\n",
    "                                {\"section\": \"profile_options\", \"payload\": _stringify_map_values(self.profile_options)},\n",
    "                                {\"section\": \"generator_settings\", \"payload\": _stringify_map_values({\n",
    "                                    \"scope\": self.scope, \"source\": self.source, \"output_format\": self.output_format,\n",
    "                                    \"output_yaml\": self.output_yaml or \"\", \"output_table\": self.output_table or \"\",\n",
    "                                    \"criticality\": self.criticality,\n",
    "                                    \"run_config_name\": r.get(\"run_config_name\", self.run_config_name),\n",
    "                                    \"include_table_name\": self.include_table_name, \"key_order\": self.key_order,\n",
    "                                    \"exclude_prefix_regex\": self.exclude_prefix_regex or \"\",\n",
    "                                })},\n",
    "                            ],\n",
    "                            \"created_by\": self.created_by,\n",
    "                            \"created_at\": _now_iso(),\n",
    "                            \"updated_by\": None,\n",
    "                            \"updated_at\": None,\n",
    "                        }\n",
    "                        rows_from_yaml.append(row_obj)\n",
    "                        # count per-table table_rows_written (we'll add to summary after the write too)\n",
    "                        if fq in per_table_summary:\n",
    "                            per_table_summary[fq][\"table_rows_written\"] = per_table_summary[fq].get(\"table_rows_written\", 0) + 1\n",
    "                        else:\n",
    "                            per_table_summary[fq] = {\n",
    "                                \"table_name\": fq,\n",
    "                                \"checks_generated\": 0,\n",
    "                                \"wrote_yaml\": True,\n",
    "                                \"yaml_path\": yp,\n",
    "                                \"table_rows_written\": 1,\n",
    "                            }\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Could not load back YAML '{yp}' for table sink: {e}\")\n",
    "\n",
    "            if self.output_table and rows_from_yaml:\n",
    "                self._write_rows_to_table(self.output_table, rows_from_yaml, mode=\"append\")\n",
    "            print(f\"[DONE] Wrote YAML files ({len(written_yaml_paths)}). Then loaded {len(rows_from_yaml)} rows into {self.output_table}.\")\n",
    "\n",
    "        elif self.output_format == \"table\":\n",
    "            if self.output_table and all_rows_for_table_sink:\n",
    "                # update per-table counts before write\n",
    "                for r in all_rows_for_table_sink:\n",
    "                    fq = r[\"table_name\"]\n",
    "                    per_table_summary.setdefault(fq, {\n",
    "                        \"table_name\": fq,\n",
    "                        \"checks_generated\": 0,\n",
    "                        \"wrote_yaml\": False,\n",
    "                        \"yaml_path\": None,\n",
    "                        \"table_rows_written\": 0,\n",
    "                    })\n",
    "                    per_table_summary[fq][\"table_rows_written\"] = per_table_summary[fq].get(\"table_rows_written\", 0) + 1\n",
    "\n",
    "                self._write_rows_to_table(self.output_table, all_rows_for_table_sink, mode=\"append\")\n",
    "            print(f\"[DONE] Wrote {len(all_rows_for_table_sink)} rows into {self.output_table}.\")\n",
    "        else:\n",
    "            print(f\"[DONE] Wrote YAML files ({len(written_yaml_paths)}).\")\n",
    "\n",
    "        # Print a nice per-table summary\n",
    "        self._show_summary_table(per_table_summary)\n",
    "\n",
    "    # Storage config passthrough (kept)\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "\n",
    "# -------------------- Usage examples --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": 42,\n",
    "        \"limit\": 1000,\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_in_count\": 10,\n",
    "        \"round\": True,\n",
    "        # other passthrough keys are fine\n",
    "    }\n",
    "\n",
    "    # Example A — scope=\"table\": write BOTH (YAMLs first, then load exactly those YAMLs into the table)\n",
    "    CheckGenerator(\n",
    "        scope=\"schema\",                                  # \"pipeline\" | \"catalog\" | \"schema\" | \"table\" | \"file\" | \"folder\"\n",
    "        source=\"de_prd.gold\",\n",
    "        output_format=\"both\",                           # \"yaml\" | \"table\" | \"both\"\n",
    "        output_yaml=\"/Volumes/dq_dev/dqx/generated_checks/2025-08-12/\",  # Workspace Folder or Volume\n",
    "        yaml_metadata=True,      #(yaml_modifier)      # True = Add run metadata to file | False = Don't add metadata to file\n",
    "        key_order=\"custom\",      #(yaml_modifier)      # \"custom\" = our ordered YAML with list items; \"engine\" = DQX default writer\n",
    "        include_table_name=True, #(yaml_modifier)\n",
    "        output_table=\"dq_dev.dqx.generated_checks_config\",  # 'Table_fqn'  | 'None' if not writing to table\n",
    "        profile_options=profile_options,\n",
    "        exclude_prefix_regex=r\"^tama\",               # exclude tables whose prefix (before _) matches\n",
    "        created_by=\"LMG\",          #(table_modifier)      # populates 'created_by' column\n",
    "        columns=None,              #(check_modifier)      # only valid when scope==\"table\"\n",
    "        run_config_name=\"default\", #(check_modifier)\n",
    "        criticality=\"error\",       #(check_modifier)       # \"error\" | \"warn\"\n",
    "        table_doc=DQX_GENERATED_CHECKS_CONFIG_METADATA,   # used if/when we create the output table\n",
    "    ).run()\n",
    "\n",
    "    \"\"\"\n",
    "    # Example B — scope=\"catalog\": write BOTH (YAMLs first, then load those YAMLs into table)\n",
    "    CheckGenerator(\n",
    "        scope=\"catalog\",\n",
    "        source=\"de_prd\",\n",
    "        output_format=\"both\",\n",
    "        output_yaml=\"dbfs:/mnt/dqx/generated_checks/de_prd\",\n",
    "        output_table=\"dq_dev.dqx.checks_generated_config\",\n",
    "        profile_options=profile_options,\n",
    "        exclude_prefix_regex=r\"^tamarack$\",\n",
    "        created_by=\"LMG\",\n",
    "        columns=None,\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",\n",
    "        include_table_name=True,\n",
    "        yaml_metadata=True,\n",
    "        table_doc=DQX_GENERATED_CHECKS_CONFIG_METADATA,\n",
    "    ).run()\n",
    "\n",
    "    # Example C — scope=\"table\": write directly to table (no YAML)\n",
    "    CheckGenerator(\n",
    "        scope=\"table\",\n",
    "        source=\"dq_prd.monitoring.job_run_audit\",\n",
    "        output_format=\"table\",\n",
    "        output_yaml=None,\n",
    "        output_table=\"dq_dev.dqx.checks_generated_config\",\n",
    "        profile_options=profile_options,\n",
    "        exclude_prefix_regex=None,\n",
    "        created_by=\"LMG\",\n",
    "        columns=None,\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",\n",
    "        include_table_name=True,\n",
    "        yaml_metadata=False,\n",
    "        table_doc=DQX_GENERATED_CHECKS_CONFIG_METADATA,\n",
    "    ).run()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2220230-6333-4a68-a8f5-c87878051f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_generate_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
