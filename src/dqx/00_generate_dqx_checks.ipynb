{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4fe261-69b0-4d1a-a5a0-cb4d445b418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8383f8f1-9db5-46eb-abc3-d472fbc64507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- DQX profiling options: all known keys --\n",
    "profile_options = {\n",
    "    # \"round\": True,                 # (Removed - not valid for this profiler version)\n",
    "    \"max_in_count\": 10,            # Max distinct values for is_in rule\n",
    "    \"distinct_ratio\": 0.05,        # Max unique/total ratio for is_in rule\n",
    "    \"max_null_ratio\": 0.01,        # Max null fraction to allow is_not_null rule\n",
    "    \"remove_outliers\": True,       # Remove outliers for min/max\n",
    "    \"outlier_columns\": [],         # Only these columns get outlier removal (empty=all numerics)\n",
    "    \"num_sigmas\": 3,               # Stddev for outlier removal (z-score cutoff)\n",
    "    \"trim_strings\": True,          # Strip whitespace before profiling strings\n",
    "    \"max_empty_ratio\": 0.01,       # Max empty string ratio for is_not_null_or_empty\n",
    "    \"sample_fraction\": 0.3,        # Row fraction to sample\n",
    "    \"sample_seed\": None,           # Seed for reproducibility (set int for deterministic)\n",
    "    \"limit\": 1000,                 # Max number of rows to profile\n",
    "    \"profile_types\": None,         # List of rule types (e.g. [\"is_in\", \"is_not_null\"]); None=default\n",
    "    \"min_length\": None,            # Min string length to consider (None disables)\n",
    "    \"max_length\": None,            # Max string length to consider (None disables)\n",
    "    \"include_histograms\": False,   # Compute histograms as part of profiling\n",
    "    \"min_value\": None,             # Numeric min override (None disables)\n",
    "    \"max_value\": None,             # Numeric max override (None disables)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2168addb-58e3-4aae-853c-7e2f844cda7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8baa8a-c510-47e7-b107-3204ff7b2c9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29833b1c-0236-46f9-a6f9-adb15dd55ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dqx_rule_generator.py\n",
    "# Full file — adds YAML list support via header-comment parsing + display() preview.\n",
    "# NOTHING REMOVED: all params/options stay. Renamed yaml_key_order -> key_order.\n",
    "# Single-table & CSV FQNs still work. One YAML file per table is written in output_location.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from typing import List, Optional, Dict, Any, Literal, Tuple\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "\n",
    "# Keys currently shown in DQX docs for profiler \"options\" (kept permissive; we only warn on extras)\n",
    "DOC_SUPPORTED_KEYS = {\n",
    "    \"sample_fraction\",\n",
    "    \"sample_seed\",\n",
    "    \"limit\",\n",
    "    \"remove_outliers\",\n",
    "    \"outlier_columns\",\n",
    "    \"num_sigmas\",\n",
    "    \"max_null_ratio\",\n",
    "    \"trim_strings\",\n",
    "    \"max_empty_ratio\",\n",
    "    \"distinct_ratio\",\n",
    "    \"max_in_count\",\n",
    "    \"round\",\n",
    "}\n",
    "\n",
    "# ---------- YAML file support (header-comment parsing for defaults) ----------\n",
    "_YAML_PATH_RE = re.compile(r\"\\.(ya?ml)$\", re.IGNORECASE)\n",
    "_FROM_INFOSCHEMA = re.compile(r\"FROM\\s+([A-Za-z0-9_]+)\\.information_schema\\.tables\", re.IGNORECASE)\n",
    "_TABLE_SCHEMA_EQ = re.compile(r\"table_schema\\s*=\\s*'([^']+)'\", re.IGNORECASE)\n",
    "\n",
    "def _is_yaml_path(path: str) -> bool:\n",
    "    return bool(_YAML_PATH_RE.search(path))\n",
    "\n",
    "def _resolve_local_like_path(path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try hard to resolve a repo-relative file path from a notebook:\n",
    "    - as-given\n",
    "    - join with cwd\n",
    "    - walk up 6 parents and join\n",
    "    Returns a filesystem path if found, else None.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    cwd = os.getcwd()\n",
    "    base = cwd\n",
    "    for _ in range(6):\n",
    "        cand = os.path.abspath(os.path.join(base, path))\n",
    "        candidates.append(cand)\n",
    "        if os.path.exists(cand):\n",
    "            return cand\n",
    "        parent = os.path.dirname(base)\n",
    "        if parent == base:\n",
    "            break\n",
    "        base = parent\n",
    "    return None\n",
    "\n",
    "def _read_text_any(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read small text files from:\n",
    "      - repo/local filesystem (relative or absolute)\n",
    "      - dbfs:/, /dbfs/, /Volumes/\n",
    "      - workspace files path (starts with '/'), via Files API\n",
    "    \"\"\"\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"dbutils is required to read from DBFS/Volumes\") from e\n",
    "        target = path if path.startswith(\"dbfs:\") else (f\"dbfs:{path}\" if path.startswith(\"/\") else f\"dbfs:/{path}\")\n",
    "        return dbutils.fs.head(target, 10 * 1024 * 1024)\n",
    "\n",
    "    # Absolute workspace file path (e.g. /Workspace/Repos/.../file.yaml)\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            # newer SDK signature\n",
    "            data = wc.files.download(file_path=path).read()\n",
    "        except TypeError:\n",
    "            # older SDK signature\n",
    "            data = wc.files.download(path=path).read()\n",
    "        return data.decode(\"utf-8\")\n",
    "\n",
    "    # Relative/local resolution (repo-friendly)\n",
    "    resolved = _resolve_local_like_path(path)\n",
    "    if resolved and os.path.isfile(resolved):\n",
    "        with open(resolved, \"r\", encoding=\"utf-8\") as fh:\n",
    "            return fh.read()\n",
    "\n",
    "    # Nothing worked → helpful error\n",
    "    msg = [\n",
    "        f\"Could not find YAML at '{path}'.\",\n",
    "        f\"cwd={os.getcwd()}\",\n",
    "        \"Hints:\",\n",
    "        \"  - If this file is in your repo, pass a path relative to the notebook or the repo root.\",\n",
    "        \"  - Or pass an absolute workspace path like '/Workspace/Repos/.../file.yaml'.\",\n",
    "        \"  - Or use a DBFS/Volumes path like 'dbfs:/...' or '/Volumes/...'.\",\n",
    "    ]\n",
    "    raise FileNotFoundError(\"\\n\".join(msg))\n",
    "\n",
    "def _parse_global_hints_from_comments(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Pull defaults from header comments only (no YAML keys):\n",
    "      - catalog from: 'FROM <catalog>.information_schema.tables'\n",
    "      - schema  from: \"table_schema = '<schema>'\"\n",
    "    \"\"\"\n",
    "    m_cat = _FROM_INFOSCHEMA.search(text)\n",
    "    m_sch = _TABLE_SCHEMA_EQ.search(text)\n",
    "    cat = m_cat.group(1) if m_cat else None\n",
    "    sch = m_sch.group(1) if m_sch else None\n",
    "    return cat, sch\n",
    "\n",
    "def _ensure_fqns(names: List[str], hint_catalog: Optional[str], hint_schema: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Accept: catalog.schema.table | schema.table | table\n",
    "    Use comment-derived defaults to fill missing parts. Dotted entries override per item.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    for n in names:\n",
    "        n = n.strip()\n",
    "        if not n:\n",
    "            continue\n",
    "        parts = n.split(\".\")\n",
    "        if len(parts) == 3:\n",
    "            out.append(n)\n",
    "        elif len(parts) == 2:\n",
    "            if not hint_catalog:\n",
    "                raise ValueError(f\"'{n}' lacks catalog; add it to the item or provide a comment default.\")\n",
    "            out.append(f\"{hint_catalog}.{n}\")\n",
    "        elif len(parts) == 1:\n",
    "            if not (hint_catalog and hint_schema):\n",
    "                raise ValueError(f\"'{n}' needs catalog & schema; set via comments or use dotted forms.\")\n",
    "            out.append(f\"{hint_catalog}.{hint_schema}.{n}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized table format: {n}\")\n",
    "    bad = [t for t in out if t.count(\".\") != 2]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Invalid FQN(s) after resolution: {bad}\")\n",
    "    return sorted(set(out))\n",
    "\n",
    "def _discover_tables_from_yaml_by_comments(yaml_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load the YAML file (with e.g. 'table_name:' list) and use ONLY header comments\n",
    "    to infer catalog/schema defaults. Dotted items override per entry.\n",
    "    \"\"\"\n",
    "    text = _read_text_any(yaml_path)\n",
    "    cat_hint, sch_hint = _parse_global_hints_from_comments(text)\n",
    "\n",
    "    obj = yaml.safe_load(io.StringIO(text))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(f\"YAML must contain a mapping with a list; got: {type(obj).__name__}\")\n",
    "\n",
    "    names = None\n",
    "    for key in (\"table_name\", \"tables\", \"table_names\", \"list\"):\n",
    "        if isinstance(obj.get(key), list):\n",
    "            names = [str(x).strip() for x in obj[key] if x]\n",
    "            break\n",
    "    if not names:\n",
    "        raise ValueError(f\"No table list found in YAML: {yaml_path}\")\n",
    "\n",
    "    fqns = _ensure_fqns(names, cat_hint, sch_hint)\n",
    "    print(f\"[INFO] Parsed defaults from comments: catalog={cat_hint} schema={sch_hint}\")\n",
    "    return fqns\n",
    "\n",
    "def _display_table_preview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    \"\"\"\n",
    "    Use display() (Databricks) for a clean, interactive preview instead of printing.\n",
    "    \"\"\"\n",
    "    rows = [(f, *f.split(\".\")) for f in fqns]\n",
    "    df = spark.createDataFrame(rows, \"fqn string, catalog string, schema string, table string\")\n",
    "    print(f\"\\n=== {title} ({len(fqns)}) ===\")\n",
    "    try:\n",
    "        display(df)\n",
    "    except NameError:\n",
    "        df.show(len(fqns), truncate=False)\n",
    "\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,                       # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param: str,                 # pipeline CSV | catalog | catalog.schema | table FQN CSV | YAML path\n",
    "        output_format: str,              # \"yaml\" | \"table\"\n",
    "        output_location: str,            # yaml: folder or file; table: catalog.schema.table\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,     # e.g. \".tmp_*\"\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,       # None => whole table (only if mode==\"table\")\n",
    "        run_config_name: str = \"default\",          # DQX run group tag\n",
    "        criticality: str = \"warn\",                 # \"warn\" | \"error\"\n",
    "        key_order: Literal[\"engine\", \"custom\"] = \"custom\",  # \"engine\" uses DQX save; \"custom\" enforces key order\n",
    "        include_table_name: bool = True,           # include table_name in each rule dict\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_location = output_location\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.key_order = key_order\n",
    "        self.include_table_name = include_table_name\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "        if self.output_format not in {\"yaml\", \"table\"}:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table'.\")\n",
    "        if self.output_format == \"yaml\" and not self.output_location:\n",
    "            raise ValueError(\"When output_format='yaml', provide output_location (folder or file).\")\n",
    "\n",
    "    # ---------- profile options: pass via 'options' kwarg, warn on unknowns ----------\n",
    "    def _profile_call_kwargs(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Build kwargs for DQProfiler.profile / profile_table.\n",
    "        We pass:\n",
    "          - cols=self.columns (when provided)\n",
    "          - options=self.profile_options (dict; profiler reads keys internally)\n",
    "        We only WARN on keys not in the current documented set; we do not drop them.\n",
    "        \"\"\"\n",
    "        kwargs: Dict[str, Any] = {}\n",
    "        if self.columns is not None:\n",
    "            kwargs[\"cols\"] = self.columns\n",
    "        if self.profile_options:\n",
    "            unknown = sorted(set(self.profile_options) - DOC_SUPPORTED_KEYS)\n",
    "            if unknown:\n",
    "                print(f\"[INFO] Profiling options not in current docs (passing through anyway): {unknown}\")\n",
    "            kwargs[\"options\"] = self.profile_options\n",
    "        return kwargs\n",
    "\n",
    "    # ---------- discovery ----------\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:             {self.mode}\")\n",
    "        print(f\"name_param:       {self.name_param}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_location:  {self.output_location}\")\n",
    "        print(f\"exclude_pattern:  {self.exclude_pattern}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"key_order:        {self.key_order}\")\n",
    "        print(f\"include_table_name: {self.include_table_name}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if self.mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{self.mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "        if self.columns is not None and self.mode != \"table\":\n",
    "            raise ValueError(\"The 'columns' parameter can only be used in mode='table'.\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "\n",
    "        if self.mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in self.name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                discovered += [x for x in pipeline_tables if x]\n",
    "\n",
    "        elif self.mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = self.name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                discovered += [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if self.name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = self.name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            discovered = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        else:  # table\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            if _is_yaml_path(self.name_param):\n",
    "                print(f\"[INFO] name_param is a YAML list → {self.name_param}\")\n",
    "                discovered = _discover_tables_from_yaml_by_comments(self.name_param)\n",
    "            else:\n",
    "                tables = [t.strip() for t in self.name_param.split(\",\") if t.strip()]\n",
    "                for t in tables:\n",
    "                    if t.count(\".\") != 2:\n",
    "                        raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "                discovered = tables\n",
    "\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "\n",
    "        # Interactive preview instead of plain prints\n",
    "        _display_table_preview(self.spark, discovered, title=\"Final table list to generate DQX rules for\")\n",
    "\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    # ---------- storage config helpers ----------\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def _workspace_files_upload(path: str, payload: bytes) -> None:\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            wc.files.upload(file_path=path, contents=payload, overwrite=True)  # newer SDK\n",
    "        except TypeError:\n",
    "            wc.files.upload(path=path, contents=payload, overwrite=True)       # older SDK\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_parent(path: str) -> None:\n",
    "        \"\"\"Create parent dir for local paths.\"\"\"\n",
    "        parent = os.path.dirname(path)\n",
    "        if parent and not os.path.exists(parent):\n",
    "            os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dbfs_parent(dbutils, path: str) -> None:\n",
    "        parent = path.rsplit(\"/\", 1)[0] if \"/\" in path else path\n",
    "        if parent:\n",
    "            dbutils.fs.mkdirs(parent)\n",
    "\n",
    "    # ---------- DQX check shaping ----------\n",
    "    def _dq_constraint_to_check(\n",
    "        self,\n",
    "        rule_name: str,\n",
    "        constraint_sql: str,\n",
    "        table_name: str,\n",
    "        criticality: str,\n",
    "        run_config_name: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert a profiler constraint (SQL) into a DQX check dict.\n",
    "        Key order: table_name, name, criticality, run_config_name, check (insertion order).\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": criticality,\n",
    "            \"run_config_name\": run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    \"expression\": constraint_sql,\n",
    "                    \"name\": rule_name,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if self.include_table_name:\n",
    "            d = {\"table_name\": table_name, **d}\n",
    "        return d\n",
    "\n",
    "    # ---------- YAML writers ----------\n",
    "    def _write_yaml_ordered(self, checks: List[Dict[str, Any]], path: str) -> None:\n",
    "        \"\"\"\n",
    "        Dump YAML preserving key order and upload:\n",
    "          - /Volumes/... | dbfs:/... | /dbfs/... -> dbutils.fs.put (mkdirs parent)\n",
    "          - /Shared/... (workspace files) -> Files API\n",
    "          - relative/local path -> os.makedirs + open(...)\n",
    "        \"\"\"\n",
    "        yaml_str = yaml.safe_dump(checks, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "        # DBFS / Volumes\n",
    "        if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "            try:\n",
    "                from databricks.sdk.runtime import dbutils\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"dbutils is required to write to DBFS/Volumes.\")\n",
    "            target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "            self._ensure_dbfs_parent(dbutils, target.rsplit(\"/\", 1)[0])\n",
    "            dbutils.fs.put(target, yaml_str, True)\n",
    "            print(f\"[RUN] Wrote ordered YAML to {path}\")\n",
    "            return\n",
    "\n",
    "        # Workspace files\n",
    "        if path.startswith(\"/\"):\n",
    "            self._workspace_files_upload(path, yaml_str.encode(\"utf-8\"))\n",
    "            print(f\"[RUN] Wrote ordered YAML to workspace file: {path}\")\n",
    "            return\n",
    "\n",
    "        # Local (driver) relative/absolute filesystem (Repos-friendly)\n",
    "        full_path = os.path.abspath(path)\n",
    "        self._ensure_parent(full_path)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(yaml_str)\n",
    "        print(f\"[RUN] Wrote ordered YAML to local path: {full_path}\")\n",
    "\n",
    "    # ---------- main ----------\n",
    "    def run(self):\n",
    "        try:\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "            call_kwargs = self._profile_call_kwargs()\n",
    "            print(\"[RUN] Profiler call kwargs:\")\n",
    "            print(f\"  cols:    {call_kwargs.get('cols')}\")\n",
    "            print(f\"  options: {json.dumps(call_kwargs.get('options', {}), indent=2)}\")\n",
    "\n",
    "            dq_engine = DQEngine(WorkspaceClient())\n",
    "            total_checks = 0\n",
    "\n",
    "            for fq_table in tables:\n",
    "                if fq_table.count(\".\") != 2:\n",
    "                    print(f\"[WARN] Skipping invalid table name: {fq_table}\")\n",
    "                    continue\n",
    "                cat, sch, tab = fq_table.split(\".\")\n",
    "\n",
    "                # Verify readability\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table readability: {fq_table}\")\n",
    "                    self.spark.table(fq_table).limit(1).collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    # DataFrame profiling with options and optional cols\n",
    "                    summary_stats, profiles = profiler.profile(df, **call_kwargs)\n",
    "                    # If you prefer table-based API:\n",
    "                    # summary_stats, profiles = profiler.profile_table(table=fq_table, **call_kwargs)\n",
    "\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                checks: List[Dict[str, Any]] = []\n",
    "                for rule_name, constraint in (rules_dict or {}).items():\n",
    "                    checks.append(\n",
    "                        self._dq_constraint_to_check(\n",
    "                            rule_name=rule_name,\n",
    "                            constraint_sql=constraint,\n",
    "                            table_name=fq_table,\n",
    "                            criticality=self.criticality,\n",
    "                            run_config_name=self.run_config_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if not checks:\n",
    "                    print(f\"[INFO] No checks generated for {fq_table}.\")\n",
    "                    continue\n",
    "\n",
    "                # Destination selection\n",
    "                if self.output_format == \"yaml\":            # \"yaml\" | \"table\"\n",
    "                    # Directory -> {table}.yaml ; or exact file path\n",
    "                    if self.output_location.endswith((\".yaml\", \".yml\")):\n",
    "                        path = self.output_location\n",
    "                    else:\n",
    "                        path = f\"{self.output_location.rstrip('/')}/{tab}.yaml\"\n",
    "\n",
    "                    if self.key_order == \"engine\":\n",
    "                        cfg = self._infer_file_storage_config(path)\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks via DQX to: {path}\")\n",
    "                        dq_engine.save_checks(checks, config=cfg)\n",
    "                    else:  # \"custom\"\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks with strict key order to: {path}\")\n",
    "                        self._write_yaml_ordered(checks, path)\n",
    "\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "                else:  # table sink\n",
    "                    cfg = self._table_storage_config(\n",
    "                        table_fqn=self.output_location,\n",
    "                        run_config_name=self.run_config_name,\n",
    "                        mode=\"append\"\n",
    "                    )\n",
    "                    print(f\"[RUN] Appending {len(checks)} checks to table: {self.output_location} (run_config_name={self.run_config_name})\")\n",
    "                    dq_engine.save_checks(checks, config=cfg)\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "            print(f\"[RUN] {'Successfully saved' if total_checks else 'No'} checks. Count: {total_checks}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "\n",
    "# -------------------- Usage examples --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        # Sampling\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": 42,\n",
    "        \"limit\": 1000,\n",
    "        # Outliers\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        # Nulls / empties\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        # Distincts → is_in\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_in_count\": 10,\n",
    "        # Rounding\n",
    "        \"round\": True,\n",
    "        # (Keys not in current docs will still pass through; you’ll see a one-time INFO warning)\n",
    "        # \"include_histograms\": False,\n",
    "        # \"min_length\": None,\n",
    "        # \"max_length\": None,\n",
    "        # \"min_value\": None,\n",
    "        # \"max_value\": None,\n",
    "        # \"profile_types\": None,\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Example A: Single table (unchanged)\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"dq_prd.monitoring.job_run_audit\",   # depends on mode\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"dqx_checks\",                   # yaml dir (local) OR \"/Shared/...\" OR \"dbfs:/...\" OR \"/Volumes/...\"\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",                             # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()\n",
    "    \"\"\"\n",
    "    # Example B: YAML list of tables + header comments (wkdy_* example)\n",
    "    #   Notebook path: src/dqx/00_generate_dqx_checks\n",
    "    #   YAML file path (relative to repo): info/wkdy_table/wkdy_table_info/wkdy_table_list-gold.yaml\n",
    "    #   Output folder:                      info/wkdy_table/wkdy_table_info/wkdy_generated_dqx_checks\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"resources/dqx_checks_generated/audit/audit_table_list-gold.yaml\",\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"resources/dqx_checks_generated/audit/audit_generated_dqx_checks\",  # one YAML per table\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",                             # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b74e96-bc01-4a77-8fab-a61e91462f59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TEMPORARY - Duplicate of above cell, creating to run temporary code"
    }
   },
   "outputs": [],
   "source": [
    "# dqx_rule_generator.py\n",
    "# Full file — adds YAML list support via header-comment parsing + display() preview.\n",
    "# NOTHING REMOVED: all params/options stay. Renamed yaml_key_order -> key_order.\n",
    "# Single-table & CSV FQNs still work. One YAML file per table is written in output_location.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from typing import List, Optional, Dict, Any, Literal, Tuple\n",
    "\n",
    "import yaml\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.labs.dqx.config import (\n",
    "    FileChecksStorageConfig,\n",
    "    WorkspaceFileChecksStorageConfig,\n",
    "    TableChecksStorageConfig,\n",
    "    VolumeFileChecksStorageConfig,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def glob_to_regex(glob_pattern: str) -> str:\n",
    "    if not glob_pattern or not glob_pattern.startswith('.'):\n",
    "        raise ValueError(\"Exclude pattern must start with a dot, e.g. '.tamarack_*'\")\n",
    "    glob = glob_pattern[1:]\n",
    "    regex = re.escape(glob).replace(r'\\*', '.*')\n",
    "    return '^' + regex + '$'\n",
    "\n",
    "\n",
    "# Keys currently shown in DQX docs for profiler \"options\" (kept permissive; we only warn on extras)\n",
    "DOC_SUPPORTED_KEYS = {\n",
    "    \"sample_fraction\",\n",
    "    \"sample_seed\",\n",
    "    \"limit\",\n",
    "    \"remove_outliers\",\n",
    "    \"outlier_columns\",\n",
    "    \"num_sigmas\",\n",
    "    \"max_null_ratio\",\n",
    "    \"trim_strings\",\n",
    "    \"max_empty_ratio\",\n",
    "    \"distinct_ratio\",\n",
    "    \"max_in_count\",\n",
    "    \"round\",\n",
    "}\n",
    "\n",
    "# ---------- YAML file support (header-comment parsing for defaults) ----------\n",
    "_YAML_PATH_RE = re.compile(r\"\\.(ya?ml)$\", re.IGNORECASE)\n",
    "_FROM_INFOSCHEMA = re.compile(r\"FROM\\s+([A-Za-z0-9_]+)\\.information_schema\\.tables\", re.IGNORECASE)\n",
    "_TABLE_SCHEMA_EQ = re.compile(r\"table_schema\\s*=\\s*'([^']+)'\", re.IGNORECASE)\n",
    "\n",
    "def _is_yaml_path(path: str) -> bool:\n",
    "    return bool(_YAML_PATH_RE.search(path))\n",
    "\n",
    "def _resolve_local_like_path(path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try hard to resolve a repo-relative file path from a notebook:\n",
    "    - as-given\n",
    "    - join with cwd\n",
    "    - walk up 6 parents and join\n",
    "    Returns a filesystem path if found, else None.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    cwd = os.getcwd()\n",
    "    base = cwd\n",
    "    for _ in range(6):\n",
    "        cand = os.path.abspath(os.path.join(base, path))\n",
    "        candidates.append(cand)\n",
    "        if os.path.exists(cand):\n",
    "            return cand\n",
    "        parent = os.path.dirname(base)\n",
    "        if parent == base:\n",
    "            break\n",
    "        base = parent\n",
    "    return None\n",
    "\n",
    "def _read_text_any(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read small text files from:\n",
    "      - repo/local filesystem (relative or absolute)\n",
    "      - dbfs:/, /dbfs/, /Volumes/\n",
    "      - workspace files path (starts with '/'), via Files API\n",
    "    \"\"\"\n",
    "    # DBFS / Volumes\n",
    "    if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "        try:\n",
    "            from databricks.sdk.runtime import dbutils\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"dbutils is required to read from DBFS/Volumes\") from e\n",
    "        target = path if path.startswith(\"dbfs:\") else (f\"dbfs:{path}\" if path.startswith(\"/\") else f\"dbfs:/{path}\")\n",
    "        return dbutils.fs.head(target, 10 * 1024 * 1024)\n",
    "\n",
    "    # Absolute workspace file path (e.g. /Workspace/Repos/.../file.yaml)\n",
    "    if path.startswith(\"/\"):\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            # newer SDK signature\n",
    "            data = wc.files.download(file_path=path).read()\n",
    "        except TypeError:\n",
    "            # older SDK signature\n",
    "            data = wc.files.download(path=path).read()\n",
    "        return data.decode(\"utf-8\")\n",
    "\n",
    "    # Relative/local resolution (repo-friendly)\n",
    "    resolved = _resolve_local_like_path(path)\n",
    "    if resolved and os.path.isfile(resolved):\n",
    "        with open(resolved, \"r\", encoding=\"utf-8\") as fh:\n",
    "            return fh.read()\n",
    "\n",
    "    # Nothing worked → helpful error\n",
    "    msg = [\n",
    "        f\"Could not find YAML at '{path}'.\",\n",
    "        f\"cwd={os.getcwd()}\",\n",
    "        \"Hints:\",\n",
    "        \"  - If this file is in your repo, pass a path relative to the notebook or the repo root.\",\n",
    "        \"  - Or pass an absolute workspace path like '/Workspace/Repos/.../file.yaml'.\",\n",
    "        \"  - Or use a DBFS/Volumes path like 'dbfs:/...' or '/Volumes/...'.\",\n",
    "    ]\n",
    "    raise FileNotFoundError(\"\\n\".join(msg))\n",
    "\n",
    "def _parse_global_hints_from_comments(text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Pull defaults from header comments only (no YAML keys):\n",
    "      - catalog from: 'FROM <catalog>.information_schema.tables'\n",
    "      - schema  from: \"table_schema = '<schema>'\"\n",
    "    \"\"\"\n",
    "    m_cat = _FROM_INFOSCHEMA.search(text)\n",
    "    m_sch = _TABLE_SCHEMA_EQ.search(text)\n",
    "    cat = m_cat.group(1) if m_cat else None\n",
    "    sch = m_sch.group(1) if m_sch else None\n",
    "    return cat, sch\n",
    "\n",
    "def _ensure_fqns(names: List[str], hint_catalog: Optional[str], hint_schema: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Accept: catalog.schema.table | schema.table | table\n",
    "    Use comment-derived defaults to fill missing parts. Dotted entries override per item.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    for n in names:\n",
    "        n = n.strip()\n",
    "        if not n:\n",
    "            continue\n",
    "        parts = n.split(\".\")\n",
    "        if len(parts) == 3:\n",
    "            out.append(n)\n",
    "        elif len(parts) == 2:\n",
    "            if not hint_catalog:\n",
    "                raise ValueError(f\"'{n}' lacks catalog; add it to the item or provide a comment default.\")\n",
    "            out.append(f\"{hint_catalog}.{n}\")\n",
    "        elif len(parts) == 1:\n",
    "            if not (hint_catalog and hint_schema):\n",
    "                raise ValueError(f\"'{n}' needs catalog & schema; set via comments or use dotted forms.\")\n",
    "            out.append(f\"{hint_catalog}.{hint_schema}.{n}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized table format: {n}\")\n",
    "    bad = [t for t in out if t.count(\".\") != 2]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Invalid FQN(s) after resolution: {bad}\")\n",
    "    return sorted(set(out))\n",
    "\n",
    "def _discover_tables_from_yaml_by_comments(yaml_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load the YAML file (with e.g. 'table_name:' list) and use ONLY header comments\n",
    "    to infer catalog/schema defaults. Dotted items override per entry.\n",
    "    \"\"\"\n",
    "    text = _read_text_any(yaml_path)\n",
    "    cat_hint, sch_hint = _parse_global_hints_from_comments(text)\n",
    "\n",
    "    obj = yaml.safe_load(io.StringIO(text))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(f\"YAML must contain a mapping with a list; got: {type(obj).__name__}\")\n",
    "\n",
    "    names = None\n",
    "    for key in (\"table_name\", \"tables\", \"table_names\", \"list\"):\n",
    "        if isinstance(obj.get(key), list):\n",
    "            names = [str(x).strip() for x in obj[key] if x]\n",
    "            break\n",
    "    if not names:\n",
    "        raise ValueError(f\"No table list found in YAML: {yaml_path}\")\n",
    "\n",
    "    fqns = _ensure_fqns(names, cat_hint, sch_hint)\n",
    "    print(f\"[INFO] Parsed defaults from comments: catalog={cat_hint} schema={sch_hint}\")\n",
    "    return fqns\n",
    "\n",
    "def _display_table_preview(spark: SparkSession, fqns: List[str], title: str = \"Resolved Tables\") -> None:\n",
    "    \"\"\"\n",
    "    Use display() (Databricks) for a clean, interactive preview instead of printing.\n",
    "    \"\"\"\n",
    "    rows = [(f, *f.split(\".\")) for f in fqns]\n",
    "    df = spark.createDataFrame(rows, \"fqn string, catalog string, schema string, table string\")\n",
    "    print(f\"\\n=== {title} ({len(fqns)}) ===\")\n",
    "    try:\n",
    "        display(df)\n",
    "    except NameError:\n",
    "        df.show(len(fqns), truncate=False)\n",
    "\n",
    "\n",
    "class RuleGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,                       # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param: str,                 # pipeline CSV | catalog | catalog.schema | table FQN CSV | YAML path\n",
    "        output_format: str,              # \"yaml\" | \"table\"\n",
    "        output_location: str,            # yaml: folder or file; table: catalog.schema.table\n",
    "        profile_options: Dict[str, Any],\n",
    "        exclude_pattern: Optional[str] = None,     # e.g. \".tmp_*\"\n",
    "        created_by: Optional[str] = \"LMG\",\n",
    "        columns: Optional[List[str]] = None,       # None => whole table (only if mode==\"table\")\n",
    "        run_config_name: str = \"default\",          # DQX run group tag\n",
    "        criticality: str = \"warn\",                 # \"warn\" | \"error\"\n",
    "        key_order: Literal[\"engine\", \"custom\"] = \"custom\",  # \"engine\" uses DQX save; \"custom\" enforces key order\n",
    "        include_table_name: bool = True,           # include table_name in each rule dict\n",
    "    ):\n",
    "        self.mode = mode.lower().strip()\n",
    "        self.name_param = name_param\n",
    "        self.output_format = output_format.lower().strip()\n",
    "        self.output_location = output_location\n",
    "        self.profile_options = profile_options or {}\n",
    "        self.exclude_pattern = exclude_pattern\n",
    "        self.created_by = created_by\n",
    "        self.columns = columns\n",
    "        self.run_config_name = run_config_name\n",
    "        self.criticality = criticality\n",
    "        self.key_order = key_order\n",
    "        self.include_table_name = include_table_name\n",
    "\n",
    "        self.spark = SparkSession.getActiveSession()\n",
    "        if not self.spark:\n",
    "            raise RuntimeError(\"No active Spark session found. Run this in a Databricks notebook.\")\n",
    "\n",
    "        if self.output_format not in {\"yaml\", \"table\"}:\n",
    "            raise ValueError(\"output_format must be 'yaml' or 'table'.\")\n",
    "        if self.output_format == \"yaml\" and not self.output_location:\n",
    "            raise ValueError(\"When output_format='yaml', provide output_location (folder or file).\")\n",
    "\n",
    "    # ---------- profile options: pass via 'options' kwarg, warn on unknowns ----------\n",
    "    def _profile_call_kwargs(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Build kwargs for DQProfiler.profile / profile_table.\n",
    "        We pass:\n",
    "          - cols=self.columns (when provided)\n",
    "          - options=self.profile_options (dict; profiler reads keys internally)\n",
    "        We only WARN on keys not in the current documented set; we do not drop them.\n",
    "        \"\"\"\n",
    "        kwargs: Dict[str, Any] = {}\n",
    "        if self.columns is not None:\n",
    "            kwargs[\"cols\"] = self.columns\n",
    "        if self.profile_options:\n",
    "            unknown = sorted(set(self.profile_options) - DOC_SUPPORTED_KEYS)\n",
    "            if unknown:\n",
    "                print(f\"[INFO] Profiling options not in current docs (passing through anyway): {unknown}\")\n",
    "            kwargs[\"options\"] = self.profile_options\n",
    "        return kwargs\n",
    "\n",
    "    # ---------- discovery ----------\n",
    "    def _exclude_tables_by_pattern(self, fq_tables: List[str]) -> List[str]:\n",
    "        if not self.exclude_pattern:\n",
    "            return fq_tables\n",
    "        regex = glob_to_regex(self.exclude_pattern)\n",
    "        pattern = re.compile(regex)\n",
    "        filtered = []\n",
    "        for fq in fq_tables:\n",
    "            tbl = fq.split('.')[-1]\n",
    "            if not pattern.match(tbl):\n",
    "                filtered.append(fq)\n",
    "        print(f\"[INFO] Excluded {len(fq_tables) - len(filtered)} tables by pattern '{self.exclude_pattern}'\")\n",
    "        return filtered\n",
    "\n",
    "    def _discover_tables(self) -> List[str]:\n",
    "        print(\"\\n===== PARAMETERS PASSED THIS RUN =====\")\n",
    "        print(f\"mode:             {self.mode}\")\n",
    "        print(f\"name_param:       {self.name_param}\")\n",
    "        print(f\"output_format:    {self.output_format}\")\n",
    "        print(f\"output_location:  {self.output_location}\")\n",
    "        print(f\"exclude_pattern:  {self.exclude_pattern}\")\n",
    "        print(f\"created_by:       {self.created_by}\")\n",
    "        print(f\"columns:          {self.columns}\")\n",
    "        print(f\"run_config_name:  {self.run_config_name}\")\n",
    "        print(f\"criticality:      {self.criticality}\")\n",
    "        print(f\"key_order:        {self.key_order}\")\n",
    "        print(f\"include_table_name: {self.include_table_name}\")\n",
    "        print(f\"profile_options:\")\n",
    "        for k, v in self.profile_options.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(\"======================================\\n\")\n",
    "\n",
    "        allowed_modes = {\"pipeline\", \"catalog\", \"schema\", \"table\"}\n",
    "        if self.mode not in allowed_modes:\n",
    "            raise ValueError(f\"Invalid mode '{self.mode}'. Must be one of: {sorted(allowed_modes)}.\")\n",
    "        if self.columns is not None and self.mode != \"table\":\n",
    "            raise ValueError(\"The 'columns' parameter can only be used in mode='table'.\")\n",
    "\n",
    "        discovered: List[str] = []\n",
    "\n",
    "        if self.mode == \"pipeline\":\n",
    "            print(\"Searching for pipeline output tables...\")\n",
    "            ws = WorkspaceClient()\n",
    "            pipelines = [p.strip() for p in self.name_param.split(\",\") if p.strip()]\n",
    "            print(f\"Pipelines passed: {pipelines}\")\n",
    "            for pipeline_name in pipelines:\n",
    "                print(f\"Finding output tables for pipeline: {pipeline_name}\")\n",
    "                pls = list(ws.pipelines.list_pipelines())\n",
    "                pl = next((p for p in pls if p.name == pipeline_name), None)\n",
    "                if not pl:\n",
    "                    raise RuntimeError(f\"Pipeline '{pipeline_name}' not found via SDK.\")\n",
    "                latest_update = pl.latest_updates[0].update_id\n",
    "                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)\n",
    "                pipeline_tables = [\n",
    "                    getattr(ev.origin, \"flow_name\", None)\n",
    "                    for ev in events\n",
    "                    if getattr(ev.origin, \"update_id\", None) == latest_update and getattr(ev.origin, \"flow_name\", None)\n",
    "                ]\n",
    "                discovered += [x for x in pipeline_tables if x]\n",
    "\n",
    "        elif self.mode == \"catalog\":\n",
    "            print(\"Searching for tables in catalog...\")\n",
    "            catalog = self.name_param.strip()\n",
    "            schemas = [row.namespace for row in self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "            for s in schemas:\n",
    "                tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{s}\").collect()\n",
    "                discovered += [f\"{catalog}.{s}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        elif self.mode == \"schema\":\n",
    "            print(\"Searching for tables in schema...\")\n",
    "            if self.name_param.count(\".\") != 1:\n",
    "                raise ValueError(\"For 'schema' mode, name_param must be catalog.schema\")\n",
    "            catalog, schema = self.name_param.strip().split(\".\")\n",
    "            tbls = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            discovered = [f\"{catalog}.{schema}.{row.tableName}\" for row in tbls]\n",
    "\n",
    "        else:  # table\n",
    "            print(\"Profiling one or more specific tables...\")\n",
    "            if _is_yaml_path(self.name_param):\n",
    "                print(f\"[INFO] name_param is a YAML list → {self.name_param}\")\n",
    "                discovered = _discover_tables_from_yaml_by_comments(self.name_param)\n",
    "            else:\n",
    "                tables = [t.strip() for t in self.name_param.split(\",\") if t.strip()]\n",
    "                for t in tables:\n",
    "                    if t.count(\".\") != 2:\n",
    "                        raise ValueError(f\"Table name '{t}' must be fully qualified (catalog.schema.table)\")\n",
    "                discovered = tables\n",
    "\n",
    "        print(\"\\nRunning exclude pattern filtering (if any)...\")\n",
    "        discovered = self._exclude_tables_by_pattern(discovered)\n",
    "\n",
    "        # Interactive preview instead of plain prints\n",
    "        _display_table_preview(self.spark, discovered, title=\"Final table list to generate DQX rules for\")\n",
    "\n",
    "        print(\"==========================================\\n\")\n",
    "        return sorted(set(discovered))\n",
    "\n",
    "    # ---------- storage config helpers ----------\n",
    "    @staticmethod\n",
    "    def _infer_file_storage_config(file_path: str):\n",
    "        if file_path.startswith(\"/Volumes/\"):\n",
    "            return VolumeFileChecksStorageConfig(location=file_path)\n",
    "        if file_path.startswith(\"/\"):\n",
    "            return WorkspaceFileChecksStorageConfig(location=file_path)\n",
    "        return FileChecksStorageConfig(location=file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _table_storage_config(table_fqn: str, run_config_name: Optional[str] = None, mode: str = \"append\"):\n",
    "        return TableChecksStorageConfig(location=table_fqn, run_config_name=run_config_name, mode=mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def _workspace_files_upload(path: str, payload: bytes) -> None:\n",
    "        wc = WorkspaceClient()\n",
    "        try:\n",
    "            wc.files.upload(file_path=path, contents=payload, overwrite=True)  # newer SDK\n",
    "        except TypeError:\n",
    "            wc.files.upload(path=path, contents=payload, overwrite=True)       # older SDK\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_parent(path: str) -> None:\n",
    "        \"\"\"Create parent dir for local paths.\"\"\"\n",
    "        parent = os.path.dirname(path)\n",
    "        if parent and not os.path.exists(parent):\n",
    "            os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dbfs_parent(dbutils, path: str) -> None:\n",
    "        parent = path.rsplit(\"/\", 1)[0] if \"/\" in path else path\n",
    "        if parent:\n",
    "            dbutils.fs.mkdirs(parent)\n",
    "\n",
    "    # ---------- DQX check shaping ----------\n",
    "    def _dq_constraint_to_check(\n",
    "        self,\n",
    "        rule_name: str,\n",
    "        constraint_sql: str,\n",
    "        table_name: str,\n",
    "        criticality: str,\n",
    "        run_config_name: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert a profiler constraint (SQL) into a DQX check dict.\n",
    "        Key order: table_name, name, criticality, run_config_name, check (insertion order).\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"name\": rule_name,\n",
    "            \"criticality\": criticality,\n",
    "            \"run_config_name\": run_config_name,\n",
    "            \"check\": {\n",
    "                \"function\": \"sql_expression\",\n",
    "                \"arguments\": {\n",
    "                    \"expression\": constraint_sql,\n",
    "                    \"name\": rule_name,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if self.include_table_name:\n",
    "            d = {\"table_name\": table_name, **d}\n",
    "        return d\n",
    "\n",
    "    # ---------- YAML writers ----------\n",
    "    def _write_yaml_ordered(self, checks: List[Dict[str, Any]], path: str) -> None:\n",
    "        \"\"\"\n",
    "        Dump YAML preserving key order and upload:\n",
    "          - /Volumes/... | dbfs:/... | /dbfs/... -> dbutils.fs.put (mkdirs parent)\n",
    "          - /Shared/... (workspace files) -> Files API\n",
    "          - relative/local path -> os.makedirs + open(...)\n",
    "        \"\"\"\n",
    "        yaml_str = yaml.safe_dump(checks, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "        # DBFS / Volumes\n",
    "        if path.startswith(\"dbfs:/\") or path.startswith(\"/dbfs/\") or path.startswith(\"/Volumes/\"):\n",
    "            try:\n",
    "                from databricks.sdk.runtime import dbutils\n",
    "            except Exception:\n",
    "                raise RuntimeError(\"dbutils is required to write to DBFS/Volumes.\")\n",
    "            target = path if path.startswith(\"dbfs:/\") else (f\"dbfs:{path}\" if not path.startswith(\"dbfs:\") else path)\n",
    "            self._ensure_dbfs_parent(dbutils, target.rsplit(\"/\", 1)[0])\n",
    "            dbutils.fs.put(target, yaml_str, True)\n",
    "            print(f\"[RUN] Wrote ordered YAML to {path}\")\n",
    "            return\n",
    "\n",
    "        # Workspace files\n",
    "        if path.startswith(\"/\"):\n",
    "            self._workspace_files_upload(path, yaml_str.encode(\"utf-8\"))\n",
    "            print(f\"[RUN] Wrote ordered YAML to workspace file: {path}\")\n",
    "            return\n",
    "\n",
    "        # Local (driver) relative/absolute filesystem (Repos-friendly)\n",
    "        full_path = os.path.abspath(path)\n",
    "        self._ensure_parent(full_path)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(yaml_str)\n",
    "        print(f\"[RUN] Wrote ordered YAML to local path: {full_path}\")\n",
    "\n",
    "    # ---------- main ----------\n",
    "    def run(self):\n",
    "        try:\n",
    "            tables = self._discover_tables()\n",
    "            print(\"[RUN] Beginning DQX rule generation on these tables:\")\n",
    "            for t in tables:\n",
    "                print(f\"  {t}\")\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "            call_kwargs = self._profile_call_kwargs()\n",
    "            print(\"[RUN] Profiler call kwargs:\")\n",
    "            print(f\"  cols:    {call_kwargs.get('cols')}\")\n",
    "            print(f\"  options: {json.dumps(call_kwargs.get('options', {}), indent=2)}\")\n",
    "\n",
    "            dq_engine = DQEngine(WorkspaceClient())\n",
    "            total_checks = 0\n",
    "\n",
    "            for fq_table in tables:\n",
    "                if fq_table.count(\".\") != 2:\n",
    "                    print(f\"[WARN] Skipping invalid table name: {fq_table}\")\n",
    "                    continue\n",
    "                cat, sch, tab = fq_table.split(\".\")\n",
    "\n",
    "                # Verify readability\n",
    "                try:\n",
    "                    print(f\"[RUN] Checking table readability: {fq_table}\")\n",
    "                    self.spark.table(fq_table).limit(1).collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Table {fq_table} not readable in Spark: {e}\")\n",
    "                    continue\n",
    "\n",
    "                profiler = DQProfiler(WorkspaceClient())\n",
    "                generator = DQDltGenerator(WorkspaceClient())\n",
    "                df = self.spark.table(fq_table)\n",
    "\n",
    "                try:\n",
    "                    print(f\"[RUN] Profiling and generating rules for: {fq_table}\")\n",
    "                    # DataFrame profiling with options and optional cols\n",
    "                    summary_stats, profiles = profiler.profile(df, **call_kwargs)\n",
    "                    # If you prefer table-based API:\n",
    "                    # summary_stats, profiles = profiler.profile_table(table=fq_table, **call_kwargs)\n",
    "\n",
    "                    rules_dict = generator.generate_dlt_rules(profiles, language=\"Python_Dict\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Profiling failed for {fq_table}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                checks: List[Dict[str, Any]] = []\n",
    "                for rule_name, constraint in (rules_dict or {}).items():\n",
    "                    checks.append(\n",
    "                        self._dq_constraint_to_check(\n",
    "                            rule_name=rule_name,\n",
    "                            constraint_sql=constraint,\n",
    "                            table_name=fq_table,\n",
    "                            criticality=self.criticality,\n",
    "                            run_config_name=self.run_config_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if not checks:\n",
    "                    print(f\"[INFO] No checks generated for {fq_table}.\")\n",
    "                    continue\n",
    "\n",
    "                # Destination selection\n",
    "                if self.output_format == \"yaml\":            # \"yaml\" | \"table\"\n",
    "                    # Directory -> {table}.yaml ; or exact file path\n",
    "                    if self.output_location.endswith((\".yaml\", \".yml\")):\n",
    "                        path = self.output_location\n",
    "                    else:\n",
    "                        path = f\"{self.output_location.rstrip('/')}/{tab}.yaml\"\n",
    "\n",
    "                    if self.key_order == \"engine\":\n",
    "                        cfg = self._infer_file_storage_config(path)\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks via DQX to: {path}\")\n",
    "                        dq_engine.save_checks(checks, config=cfg)\n",
    "                    else:  # \"custom\"\n",
    "                        print(f\"[RUN] Saving {len(checks)} checks with strict key order to: {path}\")\n",
    "                        self._write_yaml_ordered(checks, path)\n",
    "\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "                else:  # table sink\n",
    "                    cfg = self._table_storage_config(\n",
    "                        table_fqn=self.output_location,\n",
    "                        run_config_name=self.run_config_name,\n",
    "                        mode=\"append\"\n",
    "                    )\n",
    "                    print(f\"[RUN] Appending {len(checks)} checks to table: {self.output_location} (run_config_name={self.run_config_name})\")\n",
    "                    dq_engine.save_checks(checks, config=cfg)\n",
    "                    total_checks += len(checks)\n",
    "\n",
    "            print(f\"[RUN] {'Successfully saved' if total_checks else 'No'} checks. Count: {total_checks}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rule generation failed: {e}\")\n",
    "\n",
    "\n",
    "# -------------------- Usage examples --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    profile_options = {\n",
    "        # Sampling\n",
    "        \"sample_fraction\": 0.3,\n",
    "        \"sample_seed\": 42,\n",
    "        \"limit\": 1000,\n",
    "        # Outliers\n",
    "        \"remove_outliers\": True,\n",
    "        \"outlier_columns\": [],\n",
    "        \"num_sigmas\": 3,\n",
    "        # Nulls / empties\n",
    "        \"max_null_ratio\": 0.01,\n",
    "        \"trim_strings\": True,\n",
    "        \"max_empty_ratio\": 0.01,\n",
    "        # Distincts → is_in\n",
    "        \"distinct_ratio\": 0.05,\n",
    "        \"max_in_count\": 10,\n",
    "        # Rounding\n",
    "        \"round\": True,\n",
    "        # (Keys not in current docs will still pass through; you’ll see a one-time INFO warning)\n",
    "        # \"include_histograms\": False,\n",
    "        # \"min_length\": None,\n",
    "        # \"max_length\": None,\n",
    "        # \"min_value\": None,\n",
    "        # \"max_value\": None,\n",
    "        # \"profile_types\": None,\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Example A: Single table (unchanged)\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"dq_prd.monitoring.job_run_audit\",   # depends on mode\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"dqx_checks\",                   # yaml dir (local) OR \"/Shared/...\" OR \"dbfs:/...\" OR \"/Volumes/...\"\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",                             # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()\n",
    "    \"\"\"\n",
    "    # Example B: YAML list of tables + header comments (wkdy_* example)\n",
    "    #   Notebook path: src/dqx/00_generate_dqx_checks\n",
    "    #   YAML file path (relative to repo): info/wkdy_table/wkdy_table_info/wkdy_table_list-gold.yaml\n",
    "    #   Output folder:                      info/wkdy_table/wkdy_table_info/wkdy_generated_dqx_checks\n",
    "    RuleGenerator(\n",
    "        mode=\"table\",                                   # \"pipeline\" | \"catalog\" | \"schema\" | \"table\"\n",
    "        name_param=\"resources/dqx_checks_generated/crm/crm_table_list-gold.yaml\",\n",
    "        output_format=\"yaml\",                           # \"yaml\" | \"table\"\n",
    "        output_location=\"resources/dqx_checks_generated/crm/crm_generated_dqx_checks\",  # one YAML per table\n",
    "        profile_options=profile_options,\n",
    "        columns=None,                                   # None => whole table (only valid when mode==\"table\")\n",
    "        exclude_pattern=None,                           # e.g. \".tmp_*\"\n",
    "        created_by=\"LMG\",\n",
    "        run_config_name=\"default\",\n",
    "        criticality=\"error\",\n",
    "        key_order=\"custom\",                             # \"engine\" or \"custom\"\n",
    "        include_table_name=True,\n",
    "    ).run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_generate_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
