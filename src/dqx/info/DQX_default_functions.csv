
dqx/src/databricks/labs/dqx/check_funcs.py at v0.8.0 · databrickslabs/dqx				
Reference	Level	Check	Description	Arguments
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_null	Checks whether the values in the input column are not null.	column: column to check (can be a string column name or a column expression)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_empty	Checks whether the values in the input column are not empty (but may be null).	column: column to check (can be a string column name or a column expression)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_null_and_not_empty	Checks whether the values in the input column are not null and not empty.	column: column to check (can be a string column name or a column expression); trim_strings: optional boolean flag to trim spaces from strings
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_in_list	Checks whether the values in the input column are present in the list of allowed values (null values are allowed). This check is not suited for large lists of allowed values. In such cases, it’s recommended to use the foreign_key dataset-level check instead.	column: column to check (can be a string column name or a column expression); allowed: list of allowed values
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_null_and_is_in_list	Checks whether the values in the input column are not null and present in the list of allowed values. This check is not suited for large lists of allowed values. In such cases, it’s recommended to use the foreign_key dataset-level check instead.	column: column to check (can be a string column name or a column expression); allowed: list of allowed values
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_null_and_not_empty_array	Checks whether the values in the array input column are not null and not empty.	column: column to check (can be a string column name or a column expression)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_in_range	Checks whether the values in the input column are in the provided range (inclusive of both boundaries).	column: column to check (can be a string column name or a column expression); min_limit: min limit as number, date, timestamp, column name or sql expression; max_limit: max limit as number, date, timestamp, column name or sql expression
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_in_range	Checks whether the values in the input column are outside the provided range (inclusive of both boundaries).	column: column to check (can be a string column name or a column expression); min_limit: min limit as number, date, timestamp, column name or sql expression; max_limit: max limit as number, date, timestamp, column name or sql expression
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_less_than	Checks whether the values in the input column are not less than the provided limit.	column: column to check (can be a string column name or a column expression); limit: limit as number, date, timestamp, column name or sql expression
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_greater_than	Checks whether the values in the input column are not greater than the provided limit.	column: column to check (can be a string column name or a column expression); limit: limit as number, date, timestamp, column name or sql expression
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_valid_date	Checks whether the values in the input column have valid date formats.	column: column to check (can be a string column name or a column expression); date_format: optional date format (e.g. 'yyyy-mm-dd')
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_valid_timestamp	Checks whether the values in the input column have valid timestamp formats.	column: column to check (can be a string column name or a column expression); timestamp_format: optional timestamp format (e.g. 'yyyy-mm-dd HH:mm:ss')
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_in_future	Checks whether the values in the input column contain a timestamp that is not in the future, where 'future' is defined as current_timestamp + offset (in seconds).	column: column to check (can be a string column name or a column expression); offset: offset to use; curr_timestamp: current timestamp, if not provided current_timestamp() function is used
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_not_in_near_future	Checks whether the values in the input column contain a timestamp that is not in the near future, where 'near future' is defined as greater than the current timestamp but less than the current_timestamp + offset (in seconds).	column: column to check (can be a string column name or a column expression); offset: offset to use; curr_timestamp: current timestamp, if not provided current_timestamp() function is used
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_older_than_n_days	Checks whether the values in one input column are at least N days older than the values in another column.	column: column to check (can be a string column name or a column expression); days: number of days; curr_date: current date, if not provided current_date() function is used; negate: if the condition should be negated
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_older_than_col2_for_n_days	Checks whether the values in one input column are at least N days older than the values in another column.	column1: first column to check (can be a string column name or a column expression); column2: second column to check (can be a string column name or a column expression); days: number of days; negate: if the condition should be negated
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	regex_match	Checks whether the values in the input column match a given regex.	column: column to check (can be a string column name or a column expression); regex: regex to check; negate: if the condition should be negated (true) or not
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_valid_ipv4_address	Checks whether the values in the input column have valid IPv4 address format.	column to check (can be a string column name or a column expression)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_ipv4_address_in_cidr	Checks whether the values in the input column have valid IPv4 address format and fall within the given CIDR block.	column: column to check (can be a string column name or a column expression); cidr_block: CIDR block string
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	sql_expression	Checks whether the values meet the condition provided as an SQL expression, e.g. a = 'str1' and a > b. SQL expressions are evaluated at runtime, so ensure that the expression is safe and that functions used within it (e.g. h3_ischildof, division) do not throw exceptions. You can achieve this by validating input arguments or columns beforehand using guards such as CASE WHEN, IS NOT NULL, RLIKE, or type try casts.	expression: sql expression to check on a DataFrame (fail the check if expression evaluates to True, pass if it evaluates to False); msg: optional message to output; name: optional name of the resulting column (it can be overwritten by name specified at the check level); negate: if the condition should be negated; columns: optional list of columns to be used for reporting and as name prefix if name not provided, unused in the actual logic
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#usage-examples-of-row-level-checks	row	is_data_fresh	Checks whether the values in the input timestamp column are not older than the specified number of minutes from the base timestamp column. This is useful for identifying stale data due to delayed pipelines and helps catch upstream issues early.	column: column of type timestamp/date to check (can be a string column name or a column expression); max_age_minutes: maximum age in minutes before data is considered stale; base_timestamp: optional base timestamp column from which the stale check is calculated. This can be a string, column expression, datetime value or literal value ex:F.lit(datetime(2024,1,1)). If not provided current_timestamp() function is used
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	is_unique	Checks whether the values in the input column are unique and reports an issue for each row that contains a duplicate value. It supports uniqueness check for multiple columns (composite key). Null values are not considered duplicates by default, following the ANSI SQL standard.	columns: columns to check (can be a list of column names or column expressions); nulls_distinct: controls how null values are treated, default is True, thus nulls are not duplicates, eg. (NULL, NULL) not equals (NULL, NULL) and (1, NULL) not equals (1, NULL)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	is_aggr_not_greater_than	Checks whether the aggregated values over group of rows or all rows are not greater than the provided limit.	column: column to check (can be a string column name or a column expression), optional for 'count' aggregation; limit: limit as number, column name or sql expression; aggr_type: aggregation function to use, such as "count" (default), "sum", "avg", "min", and "max"; group_by: (optional) list of columns or column expressions to group the rows for aggregation (no grouping by default)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	is_aggr_not_less_than	Checks whether the aggregated values over group of rows or all rows are not less than the provided limit.	column: column to check (can be a string column name or a column expression), optional for 'count' aggregation; limit: limit as number, column name or sql expression; aggr_type: aggregation function to use, such as "count" (default), "sum", "avg", "min", and "max"; group_by: (optional) list of columns or column expressions to group the rows for aggregation (no grouping by default)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	is_aggr_equal	Checks whether the aggregated values over group of rows or all rows are equal to the provided limit.	column: column to check (can be a string column name or a column expression), optional for 'count' aggregation; limit: limit as number, column name or sql expression; aggr_type: aggregation function to use, such as "count" (default), "sum", "avg", "min", and "max"; group_by: (optional) list of columns or column expressions to group the rows for aggregation (no grouping by default)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	is_aggr_not_equal	Checks whether the aggregated values over group of rows or all rows are not equal to the provided limit.	column: column to check (can be a string column name or a column expression), optional for 'count' aggregation; limit: limit as number, column name or sql expression; aggr_type: aggregation function to use, such as "count" (default), "sum", "avg", "min", and "max"; group_by: (optional) list of columns or column expressions to group the rows for aggregation (no grouping by default)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	foreign_key (aka is_in_list)	Checks whether input column or columns can be found in the reference DataFrame or Table (foreign key check). It supports foreign key check on single and composite keys. This check can be used to validate whether values in the input column(s) exist in a predefined list of allowed values (stored in the reference DataFrame or Table). It serves as a scalable alternative to is_in_list row-level checks, when working with large lists.	columns: columns to check (can be a list of string column names or column expressions); ref_columns: columns to check for existence in the reference DataFrame or Table (can be a list string column name or a column expression); ref_df_name: (optional) name of the reference DataFrame (dictionary of DataFrames can be passed when applying checks); ref_table: (optional) fully qualified reference table name; either ref_df_name or ref_table must be provided but never both; the number of passed columns and ref_columns must match and keys are checks in the given order; negate: if True the condition is negated (i.e. the check fails when the foreign key values exist in the reference DataFrame/Table), if False the check fails when the foreign key values do not exist in the reference
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	sql_query	Checks whether the condition column produced by a SQL query is satisfied. The check expects the query to return a boolean condition column indicating whether a record meets the requirement (True = fail, False = pass), and one or more merge columns so that results can be joined back to the input DataFrame to preserve all original records. Important considerations: if merge columns aren't unique, multiple query rows can attach to a single input row, potentially causing false positives. Performance tip: since the check must join back to the input DataFrame to retain original records, writing a custom dataset-level rule is usually more performant than sql_query check.	query: query string, must return all merge columns and condition column; input_placeholder: name to be used in the sql query as {{ input_placeholder }} to refer to the input DataFrame, optional reference DataFrames are referred by the name provided in the dictionary of reference DataFrames (e.g. {{ ref_view }}, dictionary of DataFrames can be passed when applying checks); merge_columns: list of columns used for merging with the input DataFrame which must exist in the input DataFrame and be present in output of the sql query; condition_column: name of the column indicating a violation (False = pass, True = fail); msg: (optional) message to output; name: (optional) name of the resulting check (it can be overwritten by name specified at the check level); negate: if the condition should be negated
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	compare_datasets	Compares two DataFrames at both row and column levels, providing detailed information about differences, including new or missing rows and column-level changes. Only columns present in both the source and reference DataFrames are compared. Use with caution if check_missing_records is enabled, as this may increase the number of rows in the output beyond the original input DataFrame. The comparison does not support Map types (any column comparison on map type is skipped automatically). Comparing datasets is valuable for validating data during migrations, detecting drift, performing regression testing, or verifying synchronization between source and target systems.	columns: columns to use for row matching with the reference DataFrame (can be a list of string column names or column expressions, but only simple column expressions are allowed such as 'F.col("col1")'), if not having primary keys or wanting to match against all columns you can pass 'df.columns'; ref_columns: list of columns in the reference DataFrame or Table to row match against the source DataFrame (can be a list of string column names or column expressions, but only simple column expressions are allowed such as 'F.col("col1")'), if not having primary keys or wanting to match against all columns you can pass 'ref_df.columns'; note that columns are matched with ref_columns by position, so the order of the provided columns in both lists must be exactly aligned; exclude_columns: (optional) list of columns to exclude from the value comparison but not from row matching (can be a list of string column names or column expressions, but only simple column expressions are allowed such as 'F.col("col1")'); the exclude_columns field does not alter the list of columns used to determine row matches (columns), it only controls which columns are skipped during the value comparison; ref_df_name: (optional) name of the reference DataFrame (dictionary of DataFrames can be passed when applying checks); ref_table: (optional) fully qualified reference table name; either ref_df_name or ref_table must be provided but never both; the number of passed columns and ref_columns must match and keys are checks in the given order; check_missing_records: perform a FULL OUTER JOIN to identify records that are missing from source or reference DataFrames, default is False; use with caution as this may increase the number of rows in the output, as unmatched rows from both sides are included; null_safe_row_matching: (optional) treat NULLs as equal when matching rows using columns and ref_columns (default: True); null_safe_column_value_matching: (optional) treat NULLs as equal when comparing column values (default: True)
https://databrickslabs.github.io/dqx/docs/reference/quality_rules/#dataset-level-checks-reference	dataset	is_data_fresh_per_time_window	Freshness check that validates whether at least X records arrive within every Y-minute time window.	column: timestamp column (can be a string column name or a column expression); window_minutes: time window in minutes to check for data arrival; min_records_per_window: minimum number of records expected per time window; lookback_windows: (optional) number of time windows to look back from curr_timestamp, it filters records to include only those within the specified number of time windows from curr_timestamp (if no lookback is provided, the check is applied to the entire dataset); curr_timestamp: (optional) current timestamp column (if not provided, current_timestamp() function is used)