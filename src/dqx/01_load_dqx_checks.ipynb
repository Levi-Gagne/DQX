{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load DQX Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff95a1d5-3010-440c-a60f-e8b4c7fcc84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbe9a89-330c-4868-9766-f17332b0f64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Runbook — `01_load_dqx_checks` (DQX Rule Loader)\n",
    "\n",
    "**Purpose**  \n",
    "Load and validate DQX rules defined in YAML, canonicalize them into a stable `check_id`, and overwrite the target **rules catalog** table with helpful metadata, comments, and a PK declaration. Designed for Databricks + Unity Catalog.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "**Standard run**\n",
    "```python\n",
    "main()\n",
    "```\n",
    "\n",
    "**Dry run (no write)**\n",
    "```python\n",
    "main(dry_run=True)\n",
    "```\n",
    "\n",
    "**Validate only (no write)**\n",
    "```python\n",
    "main(validate_only=True)\n",
    "```\n",
    "\n",
    "**Common overrides**\n",
    "```python\n",
    "main(\n",
    "  output_config_path=\"resources/dqx_config.yaml\",\n",
    "  rules_dir=None,                          # else pulled from config\n",
    "  batch_dedupe_mode=\"warn\",                # warn | error | skip\n",
    "  time_zone=\"America/Chicago\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs & Outputs\n",
    "\n",
    "**Config (`resources/dqx_config.yaml`)**\n",
    "- `dqx_yaml_checks`: folder with YAML files (recursive; supports `dbfs:/` via `/dbfs/...` bridge)\n",
    "- `dqx_checks_config_table_name`: fully qualified UC table to overwrite, e.g. `dq_dev.dqx.checks`\n",
    "\n",
    "**YAML rule shape (per rule)**\n",
    "- Required: `table_name` (catalog.schema.table), `name`, `criticality` (`warn|warning|error`), `run_config_name`, `check.function`\n",
    "- Optional: `check.for_each_column` (list of strings), `check.arguments` (map; values stringified), `filter`, `user_metadata`, `active` (default `true`)\n",
    "\n",
    "**Output table (overwrite)**\n",
    "- Schema includes: `check_id` (PRIMARY KEY by design), `check_id_payload`, `table_name`, `name`, `criticality`, `check{function,for_each_column,arguments}`, `filter`, `run_config_name`, `user_metadata`, `yaml_path`, `active`, `created_by/at`, `updated_by/at`.\n",
    "- **Metadata**: table & column **comments** are applied.\n",
    "- **Constraint**: attempts `ALTER TABLE ... ADD CONSTRAINT pk_check_id PRIMARY KEY (check_id) RELY` (Unity Catalog).\n",
    "- **Guardrail**: hard **runtime assertion** ensures `check_id` uniqueness (fails the run if duplicates exist).\n",
    "\n",
    "---\n",
    "\n",
    "## What the Notebook Does (High Level)\n",
    "\n",
    "1) **Environment banner** — prints cluster/warehouse info with local timezone.  \n",
    "2) **Read config** — resolves `rules_dir` and `delta_table_name`.  \n",
    "3) **Discover YAMLs** — recursively finds `*.yaml|*.yml` under `rules_dir` (including nested folders).  \n",
    "4) **Validate (optional short-circuit)** — per file + per rule + **DQEngine.validate_checks**.  \n",
    "5) **Load & canonicalize** — normalizes `filter`, sorts `for_each_column`, stringifies `arguments`, builds canonical JSON payload, computes `check_id=sha256(payload)`.  \n",
    "6) **Batch de-dupe by `check_id`** — keep the lexicographically first (by `yaml_path`, `name`); mode: `warn|error|skip`.  \n",
    "7) **Diagnostics** — displays totals, sample rules, and counts by table.  \n",
    "8) **Overwrite target table** — writes Delta with `overwriteSchema=true` and timestamp casting.  \n",
    "9) **Apply documentation** — table comment (fallback to TBLPROPERTIES) + column comments (with ALTER COLUMN; fallback to COMMENT ON COLUMN).  \n",
    "10) **PK & enforcement** — add/refresh **informational PK** on `check_id` (if UC) and **assert uniqueness** at runtime.  \n",
    "11) **Result summary** — small confirmation table and a printed success line.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters (function `main`)\n",
    "\n",
    "- `output_config_path`: YAML path for loader config (default `resources/dqx_config.yaml`).  \n",
    "- `rules_dir`: override rules folder (else from config).  \n",
    "- `time_zone`: used for audit timestamps.  \n",
    "- `dry_run`: if `True`, loads + displays but **does not write**.  \n",
    "- `validate_only`: if `True`, runs validations and **exits**.  \n",
    "- `required_fields`: override minimal YAML fields (default set used).  \n",
    "- `batch_dedupe_mode`: `warn` (print & keep first) | `error` (fail) | `skip` (no dedupe).  \n",
    "- `table_doc`: optional dict to override default table/column comments.\n",
    "\n",
    "---\n",
    "\n",
    "## Prereqs & Permissions\n",
    "\n",
    "- Python deps: `pyspark`, `pyyaml`, `databricks-labs-dqx (0.8.x)` available on the cluster/warehouse.  \n",
    "- Data access: **READ** on `rules_dir` (workspace file system or `dbfs:/`), **CREATE/ALTER** on target catalog & schema, and **CREATE TABLE / ALTER TABLE** privileges to set comments/constraints.  \n",
    "- Unity Catalog strongly recommended (for PK declaration). The run still proceeds if constraint DDL is not supported; a message is logged.\n",
    "\n",
    "---\n",
    "\n",
    "## Failure Modes & Fixes\n",
    "\n",
    "- **Validation failed** (missing required fields, invalid `criticality`, `table_name` not fully qualified, DQEngine errors): fix YAML and re-run.  \n",
    "- **Duplicate `check_id`** during de-dupe or final uniqueness assertion: adjust YAML so canonical payloads differ (e.g., rule name is *not* part of identity).  \n",
    "- **Insufficient privileges** during write/comments/constraint: grant `CREATE/ALTER` on target schema/table, or run with a service principal that has the rights.  \n",
    "- **Column comments not applied**: only columns present in the table are touched; fallback DDL is attempted and any skips are logged.\n",
    "\n",
    "---\n",
    "\n",
    "## Post‑Run Sanity Checks (SQL)\n",
    "\n",
    "```sql\n",
    "-- Table present & counts\n",
    "SELECT COUNT(*) AS rules, COUNT(DISTINCT check_id) AS unique_rules FROM dq_dev.dqx.checks;\n",
    "\n",
    "-- Any dup check_id? (should be 0 rows)\n",
    "SELECT check_id, COUNT(*) c FROM dq_dev.dqx.checks GROUP BY check_id HAVING COUNT(*)>1;\n",
    "\n",
    "-- Peek arguments & coverage\n",
    "SELECT name, check.function, check.arguments FROM dq_dev.dqx.checks LIMIT 20;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Overwrite semantics: this is the **system of record** for rules derived from YAML; manual edits in the table will be lost on next run.  \n",
    "- `check_id` identity includes only `{table_name↓, filter, check.*}` — **not** `name`, `criticality`, or `run_config_name`.  \n",
    "- Argument values are persisted as strings for stability; cast/parse (e.g., `try_cast`, `from_json`) downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5042f1e5-f5d4-4dda-a0b8-3e9130961893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FLOW"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START: main (01_load_dqx_checks)\n",
    "|\n",
    "|-- 0. Environment banner\n",
    "|     |-- print_notebook_env(spark, local_timezone)\n",
    "|\n",
    "|-- 1. Load output config\n",
    "|     |-- output_config = yaml(resources/dqx_config.yaml)\n",
    "|     |-- rules_dir = output_config[\"dqx_yaml_checks\"]\n",
    "|     |-- delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "|     |-- required_fields = [\"table_name\",\"name\",\"criticality\",\"run_config_name\",\"check\"]\n",
    "|\n",
    "|-- 2. Recursively discover YAMLs\n",
    "|     |-- files = discover_yaml_files_recursive(rules_dir)  # supports dbfs:/ via /dbfs bridge\n",
    "|     |-- DISPLAY: list of YAML paths\n",
    "|\n",
    "|-- 3. Validation-only short-circuit (if validate_only=True)\n",
    "|     |-- For each file:\n",
    "|     |     |-- load_yaml_rules(file)  # supports multi-doc YAML, flattens lists\n",
    "|     |     |-- File-level checks: non-empty; no duplicate rule names\n",
    "|     |     |-- For each rule:\n",
    "|     |     |     |-- Rule-level checks:\n",
    "|     |     |     |     |-- table_name fully qualified (catalog.schema.table)\n",
    "|     |     |     |     |-- criticality ∈ {warn, warning, error}\n",
    "|     |     |     |     |-- check.function exists\n",
    "|     |     |     |-- DQEngine.validate_checks(docs) must pass\n",
    "|     |-- PRINT summary; END\n",
    "|\n",
    "|-- 4. Load + flatten rules (normal path)\n",
    "|     |-- all_rules = []\n",
    "|     |-- For each file in files:\n",
    "|           |-- docs = load_yaml_rules(file)\n",
    "|           |-- File-level checks (as above)\n",
    "|           |-- now = current_time_iso(time_zone)\n",
    "|           |-- For each rule in docs:\n",
    "|                 |-- Rule-level checks (as above)\n",
    "|                 |-- Canonicalize:\n",
    "|                 |     |-- filter := normalized whitespace (or \"\")\n",
    "|                 |     |-- check := {\n",
    "|                 |            function,\n",
    "|                 |            for_each_column := sorted list or None (validated as list[str]),\n",
    "|                 |            arguments := map with all values stringified\n",
    "|                 |        }\n",
    "|                 |     |-- user_metadata := map with stringified values (or None)\n",
    "|                 |-- payload := JSON over {table_name_lower, filter, canonical check} (sorted keys, tight separators)\n",
    "|                 |-- check_id := sha256(payload)\n",
    "|                 |-- Append flattened dict:\n",
    "|                 |     |-- keys: [check_id, check_id_payload, table_name, name, criticality,\n",
    "|                 |                check{function, for_each_column, arguments}, filter, run_config_name,\n",
    "|                 |                user_metadata, yaml_path=file, active (default True),\n",
    "|                 |                created_by=\"AdminUser\", created_at=now, updated_by=None, updated_at=None]\n",
    "|           |-- DQEngine.validate_checks(docs) must pass (file-level)\n",
    "|           |-- PRINT \"[loader] {file}: rules={n}\"\n",
    "|\n",
    "|-- 5. Batch de-duplication (by check_id only)\n",
    "|     |-- Group all_rules by check_id\n",
    "|     |-- Keep the lexicographically first (yaml_path, name); drop others\n",
    "|     |-- Mode:\n",
    "|           |-- \"warn\" (default): print detailed duplicate blocks\n",
    "|           |-- \"error\": raise\n",
    "|           |-- \"skip\": keep all (no drops)\n",
    "|\n",
    "|-- 6. Assemble DataFrame + display-first diagnostics\n",
    "|     |-- df := spark.createDataFrame(all_rules, schema=TABLE_SCHEMA)\n",
    "|     |-- DISPLAY: totals (count, distinct check_id, distinct (check_id, run_config_name))\n",
    "|     |-- DISPLAY: sample(check_id, name, run_config_name, yaml_path) ordered by yaml_path desc\n",
    "|     |-- DISPLAY: rules per table_name\n",
    "|     |-- (If very small) DISPLAY: first 3 payloads\n",
    "|\n",
    "|-- 7. Dry-run short-circuit (if dry_run=True)\n",
    "|     |-- DISPLAY: full rules preview ordered by (table_name, name)\n",
    "|     |-- END\n",
    "|\n",
    "|-- 8. Overwrite Delta target table (ALWAYS overwrite)\n",
    "|     |-- existed_before := spark.catalog.tableExists(delta_table_name)\n",
    "|     |-- ensure_schema_exists(catalog.schema)\n",
    "|     |-- Cast timestamps: created_at/updated_at := to_timestamp\n",
    "|     |-- df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(delta_table_name)\n",
    "|\n",
    "|-- 9. Apply documentation metadata (table + columns)\n",
    "|     |-- doc := materialize DQX_CHECKS_CONFIG_METADATA with {TABLE_FQN}=delta_table_name\n",
    "|     |-- Apply table comment (COMMENT ON TABLE; fallback to TBLPROPERTIES)\n",
    "|     |-- If created_now (i.e., !existed_before):\n",
    "|           |-- For each existing column: ALTER TABLE ... ALTER COLUMN ... COMMENT\n",
    "|     |-- DISPLAY: preview of table comment + column comments\n",
    "|\n",
    "|-- 10. Write result summary\n",
    "|     |-- DISPLAY: single-row summary (rules written, target table)\n",
    "|     |-- PRINT: confirmation line with target table\n",
    "|\n",
    "END: main\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74936d7c-4414-42fe-a143-cb6c7d364d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1e26e-12cc-44e8-ae5a-e7b1fa4d5049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1154f87-29b1-4cb5-8a2f-ff0737aa9cda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8ba7a8-9467-491e-8d9f-a34733f9f01d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 01_load_dqx_checks  (DISPLAY-FIRST + METADATA, COMMENTS-ALWAYS, PK + RUNTIME UNIQUENESS)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import yaml\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, types as T\n",
    "from pyspark.sql.functions import to_timestamp, col, desc\n",
    "\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "from utils.color import Color\n",
    "\n",
    "# ======================================================\n",
    "# Small helpers for Databricks-friendly tabular display\n",
    "# ======================================================\n",
    "\n",
    "def _can_display() -> bool:\n",
    "    return \"display\" in globals()\n",
    "\n",
    "def show_df(df: DataFrame, n: int = 100, truncate: bool = False) -> None:\n",
    "    if _can_display():\n",
    "        display(df.limit(n))\n",
    "    else:\n",
    "        df.show(n, truncate=truncate)\n",
    "\n",
    "def display_section(title: str) -> None:\n",
    "    print(\"\\n\" + f\"{Color.b}{Color.deep_magenta}═{Color.r}\" * 80)\n",
    "    print(f\"{Color.b}{Color.deep_magenta}║{Color.r} {Color.b}{Color.ghost_white}{title}{Color.r}\")\n",
    "    print(f\"{Color.b}{Color.deep_magenta}═{Color.r}\" * 80)\n",
    "\n",
    "# =========================\n",
    "# Target schema (Delta sink)\n",
    "# =========================\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"check_id\",            T.StringType(), False),  # PRIMARY KEY (by convention + runtime assertion)\n",
    "    T.StructField(\"check_id_payload\",    T.StringType(), False),\n",
    "    T.StructField(\"table_name\",          T.StringType(), False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",                T.StringType(), False),\n",
    "    T.StructField(\"criticality\",         T.StringType(), False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",              T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\",     T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # Ops fields\n",
    "    T.StructField(\"yaml_path\",           T.StringType(), False),\n",
    "    T.StructField(\"active\",              T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\",          T.StringType(), False),\n",
    "    T.StructField(\"created_at\",          T.StringType(), False),  # ISO string; cast on write\n",
    "    T.StructField(\"updated_by\",          T.StringType(), True),\n",
    "    T.StructField(\"updated_at\",          T.StringType(), True),\n",
    "])\n",
    "\n",
    "# ======================================================\n",
    "# Documentation dictionary\n",
    "# ======================================================\n",
    "DQX_CHECKS_CONFIG_METADATA: Dict[str, Any] = {\n",
    "    \"table\": \"<override at create time>\",\n",
    "    \"table_comment\": (\n",
    "        \"## DQX Checks Configuration\\n\"\n",
    "        \"- One row per **unique canonical rule** generated from YAML (source of truth).\\n\"\n",
    "        \"- **Primary key**: `check_id` (sha256 of canonical payload). Uniqueness is enforced by the loader and a runtime assertion.\\n\"\n",
    "        \"- Rebuilt by the loader (typically **overwrite** semantics); manual edits will be lost.\\n\"\n",
    "        \"- Used by runners to resolve rules per `run_config_name` and by logs to map back to rule identity.\\n\"\n",
    "        \"- `check_id_payload` preserves the exact canonical JSON used to compute `check_id` for reproducibility.\\n\"\n",
    "        \"- `run_config_name` is a **routing tag**, not part of identity.\\n\"\n",
    "        \"- Only rows with `active=true` are executed.\"\n",
    "    ),\n",
    "    \"columns\": {\n",
    "        \"check_id\": \"PRIMARY KEY. Stable sha256 over canonical {table_name↓, filter, check.*}.\",\n",
    "        \"check_id_payload\": \"Canonical JSON used to derive `check_id` (sorted keys, normalized values).\",\n",
    "        \"table_name\": \"Target table FQN (`catalog.schema.table`). Lowercased in payload for stability.\",\n",
    "        \"name\": \"Human-readable rule name. Used in UI/diagnostics and name-based joins when enriching logs.\",\n",
    "        \"criticality\": \"Rule severity: `warn|warning|error`. Reporting normalizes warn/warning → `warning`.\",\n",
    "        \"check\": \"Structured rule: `{function, for_each_column?, arguments?}`; argument values stringified.\",\n",
    "        \"filter\": \"Optional SQL predicate applied before evaluation (row-level). Normalized in payload.\",\n",
    "        \"run_config_name\": \"Execution group/tag. Drives which runs pick up this rule; **not** part of identity.\",\n",
    "        \"user_metadata\": \"Free-form `map<string,string>` carried through to issues for traceability.\",\n",
    "        \"yaml_path\": \"Absolute/volume path to the defining YAML doc (lineage).\",\n",
    "        \"active\": \"If `false`, rule is ignored by runners.\",\n",
    "        \"created_by\": \"Audit: creator/principal that materialized the row.\",\n",
    "        \"created_at\": \"Audit: creation timestamp (cast to TIMESTAMP on write).\",\n",
    "        \"updated_by\": \"Audit: last updater (nullable).\",\n",
    "        \"updated_at\": \"Audit: last update timestamp (nullable; cast to TIMESTAMP on write).\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def _materialize_table_doc(doc_template: Dict[str, Any], table_fqn: str) -> Dict[str, Any]:\n",
    "    copy = json.loads(json.dumps(doc_template))\n",
    "    copy[\"table\"] = table_fqn\n",
    "    if \"table_comment\" in copy and isinstance(copy[\"table_comment\"], str):\n",
    "        copy[\"table_comment\"] = copy[\"table_comment\"].replace(\"{TABLE_FQN}\", table_fqn)\n",
    "    return copy\n",
    "\n",
    "# =========================\n",
    "# YAML loading (robust)\n",
    "# =========================\n",
    "def load_yaml_rules(path: str) -> List[dict]:\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = list(yaml.safe_load_all(fh))\n",
    "    out: List[dict] = []\n",
    "    for d in docs:\n",
    "        if d is None:\n",
    "            continue\n",
    "        if isinstance(d, dict):\n",
    "            out.append(d)\n",
    "        elif isinstance(d, list):\n",
    "            out.extend([x for x in d if isinstance(x, dict)])\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Canonicalization & IDs\n",
    "# =========================\n",
    "def _canon_filter(s: Optional[str]) -> str:\n",
    "    return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "    fec = chk.get(\"for_each_column\")\n",
    "    if isinstance(fec, list):\n",
    "        out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "    args = chk.get(\"arguments\") or {}\n",
    "    canon_args: Dict[str, str] = {}\n",
    "    for k, v in args.items():\n",
    "        sv = \"\" if v is None else str(v).strip()\n",
    "        if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "            try:\n",
    "                sv = json.dumps(json.loads(sv), sort_keys=True, separators=(\",\", \":\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        canon_args[str(k)] = sv\n",
    "    out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "    return out\n",
    "\n",
    "def compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    payload_obj = {\n",
    "        \"table_name\": (table_name or \"\").lower(),\n",
    "        \"filter\": _canon_filter(filter_str),\n",
    "        \"check\": _canon_check(check_dict or {}),\n",
    "    }\n",
    "    return json.dumps(payload_obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def compute_check_id_from_payload(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode()).hexdigest()\n",
    "\n",
    "# =========================\n",
    "# Conversions / validation\n",
    "# =========================\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def validate_rules_file(rules: List[dict], file_path: str):\n",
    "    if not rules:\n",
    "        raise ValueError(f\"No rules found in {file_path} (empty or invalid YAML).\")\n",
    "    probs, seen = [], set()\n",
    "    for r in rules:\n",
    "        nm = r.get(\"name\")\n",
    "        if not nm:\n",
    "            probs.append(f\"Missing rule name in {file_path}\")\n",
    "        if nm in seen:\n",
    "            probs.append(f\"Duplicate rule name '{nm}' in {file_path}\")\n",
    "        seen.add(nm)\n",
    "    if probs:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {probs}\")\n",
    "\n",
    "def validate_rule_fields(\n",
    "    rule: dict,\n",
    "    file_path: str,\n",
    "    required_fields: List[str],\n",
    "    allowed_criticality={\"error\", \"warn\", \"warning\"},\n",
    "):\n",
    "    probs = []\n",
    "    for f in required_fields:\n",
    "        if not rule.get(f):\n",
    "            probs.append(f\"Missing required field '{f}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        probs.append(\n",
    "            f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if rule.get(\"criticality\") not in allowed_criticality:\n",
    "        probs.append(\n",
    "            f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        probs.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if probs:\n",
    "        raise ValueError(\"Rule-level validation failed: \" + \"; \".join(probs))\n",
    "\n",
    "def validate_with_dqx(rules: List[dict], file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if getattr(status, \"has_errors\", False):\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "# =========================\n",
    "# Build rows\n",
    "# =========================\n",
    "def process_yaml_file(path: str, required_fields: List[str], time_zone: str = \"UTC\", created_by: str = \"AdminUser\") -> List[dict]:\n",
    "    docs = load_yaml_rules(path)\n",
    "    if not docs:\n",
    "        print(f\"[skip] {path} has no rules.\")\n",
    "        return []\n",
    "\n",
    "    validate_rules_file(docs, path)\n",
    "\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat: List[dict] = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path, required_fields=required_fields)\n",
    "\n",
    "        raw_check = rule[\"check\"] or {}\n",
    "        payload = compute_check_id_payload(rule[\"table_name\"], raw_check, rule.get(\"filter\"))\n",
    "        check_id = compute_check_id_from_payload(payload)\n",
    "\n",
    "        function = raw_check.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(\n",
    "                f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\"\n",
    "            )\n",
    "\n",
    "        for_each = raw_check.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(\n",
    "                f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\"\n",
    "            )\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(\n",
    "                    f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\"\n",
    "                )\n",
    "\n",
    "        arguments = raw_check.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        flat.append(\n",
    "            {\n",
    "                \"check_id\": check_id,\n",
    "                \"check_id_payload\": payload,\n",
    "                \"table_name\": rule[\"table_name\"],\n",
    "                \"name\": rule[\"name\"],\n",
    "                \"criticality\": rule[\"criticality\"],\n",
    "                \"check\": {\n",
    "                    \"function\": function,\n",
    "                    \"for_each_column\": for_each if for_each else None,\n",
    "                    \"arguments\": arguments if arguments else None,\n",
    "                },\n",
    "                \"filter\": rule.get(\"filter\"),\n",
    "                \"run_config_name\": rule[\"run_config_name\"],\n",
    "                \"user_metadata\": user_metadata if user_metadata else None,\n",
    "                \"yaml_path\": path,\n",
    "                \"active\": rule.get(\"active\", True),\n",
    "                \"created_by\": created_by,\n",
    "                \"created_at\": now,\n",
    "                \"updated_by\": None,\n",
    "                \"updated_at\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat\n",
    "\n",
    "# =========================\n",
    "# Batch dedupe (on check_id ONLY)\n",
    "# =========================\n",
    "def _fmt_rule_for_dup(r: dict) -> str:\n",
    "    return (\n",
    "        f\"name={r.get('name')} | file={r.get('yaml_path')} | \"\n",
    "        f\"criticality={r.get('criticality')} | run_config={r.get('run_config_name')} | \"\n",
    "        f\"filter={r.get('filter')}\"\n",
    "    )\n",
    "\n",
    "def dedupe_rules_in_batch_by_check_id(rules: List[dict], mode: str = \"warn\") -> List[dict]:\n",
    "    groups: Dict[str, List[dict]] = {}\n",
    "    for r in rules:\n",
    "        groups.setdefault(r[\"check_id\"], []).append(r)\n",
    "\n",
    "    out: List[dict] = []\n",
    "    dropped = 0\n",
    "    blocks: List[str] = []\n",
    "\n",
    "    for cid, lst in groups.items():\n",
    "        if len(lst) == 1:\n",
    "            out.append(lst[0]); continue\n",
    "        lst = sorted(lst, key=lambda x: (x.get(\"yaml_path\", \"\"), x.get(\"name\", \"\")))\n",
    "        keep, dups = lst[0], lst[1:]\n",
    "        dropped += len(dups)\n",
    "        head = f\"[dup/batch/check_id] {len(dups)} duplicate(s) for check_id={cid[:12]}…\"\n",
    "        lines = [\"    \" + _fmt_rule_for_dup(x) for x in lst]\n",
    "        tail = f\"    -> keeping: name={keep.get('name')} | file={keep.get('yaml_path')}\"\n",
    "        blocks.append(\"\\n\".join([head, *lines, tail]))\n",
    "        out.append(keep)\n",
    "\n",
    "    if dropped:\n",
    "        msg = \"\\n\\n\".join(blocks) + f\"\\n[dedupe/batch] total dropped={dropped}\"\n",
    "        if mode == \"error\":\n",
    "            raise ValueError(msg)\n",
    "        if mode == \"warn\":\n",
    "            print(msg)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Comments + Constraints + Helpers\n",
    "# =========================\n",
    "def _esc_comment(s: str) -> str:\n",
    "    return (s or \"\").replace(\"'\", \"''\")\n",
    "\n",
    "def _q_fqn(fqn: str) -> str:\n",
    "    return \".\".join(f\"`{p}`\" for p in fqn.split(\".\"))\n",
    "\n",
    "def preview_table_documentation(spark: SparkSession, table_fqn: str, doc: Dict[str, Any]) -> None:\n",
    "    display_section(\"TABLE METADATA PREVIEW (markdown text stored in comments)\")\n",
    "    doc_df = spark.createDataFrame(\n",
    "        [(table_fqn, doc.get(\"table_comment\", \"\"))],\n",
    "        schema=\"table string, table_comment_markdown string\",\n",
    "    )\n",
    "    show_df(doc_df, n=1, truncate=False)\n",
    "\n",
    "    cols = doc.get(\"columns\", {}) or {}\n",
    "    cols_df = spark.createDataFrame(\n",
    "        [(k, v) for k, v in cols.items()],\n",
    "        schema=\"column string, column_comment_markdown string\",\n",
    "    )\n",
    "    show_df(cols_df, n=200, truncate=False)\n",
    "\n",
    "def apply_table_documentation(\n",
    "    spark: SparkSession,\n",
    "    table_fqn: str,\n",
    "    doc: Optional[Dict[str, Any]],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Apply table & column comments (ALWAYS).\n",
    "    Uses robust syntax; falls back to TBLPROPERTIES for table comment if needed.\n",
    "    \"\"\"\n",
    "    if not doc:\n",
    "        return\n",
    "    qtable = _q_fqn(table_fqn)\n",
    "\n",
    "    # Table comment\n",
    "    table_comment = _esc_comment(doc.get(\"table_comment\", \"\"))\n",
    "    if table_comment:\n",
    "        try:\n",
    "            spark.sql(f\"COMMENT ON TABLE {qtable} IS '{table_comment}'\")\n",
    "        except Exception:\n",
    "            spark.sql(f\"ALTER TABLE {qtable} SET TBLPROPERTIES ('comment' = '{table_comment}')\")\n",
    "\n",
    "    # Column comments (always attempt)\n",
    "    cols: Dict[str, str] = doc.get(\"columns\", {}) or {}\n",
    "    existing_cols = {f.name.lower() for f in spark.table(table_fqn).schema.fields}\n",
    "    for col_name, comment in cols.items():\n",
    "        if col_name.lower() not in existing_cols:\n",
    "            continue\n",
    "        qcol = f\"`{col_name}`\"\n",
    "        comment_sql = f\"ALTER TABLE {qtable} ALTER COLUMN {qcol} COMMENT '{_esc_comment(comment)}'\"\n",
    "        try:\n",
    "            spark.sql(comment_sql)\n",
    "        except Exception as e:\n",
    "            # Try COMMENT ON COLUMN as fallback\n",
    "            try:\n",
    "                spark.sql(f\"COMMENT ON COLUMN {qtable}.{qcol} IS '{_esc_comment(comment)}'\")\n",
    "            except Exception as e2:\n",
    "                print(f\"[meta] Skipped column comment for {table_fqn}.{col_name}: {e2}\")\n",
    "\n",
    "def ensure_primary_key_constraint(\n",
    "    spark: SparkSession,\n",
    "    table_fqn: str,\n",
    "    column: str = \"check_id\",\n",
    "    constraint_name: str = \"pk_check_id\",\n",
    "    rely: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Adds/refreshes an informational PRIMARY KEY constraint (Unity Catalog only).\n",
    "    Not enforced by the engine; used for metadata/optimization. Will replace if exists.\n",
    "    \"\"\"\n",
    "    qtable = _q_fqn(table_fqn)\n",
    "    opt_rely = \" RELY\" if rely else \"\"\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {qtable} ADD CONSTRAINT {constraint_name} PRIMARY KEY ({column}){opt_rely}\")\n",
    "    except Exception:\n",
    "        # try drop+add in case it already existed\n",
    "        try:\n",
    "            spark.sql(f\"ALTER TABLE {qtable} DROP CONSTRAINT {constraint_name}\")\n",
    "            spark.sql(f\"ALTER TABLE {qtable} ADD CONSTRAINT {constraint_name} PRIMARY KEY ({column}){opt_rely}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[meta] Could not set PRIMARY KEY for {table_fqn}.{column}: {e}\")\n",
    "\n",
    "def assert_unique_check_id(spark: SparkSession, table_fqn: str) -> None:\n",
    "    \"\"\"\n",
    "    Hard runtime enforcement: fail if any duplicate check_id exists.\n",
    "    \"\"\"\n",
    "    qtable = _q_fqn(table_fqn)\n",
    "    dup = spark.sql(f\"\"\"\n",
    "        SELECT check_id, COUNT(*) AS c\n",
    "        FROM {qtable}\n",
    "        GROUP BY check_id\n",
    "        HAVING COUNT(*) > 1\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()\n",
    "    if dup:\n",
    "        cid, c = dup[0][\"check_id\"], dup[0][\"c\"]\n",
    "        raise RuntimeError(f\"[ENFORCE UNIQUE] Found duplicate check_id={cid} (count={c}) in {table_fqn}. Aborting.\")\n",
    "\n",
    "# =========================\n",
    "# Recursive YAML discovery\n",
    "# =========================\n",
    "def _normalize_base(path: str) -> str:\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        return \"/dbfs/\" + path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    return path\n",
    "\n",
    "def discover_yaml_files_recursive(base_dir: str) -> List[str]:\n",
    "    base = _normalize_base(base_dir)\n",
    "    if not os.path.isdir(base):\n",
    "        raise FileNotFoundError(f\"Rules folder not found or not a directory: {base_dir} (resolved: {base})\")\n",
    "    out: List[str] = []\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "        for f in files:\n",
    "            if f.startswith(\".\"):\n",
    "                continue\n",
    "            fl = f.lower()\n",
    "            if fl.endswith(\".yaml\") or fl.endswith(\".yml\"):\n",
    "                out.append(os.path.join(root, f))\n",
    "    return sorted(out)\n",
    "\n",
    "# =========================\n",
    "# Delta I/O (ALWAYS OVERWRITE)\n",
    "# =========================\n",
    "def ensure_schema_exists(spark: SparkSession, full_table_name: str):\n",
    "    parts = full_table_name.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected a 3-part name (catalog.schema.table), got '{full_table_name}'\")\n",
    "    cat, sch, _ = parts\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "\n",
    "def overwrite_rules_into_delta(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    delta_table_name: str,\n",
    "    table_doc: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    # Track existence BEFORE write (for info only)\n",
    "    existed_before = spark.catalog.tableExists(delta_table_name)\n",
    "\n",
    "    # Ensure schema\n",
    "    ensure_schema_exists(spark, delta_table_name)\n",
    "\n",
    "    # Cast audit timestamps\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    # Overwrite (content + schema)\n",
    "    df.write.format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .option(\"overwriteSchema\", \"true\") \\\n",
    "      .saveAsTable(delta_table_name)\n",
    "\n",
    "    # Apply comments (table + columns) ALWAYS\n",
    "    doc_to_apply = _materialize_table_doc(table_doc or DQX_CHECKS_CONFIG_METADATA, delta_table_name)\n",
    "    apply_table_documentation(spark, delta_table_name, doc_to_apply)\n",
    "\n",
    "    # Add/refresh informational PK on check_id (Unity Catalog required)\n",
    "    ensure_primary_key_constraint(spark, delta_table_name, column=\"check_id\", constraint_name=\"pk_check_id\", rely=True)\n",
    "\n",
    "    # Enforce uniqueness at runtime (hard fail if violated)\n",
    "    assert_unique_check_id(spark, delta_table_name)\n",
    "\n",
    "    # Preview metadata\n",
    "    preview_table_documentation(spark, delta_table_name, doc_to_apply)\n",
    "\n",
    "    # Display a clean confirmation table\n",
    "    display_section(\"WRITE RESULT (Delta)\")\n",
    "    summary = spark.createDataFrame(\n",
    "        [(df.count(), delta_table_name, \"overwrite\", \"pk_check_id\")],\n",
    "        schema=\"`rules written` long, `target table` string, `mode` string, `constraint` string\",\n",
    "    )\n",
    "    show_df(summary, n=1)\n",
    "    print(f\"\\n{Color.b}{Color.ivory}Rules written to: '{Color.r}{Color.b}{Color.chartreuse}{delta_table_name}{Color.r}{Color.b}{Color.ivory}'  (PK declared, uniqueness asserted)\")\n",
    "\n",
    "# =========================\n",
    "# Display-first debug helpers\n",
    "# =========================\n",
    "def debug_display_batch(spark: SparkSession, df_rules: DataFrame) -> None:\n",
    "    display_section(\"SUMMARY OF RULES LOADED FROM YAML\")\n",
    "    totals = [\n",
    "        (\n",
    "            df_rules.count(),\n",
    "            df_rules.select(\"check_id\").distinct().count(),\n",
    "            df_rules.select(\"check_id\", \"run_config_name\").distinct().count(),\n",
    "        )\n",
    "    ]\n",
    "    totals_df = spark.createDataFrame(\n",
    "        totals,\n",
    "        schema=\"`total number of rules found` long, `unique rules found` long, `distinct pair of rules` long\",\n",
    "    )\n",
    "    show_df(totals_df, n=1)\n",
    "\n",
    "    display_section(\"SAMPLE OF RULES LOADED FROM YAML (check_id, name, run_config_name, yaml_path)\")\n",
    "    sample_cols = df_rules.select(\"check_id\", \"name\", \"run_config_name\", \"yaml_path\").orderBy(\n",
    "        desc(\"yaml_path\")\n",
    "    )\n",
    "    show_df(sample_cols, n=50, truncate=False)\n",
    "\n",
    "    display_section(\"RULES LOADED PER TABLE\")\n",
    "    by_table = df_rules.groupBy(\"table_name\").count().orderBy(desc(\"count\"))\n",
    "    show_df(by_table, n=200)\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules: List[dict]) -> Optional[DataFrame]:\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return None\n",
    "    df = (\n",
    "        spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "        .withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    )\n",
    "    debug_display_batch(spark, df)\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Validation (works off a file list)\n",
    "# =========================\n",
    "def validate_rule_files(file_paths: List[str], required_fields: List[str], fail_fast: bool = True) -> List[str]:\n",
    "    errors = []\n",
    "    for full_path in file_paths:\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            docs = load_yaml_rules(full_path)\n",
    "            if not docs:\n",
    "                print(f\"  (empty) skipped: {full_path}\")\n",
    "                continue\n",
    "            validate_rules_file(docs, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path, required_fields=required_fields)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "\n",
    "# =========================\n",
    "# Recursive discovery + write\n",
    "# =========================\n",
    "def _load_output_config(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as fh:\n",
    "        return yaml.safe_load(fh) or {}\n",
    "\n",
    "def main(\n",
    "    output_config_path: str = \"resources/dqx_config.yaml\",\n",
    "    rules_dir: Optional[str] = None,\n",
    "    time_zone: str = \"America/Chicago\",\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False,\n",
    "    required_fields: Optional[List[str]] = None,\n",
    "    batch_dedupe_mode: str = \"warn\",  # warn | error | skip\n",
    "    table_doc: Optional[Dict[str, Any]] = None,\n",
    "    created_by: str = \"AdminUser\",\n",
    "):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    print_notebook_env(spark, local_timezone=time_zone)\n",
    "\n",
    "    output_config = _load_output_config(output_config_path)\n",
    "    rules_dir = rules_dir or output_config[\"dqx_yaml_checks\"]\n",
    "    delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "\n",
    "    required_fields = required_fields or [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "\n",
    "    # Recursively discover YAML files\n",
    "    yaml_files = discover_yaml_files_recursive(rules_dir)\n",
    "    display_section(\"YAML FILES DISCOVERED (recursive)\")\n",
    "    files_df = spark.createDataFrame([(p,) for p in yaml_files], \"yaml_path string\")\n",
    "    show_df(files_df, n=500, truncate=False)\n",
    "\n",
    "    if validate_only:\n",
    "        print(\"\\nValidation only: not writing any rules.\")\n",
    "        errs = validate_rule_files(yaml_files, required_fields)\n",
    "        return {\n",
    "            \"mode\": \"validate_only\",\n",
    "            \"config_path\": output_config_path,\n",
    "            \"rules_files\": len(yaml_files),\n",
    "            \"errors\": errs,\n",
    "        }\n",
    "\n",
    "    # Collect rules\n",
    "    all_rules: List[dict] = []\n",
    "    for full_path in yaml_files:\n",
    "        file_rules = process_yaml_file(full_path, required_fields=required_fields, time_zone=time_zone, created_by=created_by)\n",
    "        if file_rules:\n",
    "            all_rules.extend(file_rules)\n",
    "            print(f\"[loader] {full_path}: rules={len(file_rules)}\")\n",
    "\n",
    "    if not all_rules:\n",
    "        print(\"No rules discovered; nothing to do.\")\n",
    "        return {\"mode\": \"no_op\", \"config_path\": output_config_path, \"rules_files\": len(yaml_files), \"wrote_rows\": 0}\n",
    "\n",
    "    print(f\"[loader] total parsed rules (pre-dedupe): {len(all_rules)}\")\n",
    "\n",
    "    # In-batch dedupe on check_id only\n",
    "    pre_dedupe = len(all_rules)\n",
    "    all_rules = dedupe_rules_in_batch_by_check_id(all_rules, mode=batch_dedupe_mode)\n",
    "    post_dedupe = len(all_rules)\n",
    "\n",
    "    # Assemble DataFrame and DISPLAY diagnostics\n",
    "    df = spark.createDataFrame(all_rules, schema=TABLE_SCHEMA)\n",
    "    debug_display_batch(spark, df)\n",
    "\n",
    "    unique_check_ids = df.select(\"check_id\").distinct().count()\n",
    "    distinct_pairs  = df.select(\"check_id\", \"run_config_name\").distinct().count()\n",
    "\n",
    "    if dry_run:\n",
    "        display_section(\"DRY-RUN: FULL RULES PREVIEW\")\n",
    "        show_df(df.orderBy(\"table_name\", \"name\"), n=1000, truncate=False)\n",
    "        return {\n",
    "            \"mode\": \"dry_run\",\n",
    "            \"config_path\": output_config_path,\n",
    "            \"rules_files\": len(yaml_files),\n",
    "            \"rules_pre_dedupe\": pre_dedupe,\n",
    "            \"rules_post_dedupe\": post_dedupe,\n",
    "            \"unique_check_ids\": unique_check_ids,\n",
    "            \"distinct_rule_run_pairs\": distinct_pairs,\n",
    "            \"target_table\": delta_table_name,\n",
    "            \"wrote_rows\": 0,\n",
    "        }\n",
    "\n",
    "    # ALWAYS OVERWRITE on each run\n",
    "    doc = _materialize_table_doc(table_doc or DQX_CHECKS_CONFIG_METADATA, delta_table_name)\n",
    "    overwrite_rules_into_delta(spark, df, delta_table_name, table_doc=doc)\n",
    "    wrote_rows = df.count()\n",
    "    print(f\"{Color.b}{Color.ivory}Finished writing rules to '{Color.r}{Color.b}{Color.i}{Color.sea_green}{delta_table_name}{Color.r}{Color.b}{Color.ivory}' (overwrite){Color.r}.\")\n",
    "\n",
    "    return {\n",
    "        \"mode\": \"overwrite\",\n",
    "        \"config_path\": output_config_path,\n",
    "        \"rules_files\": len(yaml_files),\n",
    "        \"rules_pre_dedupe\": pre_dedupe,\n",
    "        \"rules_post_dedupe\": post_dedupe,\n",
    "        \"unique_check_ids\": unique_check_ids,\n",
    "        \"distinct_rule_run_pairs\": distinct_pairs,\n",
    "        \"target_table\": delta_table_name,\n",
    "        \"wrote_rows\": wrote_rows,\n",
    "        \"constraint\": \"pk_check_id\",\n",
    "    }\n",
    "\n",
    "def load_checks(\n",
    "    dqx_cfg_yaml: str = \"resources/dqx_config.yaml\",\n",
    "    created_by: str = \"AdminUser\",\n",
    "    time_zone: str = \"America/Chicago\",\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False,\n",
    "    batch_dedupe_mode: str = \"warn\",\n",
    "    table_doc: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Notebook-friendly entrypoint mirroring 02_run_dqx_checks.run_checks(...).\n",
    "    \"\"\"\n",
    "    return main(\n",
    "        output_config_path=dqx_cfg_yaml,\n",
    "        rules_dir=None,\n",
    "        time_zone=time_zone,\n",
    "        dry_run=dry_run,\n",
    "        validate_only=validate_only,\n",
    "        required_fields=None,\n",
    "        batch_dedupe_mode=batch_dedupe_mode,\n",
    "        table_doc=table_doc,\n",
    "        created_by=created_by,\n",
    "    )\n",
    "\n",
    "# ---- run it ----\n",
    "res = load_checks(\n",
    "    dqx_cfg_yaml=\"resources/dqx_config.yaml\",\n",
    "    created_by=\"AdminUser\",\n",
    "    # dry_run=True,\n",
    "    # validate_only=True,\n",
    "    batch_dedupe_mode=\"warn\",\n",
    ")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
