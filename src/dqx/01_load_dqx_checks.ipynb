{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load DQX Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff95a1d5-3010-440c-a60f-e8b4c7fcc84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbe9a89-330c-4868-9766-f17332b0f64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Runbook — `01_load_dqx_checks` (DQX Rule Loader)\n",
    "\n",
    "**Purpose**  \n",
    "Load and validate DQX rules defined in YAML, canonicalize them into a stable `check_id`, and overwrite the target **rules catalog** table with helpful metadata, comments, and a PK declaration. Designed for Databricks + Unity Catalog.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "**Standard run**\n",
    "```python\n",
    "main()\n",
    "```\n",
    "\n",
    "**Dry run (no write)**\n",
    "```python\n",
    "main(dry_run=True)\n",
    "```\n",
    "\n",
    "**Validate only (no write)**\n",
    "```python\n",
    "main(validate_only=True)\n",
    "```\n",
    "\n",
    "**Common overrides**\n",
    "```python\n",
    "main(\n",
    "  output_config_path=\"resources/dqx_config.yaml\",\n",
    "  rules_dir=None,                          # else pulled from config\n",
    "  batch_dedupe_mode=\"warn\",                # warn | error | skip\n",
    "  time_zone=\"America/Chicago\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs & Outputs\n",
    "\n",
    "**Config (`resources/dqx_config.yaml`)**\n",
    "- `dqx_yaml_checks`: folder with YAML files (recursive; supports `dbfs:/` via `/dbfs/...` bridge)\n",
    "- `dqx_checks_config_table_name`: fully qualified UC table to overwrite, e.g. `dq_dev.dqx.checks`\n",
    "\n",
    "**YAML rule shape (per rule)**\n",
    "- Required: `table_name` (catalog.schema.table), `name`, `criticality` (`warn|warning|error`), `run_config_name`, `check.function`\n",
    "- Optional: `check.for_each_column` (list of strings), `check.arguments` (map; values stringified), `filter`, `user_metadata`, `active` (default `true`)\n",
    "\n",
    "**Output table (overwrite)**\n",
    "- Schema includes: `check_id` (PRIMARY KEY by design), `check_id_payload`, `table_name`, `name`, `criticality`, `check{function,for_each_column,arguments}`, `filter`, `run_config_name`, `user_metadata`, `yaml_path`, `active`, `created_by/at`, `updated_by/at`.\n",
    "- **Metadata**: table & column **comments** are applied.\n",
    "- **Constraint**: attempts `ALTER TABLE ... ADD CONSTRAINT pk_check_id PRIMARY KEY (check_id) RELY` (Unity Catalog).\n",
    "- **Guardrail**: hard **runtime assertion** ensures `check_id` uniqueness (fails the run if duplicates exist).\n",
    "\n",
    "---\n",
    "\n",
    "## What the Notebook Does (High Level)\n",
    "\n",
    "1) **Environment banner** — prints cluster/warehouse info with local timezone.  \n",
    "2) **Read config** — resolves `rules_dir` and `delta_table_name`.  \n",
    "3) **Discover YAMLs** — recursively finds `*.yaml|*.yml` under `rules_dir` (including nested folders).  \n",
    "4) **Validate (optional short-circuit)** — per file + per rule + **DQEngine.validate_checks**.  \n",
    "5) **Load & canonicalize** — normalizes `filter`, sorts `for_each_column`, stringifies `arguments`, builds canonical JSON payload, computes `check_id=sha256(payload)`.  \n",
    "6) **Batch de-dupe by `check_id`** — keep the lexicographically first (by `yaml_path`, `name`); mode: `warn|error|skip`.  \n",
    "7) **Diagnostics** — displays totals, sample rules, and counts by table.  \n",
    "8) **Overwrite target table** — writes Delta with `overwriteSchema=true` and timestamp casting.  \n",
    "9) **Apply documentation** — table comment (fallback to TBLPROPERTIES) + column comments (with ALTER COLUMN; fallback to COMMENT ON COLUMN).  \n",
    "10) **PK & enforcement** — add/refresh **informational PK** on `check_id` (if UC) and **assert uniqueness** at runtime.  \n",
    "11) **Result summary** — small confirmation table and a printed success line.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters (function `main`)\n",
    "\n",
    "- `output_config_path`: YAML path for loader config (default `resources/dqx_config.yaml`).  \n",
    "- `rules_dir`: override rules folder (else from config).  \n",
    "- `time_zone`: used for audit timestamps.  \n",
    "- `dry_run`: if `True`, loads + displays but **does not write**.  \n",
    "- `validate_only`: if `True`, runs validations and **exits**.  \n",
    "- `required_fields`: override minimal YAML fields (default set used).  \n",
    "- `batch_dedupe_mode`: `warn` (print & keep first) | `error` (fail) | `skip` (no dedupe).  \n",
    "- `table_doc`: optional dict to override default table/column comments.\n",
    "\n",
    "---\n",
    "\n",
    "## Prereqs & Permissions\n",
    "\n",
    "- Python deps: `pyspark`, `pyyaml`, `databricks-labs-dqx (0.8.x)` available on the cluster/warehouse.  \n",
    "- Data access: **READ** on `rules_dir` (workspace file system or `dbfs:/`), **CREATE/ALTER** on target catalog & schema, and **CREATE TABLE / ALTER TABLE** privileges to set comments/constraints.  \n",
    "- Unity Catalog strongly recommended (for PK declaration). The run still proceeds if constraint DDL is not supported; a message is logged.\n",
    "\n",
    "---\n",
    "\n",
    "## Failure Modes & Fixes\n",
    "\n",
    "- **Validation failed** (missing required fields, invalid `criticality`, `table_name` not fully qualified, DQEngine errors): fix YAML and re-run.  \n",
    "- **Duplicate `check_id`** during de-dupe or final uniqueness assertion: adjust YAML so canonical payloads differ (e.g., rule name is *not* part of identity).  \n",
    "- **Insufficient privileges** during write/comments/constraint: grant `CREATE/ALTER` on target schema/table, or run with a service principal that has the rights.  \n",
    "- **Column comments not applied**: only columns present in the table are touched; fallback DDL is attempted and any skips are logged.\n",
    "\n",
    "---\n",
    "\n",
    "## Post‑Run Sanity Checks (SQL)\n",
    "\n",
    "```sql\n",
    "-- Table present & counts\n",
    "SELECT COUNT(*) AS rules, COUNT(DISTINCT check_id) AS unique_rules FROM dq_dev.dqx.checks;\n",
    "\n",
    "-- Any dup check_id? (should be 0 rows)\n",
    "SELECT check_id, COUNT(*) c FROM dq_dev.dqx.checks GROUP BY check_id HAVING COUNT(*)>1;\n",
    "\n",
    "-- Peek arguments & coverage\n",
    "SELECT name, check.function, check.arguments FROM dq_dev.dqx.checks LIMIT 20;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Overwrite semantics: this is the **system of record** for rules derived from YAML; manual edits in the table will be lost on next run.  \n",
    "- `check_id` identity includes only `{table_name↓, filter, check.*}` — **not** `name`, `criticality`, or `run_config_name`.  \n",
    "- Argument values are persisted as strings for stability; cast/parse (e.g., `try_cast`, `from_json`) downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5042f1e5-f5d4-4dda-a0b8-3e9130961893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FLOW"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START: main (01_load_dqx_checks)\n",
    "|\n",
    "|-- 0. Environment banner\n",
    "|     |-- print_notebook_env(spark, local_timezone)\n",
    "|\n",
    "|-- 1. Load output config\n",
    "|     |-- output_config = yaml(resources/dqx_config.yaml)\n",
    "|     |-- rules_dir = output_config[\"dqx_yaml_checks\"]\n",
    "|     |-- delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "|     |-- required_fields = [\"table_name\",\"name\",\"criticality\",\"run_config_name\",\"check\"]\n",
    "|\n",
    "|-- 2. Recursively discover YAMLs\n",
    "|     |-- files = discover_yaml_files_recursive(rules_dir)  # supports dbfs:/ via /dbfs bridge\n",
    "|     |-- DISPLAY: list of YAML paths\n",
    "|\n",
    "|-- 3. Validation-only short-circuit (if validate_only=True)\n",
    "|     |-- For each file:\n",
    "|     |     |-- load_yaml_rules(file)  # supports multi-doc YAML, flattens lists\n",
    "|     |     |-- File-level checks: non-empty; no duplicate rule names\n",
    "|     |     |-- For each rule:\n",
    "|     |     |     |-- Rule-level checks:\n",
    "|     |     |     |     |-- table_name fully qualified (catalog.schema.table)\n",
    "|     |     |     |     |-- criticality ∈ {warn, warning, error}\n",
    "|     |     |     |     |-- check.function exists\n",
    "|     |     |     |-- DQEngine.validate_checks(docs) must pass\n",
    "|     |-- PRINT summary; END\n",
    "|\n",
    "|-- 4. Load + flatten rules (normal path)\n",
    "|     |-- all_rules = []\n",
    "|     |-- For each file in files:\n",
    "|           |-- docs = load_yaml_rules(file)\n",
    "|           |-- File-level checks (as above)\n",
    "|           |-- now = current_time_iso(time_zone)\n",
    "|           |-- For each rule in docs:\n",
    "|                 |-- Rule-level checks (as above)\n",
    "|                 |-- Canonicalize:\n",
    "|                 |     |-- filter := normalized whitespace (or \"\")\n",
    "|                 |     |-- check := {\n",
    "|                 |            function,\n",
    "|                 |            for_each_column := sorted list or None (validated as list[str]),\n",
    "|                 |            arguments := map with all values stringified\n",
    "|                 |        }\n",
    "|                 |     |-- user_metadata := map with stringified values (or None)\n",
    "|                 |-- payload := JSON over {table_name_lower, filter, canonical check} (sorted keys, tight separators)\n",
    "|                 |-- check_id := sha256(payload)\n",
    "|                 |-- Append flattened dict:\n",
    "|                 |     |-- keys: [check_id, check_id_payload, table_name, name, criticality,\n",
    "|                 |                check{function, for_each_column, arguments}, filter, run_config_name,\n",
    "|                 |                user_metadata, yaml_path=file, active (default True),\n",
    "|                 |                created_by=\"AdminUser\", created_at=now, updated_by=None, updated_at=None]\n",
    "|           |-- DQEngine.validate_checks(docs) must pass (file-level)\n",
    "|           |-- PRINT \"[loader] {file}: rules={n}\"\n",
    "|\n",
    "|-- 5. Batch de-duplication (by check_id only)\n",
    "|     |-- Group all_rules by check_id\n",
    "|     |-- Keep the lexicographically first (yaml_path, name); drop others\n",
    "|     |-- Mode:\n",
    "|           |-- \"warn\" (default): print detailed duplicate blocks\n",
    "|           |-- \"error\": raise\n",
    "|           |-- \"skip\": keep all (no drops)\n",
    "|\n",
    "|-- 6. Assemble DataFrame + display-first diagnostics\n",
    "|     |-- df := spark.createDataFrame(all_rules, schema=TABLE_SCHEMA)\n",
    "|     |-- DISPLAY: totals (count, distinct check_id, distinct (check_id, run_config_name))\n",
    "|     |-- DISPLAY: sample(check_id, name, run_config_name, yaml_path) ordered by yaml_path desc\n",
    "|     |-- DISPLAY: rules per table_name\n",
    "|     |-- (If very small) DISPLAY: first 3 payloads\n",
    "|\n",
    "|-- 7. Dry-run short-circuit (if dry_run=True)\n",
    "|     |-- DISPLAY: full rules preview ordered by (table_name, name)\n",
    "|     |-- END\n",
    "|\n",
    "|-- 8. Overwrite Delta target table (ALWAYS overwrite)\n",
    "|     |-- existed_before := spark.catalog.tableExists(delta_table_name)\n",
    "|     |-- ensure_schema_exists(catalog.schema)\n",
    "|     |-- Cast timestamps: created_at/updated_at := to_timestamp\n",
    "|     |-- df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(delta_table_name)\n",
    "|\n",
    "|-- 9. Apply documentation metadata (table + columns)\n",
    "|     |-- doc := materialize DQX_CHECKS_CONFIG_METADATA with {TABLE_FQN}=delta_table_name\n",
    "|     |-- Apply table comment (COMMENT ON TABLE; fallback to TBLPROPERTIES)\n",
    "|     |-- If created_now (i.e., !existed_before):\n",
    "|           |-- For each existing column: ALTER TABLE ... ALTER COLUMN ... COMMENT\n",
    "|     |-- DISPLAY: preview of table comment + column comments\n",
    "|\n",
    "|-- 10. Write result summary\n",
    "|     |-- DISPLAY: single-row summary (rules written, target table)\n",
    "|     |-- PRINT: confirmation line with target table\n",
    "|\n",
    "END: main\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74936d7c-4414-42fe-a143-cb6c7d364d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1e26e-12cc-44e8-ae5a-e7b1fa4d5049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1154f87-29b1-4cb5-8a2f-ff0737aa9cda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40761988-2989-4a37-8b9e-e4b441e97bce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 01_load_dqx_checks\n",
    "# Purpose: Load YAML rules into dq_{env}.dqx.checks_config\n",
    "# Requires: databricks-labs-dqx==0.8.x\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, yaml, hashlib\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, types as T, functions as F\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "# utils.* only (per requirement)\n",
    "from utils.display import show_df, display_section\n",
    "from utils.runtime import show_notebook_env\n",
    "from utils.color import Color\n",
    "from utils.console import Console\n",
    "from utils.config import ProjectConfig, ConfigError\n",
    "from utils.write import TableWriter\n",
    "from utils.table import table_exists\n",
    "from utils.path import dbfs_to_local, list_yaml_files\n",
    "\n",
    "# =========================\n",
    "# SPARK STRUCTURED SCHEMA (source of truth)\n",
    "# =========================\n",
    "CHECKS_CONFIG_STRUCT = T.StructType([\n",
    "    T.StructField(\"check_id\",         T.StringType(),   False, {\"comment\": \"PRIMARY KEY. Stable sha256 over canonical {table_name↓, filter, check.*}.\"}),\n",
    "    T.StructField(\"check_id_payload\", T.StringType(),   False, {\"comment\": \"Canonical JSON used to derive `check_id` (sorted keys, normalized values).\"}),\n",
    "    T.StructField(\"table_name\",       T.StringType(),   False, {\"comment\": \"Target table FQN (`catalog.schema.table`). Lowercased in payload for stability.\"}),\n",
    "    T.StructField(\"name\",             T.StringType(),   False, {\"comment\": \"Human-readable rule name. Used in UI/diagnostics and joins.\"}),\n",
    "    T.StructField(\"criticality\",      T.StringType(),   False, {\"comment\": \"Rule severity: `error|warn`.\"}),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False, {\"comment\": \"DQX function to run\"}),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True,  {\"comment\": \"Optional list of columns\"}),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True, {\"comment\": \"Key/value args\"}),\n",
    "    ]), False, {\"comment\": \"Structured rule `{function, for_each_column?, arguments?}`; values stringified.\"}),\n",
    "    T.StructField(\"filter\",           T.StringType(),   True,  {\"comment\": \"Optional SQL predicate applied before evaluation (row-level).\"}),\n",
    "    T.StructField(\"run_config_name\",  T.StringType(),   False, {\"comment\": \"Execution group/tag. Not part of identity.\"}),\n",
    "    T.StructField(\"user_metadata\",    T.MapType(T.StringType(), T.StringType()), True, {\"comment\": \"Free-form map<string,string>.\"}),\n",
    "    T.StructField(\"yaml_path\",        T.StringType(),   False, {\"comment\": \"Absolute/volume path to the defining YAML doc (lineage).\"}),\n",
    "    T.StructField(\"active\",           T.BooleanType(),  False, {\"comment\": \"If `false`, rule is ignored by runners.\"}),\n",
    "    T.StructField(\"created_by\",       T.StringType(),   False, {\"comment\": \"Audit: creator/principal that materialized the row.\"}),\n",
    "    T.StructField(\"created_at\",       T.TimestampType(),False, {\"comment\": \"Audit: creation timestamp (UTC).\"}),\n",
    "    T.StructField(\"updated_by\",       T.StringType(),   True,  {\"comment\": \"Audit: last updater (nullable).\"}),\n",
    "    T.StructField(\"updated_at\",       T.TimestampType(),True,  {\"comment\": \"Audit: last update timestamp (UTC, nullable).\"}),\n",
    "])\n",
    "\n",
    "# Optional: overrides without touching the struct\n",
    "CHECKS_CONFIG_COMMENTS: Dict[str, str] = {\n",
    "    # \"check_id\": \"Override comment here\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Small local helpers (no dependency on must() inside utils.config)\n",
    "# =========================\n",
    "def must(val: Any, name: str) -> Any:\n",
    "    if val is None or (isinstance(val, str) and not val.strip()):\n",
    "        raise ConfigError(f\"Missing required config: {name}\")\n",
    "    return val\n",
    "\n",
    "# =========================\n",
    "# Canonicalization & IDs\n",
    "# =========================\n",
    "def _canon_filter(s: Optional[str]) -> str:\n",
    "    return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "    fec = chk.get(\"for_each_column\")\n",
    "    if isinstance(fec, list):\n",
    "        out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "    args = chk.get(\"arguments\") or {}\n",
    "    canon_args: Dict[str, str] = {}\n",
    "    for k, v in args.items():\n",
    "        sv = \"\" if v is None else str(v).strip()\n",
    "        if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "            try:\n",
    "                sv = json.dumps(json.loads(sv), sort_keys=True, separators=(\",\", \":\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        canon_args[str(k)] = sv\n",
    "    out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "    return out\n",
    "\n",
    "def compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    payload_obj = {\"table_name\": (table_name or \"\").lower(), \"filter\": _canon_filter(filter_str), \"check\": _canon_check(check_dict or {})}\n",
    "    return json.dumps(payload_obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def compute_check_id_from_payload(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode()).hexdigest()\n",
    "\n",
    "# =========================\n",
    "# Rule YAML load/validate\n",
    "# =========================\n",
    "def load_yaml_rules(path: str) -> List[dict]:\n",
    "    p = Path(dbfs_to_local(path))\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Rules YAML not found: {p}\")\n",
    "    with open(p, \"r\") as fh:\n",
    "        docs = list(yaml.safe_load_all(fh)) or []\n",
    "    out: List[dict] = []\n",
    "    for d in docs:\n",
    "        if not d:\n",
    "            continue\n",
    "        if isinstance(d, dict):\n",
    "            out.append(d)\n",
    "        elif isinstance(d, list):\n",
    "            out.extend([x for x in d if isinstance(x, dict)])\n",
    "    return out\n",
    "\n",
    "def validate_rules_file(rules: List[dict], file_path: str):\n",
    "    if not rules:\n",
    "        raise ValueError(f\"No rules found in {file_path} (empty or invalid YAML).\")\n",
    "    probs, seen = [], set()\n",
    "    for r in rules:\n",
    "        nm = r.get(\"name\")\n",
    "        if not nm: probs.append(f\"Missing rule name in {file_path}\")\n",
    "        if nm in seen: probs.append(f\"Duplicate rule name '{nm}' in {file_path}\")\n",
    "        seen.add(nm)\n",
    "    if probs: raise ValueError(f\"File-level validation failed in {file_path}: {probs}\")\n",
    "\n",
    "def validate_rule_fields(rule: dict, file_path: str, required_fields: List[str], allowed_criticality: List[str]):\n",
    "    probs = []\n",
    "    for f in required_fields:\n",
    "        if not rule.get(f): probs.append(f\"Missing required field '{f}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        probs.append(f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"criticality\") not in set(allowed_criticality):\n",
    "        probs.append(f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        probs.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if probs: raise ValueError(\"Rule-level validation failed: \" + \"; \".join(probs))\n",
    "\n",
    "def validate_with_dqx(rules: List[dict], file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if getattr(status, \"has_errors\", False):\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "# =========================\n",
    "# Build rows from YAML docs (UTC timestamps)\n",
    "# =========================\n",
    "def process_yaml_file(\n",
    "    path: str,\n",
    "    required_fields: List[str],\n",
    "    created_by_value: str,\n",
    "    allowed_criticality: List[str],\n",
    ") -> List[dict]:\n",
    "    docs = load_yaml_rules(path)\n",
    "    if not docs:\n",
    "        print(f\"{Console.SKIP} {path} has no rules.\")\n",
    "        return []\n",
    "\n",
    "    validate_rules_file(docs, path)\n",
    "    flat: List[dict] = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path, required_fields, allowed_criticality)\n",
    "        raw_check = rule[\"check\"] or {}\n",
    "        payload   = compute_check_id_payload(rule[\"table_name\"], raw_check, rule.get(\"filter\"))\n",
    "        check_id  = compute_check_id_from_payload(payload)\n",
    "\n",
    "        function = raw_check.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\")\n",
    "\n",
    "        for_each = raw_check.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\")\n",
    "        for_each = [str(x) for x in (for_each or [])] or None\n",
    "\n",
    "        arguments = raw_check.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        created_at_ts = datetime.utcnow()\n",
    "\n",
    "        flat.append({\n",
    "            \"check_id\": check_id,\n",
    "            \"check_id_payload\": payload,\n",
    "            \"table_name\": rule[\"table_name\"],\n",
    "            \"name\": rule[\"name\"],\n",
    "            \"criticality\": rule[\"criticality\"],\n",
    "            \"check\": {\"function\": function, \"for_each_column\": for_each, \"arguments\": arguments or None},\n",
    "            \"filter\": rule.get(\"filter\"),\n",
    "            \"run_config_name\": rule[\"run_config_name\"],\n",
    "            \"user_metadata\": user_metadata or None,\n",
    "            \"yaml_path\": path,\n",
    "            \"active\": bool(rule.get(\"active\", True)),\n",
    "            \"created_by\": \"AdminUser\",\n",
    "            \"created_at\": created_at_ts,\n",
    "            \"updated_by\": None,\n",
    "            \"updated_at\": None,\n",
    "        })\n",
    "\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat\n",
    "\n",
    "# =========================\n",
    "# Dedupe (on check_id)\n",
    "# =========================\n",
    "def _fmt_rule_for_dup(r: dict) -> str:\n",
    "    return f\"name={r.get('name')} | file={r.get('yaml_path')} | criticality={r.get('criticality')} | run_config={r.get('run_config_name')} | filter={r.get('filter')}\"\n",
    "\n",
    "def dedupe_rules_in_batch_by_check_id(rules: List[dict], mode: str) -> List[dict]:\n",
    "    groups: Dict[str, List[dict]] = {}\n",
    "    for r in rules: groups.setdefault(r[\"check_id\"], []).append(r)\n",
    "    out: List[dict] = []; dropped = 0; blocks: List[str] = []\n",
    "    for cid, lst in groups.items():\n",
    "        if len(lst) == 1:\n",
    "            out.append(lst[0]); continue\n",
    "        lst = sorted(lst, key=lambda x: (x.get(\"yaml_path\",\"\"), x.get(\"name\",\"\")))\n",
    "        keep, dups = lst[0], lst[1:]; dropped += len(dups)\n",
    "        head = f\"{Console.DEDUPE} {len(dups)} duplicate(s) for check_id={cid[:12]}…\"\n",
    "        lines = [\"    \" + _fmt_rule_for_dup(x) for x in lst]\n",
    "        tail = f\"    -> keeping: name={keep.get('name')} | file={keep.get('yaml_path')}\"\n",
    "        blocks.append(\"\\n\".join([head, *lines, tail])); out.append(keep)\n",
    "    if dropped:\n",
    "        msg = \"\\n\\n\".join(blocks) + f\"\\n{Console.DEDUPE} total dropped={dropped}\"\n",
    "        if mode == \"error\": raise ValueError(msg)\n",
    "        if mode == \"warn\": print(msg)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Discover rule YAMLs\n",
    "# =========================\n",
    "def discover_yaml(cfg: ProjectConfig, rules_dir: str) -> List[str]:\n",
    "    print(f\"{Console.DEBUG} rules_dir (raw from YAML): {rules_dir}\")\n",
    "    files = list_yaml_files(rules_dir)\n",
    "    display_section(\"YAML FILES DISCOVERED (recursive)\")\n",
    "    df = SparkSession.builder.getOrCreate().createDataFrame([(p,) for p in files], \"yaml_path string\")\n",
    "    show_df(df, n=500, truncate=False)\n",
    "    return files\n",
    "\n",
    "# =========================\n",
    "# Build DF\n",
    "# =========================\n",
    "def build_df_from_rules(spark: SparkSession, rules: List[dict]) -> DataFrame:\n",
    "    return spark.createDataFrame(rules, schema=CHECKS_CONFIG_STRUCT)\n",
    "\n",
    "# =========================\n",
    "# Runner\n",
    "# =========================\n",
    "def run_checks_loader(\n",
    "    spark: SparkSession,\n",
    "    cfg: ProjectConfig,\n",
    "    *,\n",
    "    notebook_idx: int,\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    apply_meta  = bool(must(cfg.get(\"variables.apply_table_metadata\"), \"variables.apply_table_metadata\"))\n",
    "    dedupe_mode = must(cfg.get(\"variables.batch_dedupe_mode\"), \"variables.batch_dedupe_mode\")\n",
    "\n",
    "    nb = cfg.notebook(notebook_idx)\n",
    "\n",
    "    # Use data_sources.data_source_1\n",
    "    ds = nb.data_sources().data_source(1)\n",
    "    rules_dir       = must(ds.get(\"source_path\"),         f\"notebooks.notebook_{notebook_idx}.data_sources.data_source_1.source_path\")\n",
    "    allowed_crit    = must(ds.get(\"allowed_criticality\"), f\"notebooks.notebook_{notebook_idx}.data_sources.data_source_1.allowed_criticality\")\n",
    "    required_fields = must(ds.get(\"required_fields\"),     f\"notebooks.notebook_{notebook_idx}.data_sources.data_source_1.required_fields\")\n",
    "\n",
    "    # Target (checks_config)\n",
    "    t = nb.targets().target_table(1)\n",
    "    fqn           = t.full_table_name()\n",
    "    partition_by  = t.get(\"partition_by\") or []\n",
    "    write_block   = must(t.get(\"write\"), f\"{fqn}.write\")\n",
    "    table_comment = t.get(\"table_description\")\n",
    "    primary_key   = must(t.get(\"primary_key\"), f\"{fqn}.primary_key\")   # from YAML\n",
    "    table_tags    = t.table_tags()                                     # normalized in config.py\n",
    "\n",
    "    tw = TableWriter(spark)\n",
    "\n",
    "    # Create if needed (no unsupported create modes)\n",
    "    if not table_exists(spark, fqn):\n",
    "        tw.create_table(\n",
    "            fqn=fqn,\n",
    "            schema=CHECKS_CONFIG_STRUCT,\n",
    "            format=must(write_block.get(\"format\"), f\"{fqn}.write.format\"),\n",
    "            options=write_block.get(\"options\") or {},\n",
    "            partition_by=partition_by,\n",
    "            table_comment=table_comment,\n",
    "            column_comments=(CHECKS_CONFIG_COMMENTS or None),\n",
    "            table_properties=None,          # add if you map something to TBLPROPERTIES\n",
    "            table_tags=table_tags,          # tags from YAML (flat dict)\n",
    "            column_tags=None,               # per-column tags if you maintain them\n",
    "            primary_key_cols=[primary_key], # PK columns only; name auto-generated\n",
    "        )\n",
    "\n",
    "    yaml_files = discover_yaml(cfg, rules_dir)\n",
    "\n",
    "    if validate_only:\n",
    "        print(f\"{Console.VALIDATION} Validation only: not writing any rules.\")\n",
    "        errs: List[str] = []\n",
    "        for p in yaml_files:\n",
    "            try:\n",
    "                validate_rules_file(load_yaml_rules(p), p)\n",
    "            except Exception as e:\n",
    "                errs.append(f\"{p}: {e}\")\n",
    "        return {\"config_path\": cfg.path, \"rules_files\": len(yaml_files), \"errors\": errs}\n",
    "\n",
    "    # Build rows (UTC created_at), dedupe by check_id\n",
    "    all_rules: List[dict] = []\n",
    "    for full_path in yaml_files:\n",
    "        file_rules = process_yaml_file(\n",
    "            full_path,\n",
    "            required_fields=required_fields,\n",
    "            created_by_value=\"AdminUser\",\n",
    "            allowed_criticality=allowed_crit,\n",
    "        )\n",
    "        if file_rules:\n",
    "            all_rules.extend(file_rules)\n",
    "            print(f\"{Console.LOADER} {full_path}: rules={len(file_rules)}\")\n",
    "\n",
    "    pre_dedupe = len(all_rules)\n",
    "    rules = dedupe_rules_in_batch_by_check_id(all_rules, mode=dedupe_mode)\n",
    "    post_dedupe = len(rules)\n",
    "\n",
    "    if not rules:\n",
    "        print(f\"{Console.SKIP} No rules discovered; nothing to do.\")\n",
    "        return {\"config_path\": cfg.path, \"rules_files\": len(yaml_files), \"wrote_rows\": 0, \"target_table\": fqn}\n",
    "\n",
    "    print(f\"{Console.DEDUPE} total parsed rules (pre-dedupe): {pre_dedupe}\")\n",
    "    df = build_df_from_rules(spark, rules)\n",
    "\n",
    "    display_section(\"SUMMARY OF RULES LOADED FROM YAML\")\n",
    "    totals = [(df.count(), df.select(\"check_id\").distinct().count(), df.select(\"check_id\", \"run_config_name\").distinct().count())]\n",
    "    tdf = df.sparkSession.createDataFrame(\n",
    "        totals,\n",
    "        schema=\"`total number of rules found` long, `unique rules found` long, `distinct pair of rules` long\",\n",
    "    )\n",
    "    show_df(tdf, n=1)\n",
    "\n",
    "    if dry_run:\n",
    "        display_section(\"DRY-RUN: FULL RULES PREVIEW\")\n",
    "        show_df(df.orderBy(\"table_name\", \"name\"), n=1000, truncate=False)\n",
    "        return {\n",
    "            \"config_path\": cfg.path,\n",
    "            \"rules_files\": len(yaml_files),\n",
    "            \"rules_pre_dedupe\": pre_dedupe,\n",
    "            \"rules_post_dedupe\": post_dedupe,\n",
    "            \"unique_check_ids\": df.select(\"check_id\").distinct().count(),\n",
    "            \"distinct_rule_run_pairs\": df.select(\"check_id\",\"run_config_name\").distinct().count(),\n",
    "            \"target_table\": fqn,\n",
    "            \"wrote_rows\": 0,\n",
    "            \"write_mode\": must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"),\n",
    "        }\n",
    "\n",
    "    # Write (project keeps column order via target table schema)\n",
    "    tw.write_df(\n",
    "        df=df.select(*[f.name for f in spark.table(fqn).schema.fields]),\n",
    "        fqn=fqn,\n",
    "        mode=must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"),\n",
    "        format=must(write_block.get(\"format\"), f\"{fqn}.write.format\"),\n",
    "        options=write_block.get(\"options\") or {},\n",
    "    )\n",
    "\n",
    "    wrote_rows = df.count()\n",
    "    display_section(\"WRITE RESULT\")\n",
    "    summary = spark.createDataFrame([(wrote_rows, fqn, must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"))],\n",
    "                                    schema=\"`rules written` long, `target table` string, `mode` string\")\n",
    "    show_df(summary, n=1)\n",
    "    print(f\"{Color.b}{Color.ivory}Finished writing rules to '{Color.r}{Color.b}{Color.i}{Color.sea_green}{fqn}{Color.r}{Color.b}{Color.ivory}'{Color.r}.\")\n",
    "\n",
    "    return {\n",
    "        \"config_path\": cfg.path,\n",
    "        \"rules_files\": len(yaml_files),\n",
    "        \"rules_pre_dedupe\": pre_dedupe,\n",
    "        \"rules_post_dedupe\": post_dedupe,\n",
    "        \"unique_check_ids\": df.select(\"check_id\").distinct().count(),\n",
    "        \"distinct_rule_run_pairs\": df.select(\"check_id\",\"run_config_name\").distinct().count(),\n",
    "        \"target_table\": fqn,\n",
    "        \"wrote_rows\": wrote_rows,\n",
    "        \"write_mode\": must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Entrypoint (local/dev)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    show_notebook_env(spark)\n",
    "    cfg = ProjectConfig(\"resources/dqx_config.yaml\", variables={})\n",
    "    result = run_checks_loader(spark, cfg, notebook_idx=1, dry_run=False, validate_only=False)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e17589a-b2b7-4124-9b04-e218136401b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
