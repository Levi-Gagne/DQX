{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load DQX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5042f1e5-f5d4-4dda-a0b8-3e9130961893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flow"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START: update_dqx_rules_table\n",
    "|\n",
    "|-- 1. Check if Delta config table exists:\n",
    "|     |-- If table does NOT exist:\n",
    "|     |     |-- Create empty Delta table using TABLE_SCHEMA.\n",
    "|     |\n",
    "|     |-- If table DOES exist:\n",
    "|           |-- Proceed to next step.\n",
    "|\n",
    "|-- 2. For each YAML file in rules_dir:\n",
    "|     |-- Parse file (YAML load).\n",
    "|     |-- FILE-LEVEL validation (all rules in file target same table; no dup rule names; filename matches table name).\n",
    "|     |-- For each rule:\n",
    "|           |-- RULE-LEVEL validation (required fields, format, criticality, etc).\n",
    "|     |-- DQX syntax validation (DQEngine.validate_checks).\n",
    "|     |-- For each rule:\n",
    "|           |-- Extract run_config_name, look up valid_target_table and quarantine_target_table from output config.\n",
    "|           |-- Flatten and collect rule as dict (with hash_id, audit fields, etc).\n",
    "|\n",
    "|-- 3. Combine all flattened rules into one list.\n",
    "|\n",
    "|-- 4. Upsert all rules into Delta table:\n",
    "|     |-- If entry exists (yaml_path, table_name, name): UPDATE all fields except created_at/created_by.\n",
    "|     |-- If entry missing: INSERT, set created_at=now(UTC), created_by='admin', updated_at/updated_by=None.\n",
    "|\n",
    "|-- END: update_dqx_rules_table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1e26e-12cc-44e8-ae5a-e7b1fa4d5049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1154f87-29b1-4cb5-8a2f-ff0737aa9cda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47cd4da-0202-4d25-ba08-5d5a39768bdd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755036534235}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755039593521}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 01_load_dqx_checks  (DISPLAY-FIRST + METADATA, SAFE COMMENTS, RECURSIVE YAML DISCOVERY)\n",
    "# - Uses display() for clean tables\n",
    "# - Clearer section titles/column names\n",
    "# - Applies TABLE comment always (markdown), COLUMN comments only on first create\n",
    "# - Uses ALTER TABLE ... ALTER COLUMN COMMENT to avoid parser issues\n",
    "# - Shows a markdown-ish preview table of table/column docs\n",
    "# - NEW: Recursively loads *all* .yaml/.yml files under the configured folder (folders inside folders)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import yaml\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, types as T\n",
    "from pyspark.sql.functions import to_timestamp, col, desc\n",
    "\n",
    "# from delta.tables import DeltaTable   # not needed when we always overwrite\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "\n",
    "# ======================================================\n",
    "# Small helpers for Databricks-friendly tabular display\n",
    "# ======================================================\n",
    "\n",
    "def _can_display() -> bool:\n",
    "    return \"display\" in globals()\n",
    "\n",
    "def show_df(df: DataFrame, n: int = 100, truncate: bool = False) -> None:\n",
    "    if _can_display():\n",
    "        display(df.limit(n))\n",
    "    else:\n",
    "        df.show(n, truncate=truncate)\n",
    "\n",
    "def display_section(title: str) -> None:\n",
    "    print(\"\\n\" + \"═\" * 80)\n",
    "    print(f\"║ {title}\")\n",
    "    print(\"═\" * 80)\n",
    "\n",
    "# =========================\n",
    "# Target schema (Delta sink)\n",
    "# =========================\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"check_id\",            T.StringType(), False),  # sha256 over canonical payload\n",
    "    T.StructField(\"check_id_payload\",    T.StringType(), False),  # canonical JSON used to compute check_id\n",
    "    T.StructField(\"table_name\",          T.StringType(), False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",                T.StringType(), False),\n",
    "    T.StructField(\"criticality\",         T.StringType(), False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",              T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\",     T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # Ops fields\n",
    "    T.StructField(\"yaml_path\",           T.StringType(), False),\n",
    "    T.StructField(\"active\",              T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\",          T.StringType(), False),\n",
    "    T.StructField(\"created_at\",          T.StringType(), False),  # ISO string; cast on write\n",
    "    T.StructField(\"updated_by\",          T.StringType(), True),\n",
    "    T.StructField(\"updated_at\",          T.StringType(), True),\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# YAML loading (robust)\n",
    "# =========================\n",
    "def load_yaml_rules(path: str) -> List[dict]:\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = list(yaml.safe_load_all(fh))\n",
    "    out: List[dict] = []\n",
    "    for d in docs:\n",
    "        if d is None:\n",
    "            continue\n",
    "        if isinstance(d, dict):\n",
    "            out.append(d)\n",
    "        elif isinstance(d, list):\n",
    "            out.extend([x for x in d if isinstance(x, dict)])\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Canonicalization & IDs\n",
    "# =========================\n",
    "def _canon_filter(s: Optional[str]) -> str:\n",
    "    return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "    fec = chk.get(\"for_each_column\")\n",
    "    if isinstance(fec, list):\n",
    "        out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "    args = chk.get(\"arguments\") or {}\n",
    "    canon_args: Dict[str, str] = {}\n",
    "    for k, v in args.items():\n",
    "        sv = \"\" if v is None else str(v).strip()\n",
    "        if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "            try:\n",
    "                sv = json.dumps(json.loads(sv), sort_keys=True, separators=(\",\", \":\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        canon_args[str(k)] = sv\n",
    "    out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "    return out\n",
    "\n",
    "def compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    payload_obj = {\n",
    "        \"table_name\": (table_name or \"\").lower(),\n",
    "        \"filter\": _canon_filter(filter_str),\n",
    "        \"check\": _canon_check(check_dict or {}),\n",
    "    }\n",
    "    return json.dumps(payload_obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def compute_check_id_from_payload(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode()).hexdigest()\n",
    "\n",
    "# =========================\n",
    "# Conversions / validation\n",
    "# =========================\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def validate_rules_file(rules: List[dict], file_path: str):\n",
    "    if not rules:\n",
    "        raise ValueError(f\"No rules found in {file_path} (empty or invalid YAML).\")\n",
    "    probs, seen = [], set()\n",
    "    for r in rules:\n",
    "        nm = r.get(\"name\")\n",
    "        if not nm:\n",
    "            probs.append(f\"Missing rule name in {file_path}\")\n",
    "        if nm in seen:\n",
    "            probs.append(f\"Duplicate rule name '{nm}' in {file_path}\")\n",
    "        seen.add(nm)\n",
    "    if probs:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {probs}\")\n",
    "\n",
    "def validate_rule_fields(\n",
    "    rule: dict,\n",
    "    file_path: str,\n",
    "    required_fields: List[str],\n",
    "    allowed_criticality={\"error\", \"warn\", \"warning\"},\n",
    "):\n",
    "    probs = []\n",
    "    for f in required_fields:\n",
    "        if not rule.get(f):\n",
    "            probs.append(f\"Missing required field '{f}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        probs.append(\n",
    "            f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if rule.get(\"criticality\") not in allowed_criticality:\n",
    "        probs.append(\n",
    "            f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        probs.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if probs:\n",
    "        raise ValueError(\"Rule-level validation failed: \" + \"; \".join(probs))\n",
    "\n",
    "def validate_with_dqx(rules: List[dict], file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if getattr(status, \"has_errors\", False):\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "# =========================\n",
    "# Build rows\n",
    "# =========================\n",
    "def process_yaml_file(path: str, required_fields: List[str], time_zone: str = \"UTC\") -> List[dict]:\n",
    "    docs = load_yaml_rules(path)\n",
    "    if not docs:\n",
    "        print(f\"[skip] {path} has no rules (empty/comment-only).\")\n",
    "        return []\n",
    "\n",
    "    validate_rules_file(docs, path)\n",
    "\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat: List[dict] = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path, required_fields=required_fields)\n",
    "\n",
    "        raw_check = rule[\"check\"] or {}\n",
    "        payload = compute_check_id_payload(rule[\"table_name\"], raw_check, rule.get(\"filter\"))\n",
    "        check_id = compute_check_id_from_payload(payload)\n",
    "\n",
    "        function = raw_check.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(\n",
    "                f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\"\n",
    "            )\n",
    "\n",
    "        for_each = raw_check.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(\n",
    "                f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\"\n",
    "            )\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(\n",
    "                    f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\"\n",
    "                )\n",
    "\n",
    "        arguments = raw_check.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(\n",
    "                    f\"{path}: user_metadata must be a map (rule '{rule.get('name')}').\"\n",
    "                )\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        flat.append(\n",
    "            {\n",
    "                \"check_id\": check_id,\n",
    "                \"check_id_payload\": payload,\n",
    "                \"table_name\": rule[\"table_name\"],\n",
    "                \"name\": rule[\"name\"],\n",
    "                \"criticality\": rule[\"criticality\"],\n",
    "                \"check\": {\n",
    "                    \"function\": function,\n",
    "                    \"for_each_column\": for_each if for_each else None,\n",
    "                    \"arguments\": arguments if arguments else None,\n",
    "                },\n",
    "                \"filter\": rule.get(\"filter\"),\n",
    "                \"run_config_name\": rule[\"run_config_name\"],\n",
    "                \"user_metadata\": user_metadata if user_metadata else None,\n",
    "                \"yaml_path\": path,\n",
    "                \"active\": rule.get(\"active\", True),\n",
    "                \"created_by\": \"AdminUser\",\n",
    "                \"created_at\": now,\n",
    "                \"updated_by\": None,\n",
    "                \"updated_at\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat\n",
    "\n",
    "# =========================\n",
    "# Batch dedupe (on check_id ONLY)\n",
    "# =========================\n",
    "def _fmt_rule_for_dup(r: dict) -> str:\n",
    "    return (\n",
    "        f\"name={r.get('name')} | file={r.get('yaml_path')} | \"\n",
    "        f\"criticality={r.get('criticality')} | run_config={r.get('run_config_name')} | \"\n",
    "        f\"filter={r.get('filter')}\"\n",
    "    )\n",
    "\n",
    "def dedupe_rules_in_batch_by_check_id(rules: List[dict], mode: str = \"warn\") -> List[dict]:\n",
    "    groups: Dict[str, List[dict]] = {}\n",
    "    for r in rules:\n",
    "        groups.setdefault(r[\"check_id\"], []).append(r)\n",
    "\n",
    "    out: List[dict] = []\n",
    "    dropped = 0\n",
    "    blocks: List[str] = []\n",
    "\n",
    "    for cid, lst in groups.items():\n",
    "        if len(lst) == 1:\n",
    "            out.append(lst[0])\n",
    "            continue\n",
    "        lst = sorted(lst, key=lambda x: (x.get(\"yaml_path\", \"\"), x.get(\"name\", \"\")))\n",
    "        keep, dups = lst[0], lst[1:]\n",
    "        dropped += len(dups)\n",
    "        head = f\"[dup/batch/check_id] {len(dups)} duplicate(s) for check_id={cid[:12]}…\"\n",
    "        lines = [\"    \" + _fmt_rule_for_dup(x) for x in lst]\n",
    "        tail = f\"    -> keeping: name={keep.get('name')} | file={keep.get('yaml_path')}\"\n",
    "        blocks.append(\"\\n\".join([head, *lines, tail]))\n",
    "        out.append(keep)\n",
    "\n",
    "    if dropped:\n",
    "        msg = \"\\n\\n\".join(blocks) + f\"\\n[dedupe/batch] total dropped={dropped}\"\n",
    "        if mode == \"error\":\n",
    "            raise ValueError(msg)\n",
    "        if mode == \"warn\":\n",
    "            print(msg)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Table & column comments (safe, creation-only for columns)\n",
    "# =========================\n",
    "def _esc_comment(s: str) -> str:\n",
    "    return (s or \"\").replace(\"'\", \"''\")\n",
    "\n",
    "def _q_fqn(fqn: str) -> str:\n",
    "    # quote a.b.c -> `a`.`b`.`c`\n",
    "    return \".\".join(f\"`{p}`\" for p in fqn.split(\".\"))\n",
    "\n",
    "def default_table_documentation(target_table_fqn: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Default doc structure (markdown strings). Pass your own dict to main(table_doc=...).\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"table\": target_table_fqn,\n",
    "        \"table_comment\": (\n",
    "            \"# DQX Checks Configuration\\n\"\n",
    "            f\"- **Target**: `{target_table_fqn}`\\n\"\n",
    "            \"- Stores flattened Data Quality checks generated from YAML files.\\n\"\n",
    "            \"- `check_id` is a stable hash over (table_name, filter, check.*).\\n\"\n",
    "        ),\n",
    "        \"columns\": {\n",
    "            \"check_id\": \"**Hash** of canonical rule payload. Used for identity/dedupe.\",\n",
    "            \"check_id_payload\": \"Canonical **JSON** used to compute `check_id`.\",\n",
    "            \"table_name\": \"Fully qualified **target table** (`catalog.schema.table`).\",\n",
    "            \"name\": \"Human-readable **rule name**.\",\n",
    "            \"criticality\": \"Rule severity: `warn|warning|error`.\",\n",
    "            \"check\": \"Structured **check** object `{function, for_each_column, arguments}`.\",\n",
    "            \"filter\": \"Optional row-level **filter** expression.\",\n",
    "            \"run_config_name\": \"**Execution group/tag** to route this rule.\",\n",
    "            \"user_metadata\": \"User-provided **metadata** `map<string,string>`.\",\n",
    "            \"yaml_path\": \"Source **YAML** file path that defined this rule.\",\n",
    "            \"active\": \"If **false**, the rule is ignored by runners.\",\n",
    "            \"created_by\": \"Audit: **creator** of the row.\",\n",
    "            \"created_at\": \"Audit: **creation timestamp**.\",\n",
    "            \"updated_by\": \"Audit: **last updater**.\",\n",
    "            \"updated_at\": \"Audit: **last update timestamp**.\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def preview_table_documentation(spark: SparkSession, table_fqn: str, doc: Dict[str, Any]) -> None:\n",
    "    display_section(\"TABLE METADATA PREVIEW (markdown text stored in comments)\")\n",
    "    doc_df = spark.createDataFrame(\n",
    "        [(table_fqn, doc.get(\"table_comment\", \"\"))],\n",
    "        schema=\"table string, table_comment_markdown string\",\n",
    "    )\n",
    "    show_df(doc_df, n=1, truncate=False)\n",
    "\n",
    "    cols = doc.get(\"columns\", {}) or {}\n",
    "    cols_df = spark.createDataFrame(\n",
    "        [(k, v) for k, v in cols.items()],\n",
    "        schema=\"column string, column_comment_markdown string\",\n",
    "    )\n",
    "    show_df(cols_df, n=200, truncate=False)\n",
    "\n",
    "def apply_table_documentation(\n",
    "    spark: SparkSession,\n",
    "    table_fqn: str,\n",
    "    doc: Optional[Dict[str, Any]],\n",
    "    created_now: bool,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Apply table & column comments.\n",
    "    - TABLE comment: always applied (markdown string).\n",
    "    - COLUMN comments: only when created_now=True (your requested behavior).\n",
    "    Uses robust syntax to avoid parser issues.\n",
    "    \"\"\"\n",
    "    if not doc:\n",
    "        return\n",
    "\n",
    "    qtable = _q_fqn(table_fqn)\n",
    "\n",
    "    # Prefer COMMENT ON TABLE; fallback to TBLPROPERTIES\n",
    "    table_comment = _esc_comment(doc.get(\"table_comment\", \"\"))\n",
    "    if table_comment:\n",
    "        try:\n",
    "            spark.sql(f\"COMMENT ON TABLE {qtable} IS '{table_comment}'\")\n",
    "        except Exception:\n",
    "            spark.sql(f\"ALTER TABLE {qtable} SET TBLPROPERTIES ('comment' = '{table_comment}')\")\n",
    "\n",
    "    if not created_now:\n",
    "        return  # Only set column comments on first create\n",
    "\n",
    "    cols: Dict[str, str] = doc.get(\"columns\", {}) or {}\n",
    "    # Only try columns that exist\n",
    "    existing_cols = {f.name.lower() for f in spark.table(table_fqn).schema.fields}\n",
    "    for col_name, comment in cols.items():\n",
    "        if col_name.lower() not in existing_cols:\n",
    "            continue\n",
    "        qcol = f\"`{col_name}`\"\n",
    "        comment_sql = f\"ALTER TABLE {qtable} ALTER COLUMN {qcol} COMMENT '{_esc_comment(comment)}'\"\n",
    "        try:\n",
    "            spark.sql(comment_sql)\n",
    "        except Exception as e:\n",
    "            print(f\"[meta] Skipped column comment for {table_fqn}.{col_name}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# Recursive YAML discovery\n",
    "# =========================\n",
    "def _normalize_base(path: str) -> str:\n",
    "    \"\"\"Allow dbfs:/… by translating to /dbfs/… so open()/os.walk() work.\"\"\"\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        # dbfs:/a/b -> /dbfs/a/b\n",
    "        return \"/dbfs/\" + path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    return path\n",
    "\n",
    "def discover_yaml_files_recursive(base_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively find all *.yaml / *.yml under base_dir (skips hidden dirs/files).\n",
    "    Supports both workspace filesystem and dbfs:/ (via /dbfs/ bridge).\n",
    "    \"\"\"\n",
    "    base = _normalize_base(base_dir)\n",
    "    if not os.path.isdir(base):\n",
    "        raise FileNotFoundError(f\"Rules folder not found or not a directory: {base_dir} (resolved: {base})\")\n",
    "\n",
    "    out: List[str] = []\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        # skip hidden directories for noise reduction\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "        for f in files:\n",
    "            if f.startswith(\".\"):\n",
    "                continue\n",
    "            fl = f.lower()\n",
    "            if fl.endswith(\".yaml\") or fl.endswith(\".yml\"):\n",
    "                out.append(os.path.join(root, f))\n",
    "    return sorted(out)\n",
    "\n",
    "# =========================\n",
    "# Delta I/O (ALWAYS OVERWRITE)\n",
    "# =========================\n",
    "def ensure_schema_exists(spark: SparkSession, full_table_name: str):\n",
    "    parts = full_table_name.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected a 3-part name (catalog.schema.table), got '{full_table_name}'\")\n",
    "    cat, sch, _ = parts\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "\n",
    "def overwrite_rules_into_delta(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    delta_table_name: str,\n",
    "    table_doc: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    # Track existence BEFORE write\n",
    "    existed_before = spark.catalog.tableExists(delta_table_name)\n",
    "\n",
    "    # Ensure schema\n",
    "    ensure_schema_exists(spark, delta_table_name)\n",
    "\n",
    "    # Cast audit timestamps\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    # Overwrite (content + schema)\n",
    "    df.write.format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .option(\"overwriteSchema\", \"true\") \\\n",
    "      .saveAsTable(delta_table_name)\n",
    "\n",
    "    # Apply comments (table: always; columns: only on first create)\n",
    "    apply_table_documentation(spark, delta_table_name, table_doc, created_now=not existed_before)\n",
    "\n",
    "    # Preview metadata\n",
    "    if table_doc:\n",
    "        preview_table_documentation(spark, delta_table_name, table_doc)\n",
    "\n",
    "    # Display a clean confirmation table\n",
    "    display_section(\"WRITE RESULT (Delta)\")\n",
    "    summary = spark.createDataFrame(\n",
    "        [(df.count(), delta_table_name)],\n",
    "        schema=\"`rules written` long, `target table` string\",\n",
    "    )\n",
    "    show_df(summary, n=1)\n",
    "    print(f\"\\nrules written to target table: {delta_table_name}\")\n",
    "\n",
    "# =========================\n",
    "# Display-first debug helpers\n",
    "# =========================\n",
    "def debug_display_batch(spark: SparkSession, df_rules: DataFrame) -> None:\n",
    "    display_section(\"SUMMARY OF RULES LOADED FROM YAML\")\n",
    "    totals = [\n",
    "        (\n",
    "            df_rules.count(),\n",
    "            df_rules.select(\"check_id\").distinct().count(),\n",
    "            df_rules.select(\"check_id\", \"run_config_name\").distinct().count(),\n",
    "        )\n",
    "    ]\n",
    "    totals_df = spark.createDataFrame(\n",
    "        totals,\n",
    "        schema=\"`total number of rules found` long, `unique rules found` long, `distinct pair of rules` long\",\n",
    "    )\n",
    "    show_df(totals_df, n=1)\n",
    "\n",
    "    display_section(\"SAMPLE OF RULES LOADED FROM YAML (check_id, name, run_config_name, yaml_path)\")\n",
    "    sample_cols = df_rules.select(\"check_id\", \"name\", \"run_config_name\", \"yaml_path\").orderBy(\n",
    "        desc(\"yaml_path\")\n",
    "    )\n",
    "    show_df(sample_cols, n=50, truncate=False)\n",
    "\n",
    "    display_section(\"RULES LOADED PER TABLE\")\n",
    "    by_table = df_rules.groupBy(\"table_name\").count().orderBy(desc(\"count\"))\n",
    "    show_df(by_table, n=200)\n",
    "\n",
    "    # Only show payload snippet when small\n",
    "    distinct_cid = totals[0][1]\n",
    "    if distinct_cid <= 3:\n",
    "        display_section(\"PAYLOAD PREVIEW (first 3)\")\n",
    "        show_df(df_rules.select(\"check_id\", \"check_id_payload\"), n=3, truncate=False)\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules: List[dict]) -> Optional[DataFrame]:\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return None\n",
    "    df = (\n",
    "        spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "        .withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    )\n",
    "    debug_display_batch(spark, df)\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Validation (now works off a file list)\n",
    "# =========================\n",
    "def validate_rule_files(file_paths: List[str], required_fields: List[str], fail_fast: bool = True) -> List[str]:\n",
    "    errors = []\n",
    "    for full_path in file_paths:\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            docs = load_yaml_rules(full_path)\n",
    "            if not docs:\n",
    "                print(f\"  (empty) skipped: {full_path}\")\n",
    "                continue\n",
    "            validate_rules_file(docs, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path, required_fields=required_fields)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main(\n",
    "    output_config_path: str = \"resources/dqx_config.yaml\",\n",
    "    rules_dir: Optional[str] = None,\n",
    "    time_zone: str = \"America/Chicago\",\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False,\n",
    "    required_fields: Optional[List[str]] = None,\n",
    "    batch_dedupe_mode: str = \"warn\",  # warn | error | skip\n",
    "    table_doc: Optional[Dict[str, Any]] = None,   # table & column comments; markdown strings\n",
    "):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Environment banner (unchanged)\n",
    "    print_notebook_env(spark, local_timezone=time_zone)\n",
    "\n",
    "    with open(output_config_path, \"r\") as fh:\n",
    "        output_config = yaml.safe_load(fh) or {}\n",
    "    rules_dir = rules_dir or output_config[\"dqx_yaml_checks\"]\n",
    "    delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "\n",
    "    required_fields = required_fields or [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "\n",
    "    # NEW: recursively discover YAML files\n",
    "    yaml_files = discover_yaml_files_recursive(rules_dir)\n",
    "    display_section(\"YAML FILES DISCOVERED (recursive)\")\n",
    "    files_df = spark.createDataFrame([(p,) for p in yaml_files], \"yaml_path string\")\n",
    "    show_df(files_df, n=500, truncate=False)\n",
    "\n",
    "    if validate_only:\n",
    "        print(\"\\nValidation only: not writing any rules.\")\n",
    "        validate_rule_files(yaml_files, required_fields)\n",
    "        return\n",
    "\n",
    "    # collect rules\n",
    "    all_rules: List[dict] = []\n",
    "    for full_path in yaml_files:\n",
    "        file_rules = process_yaml_file(full_path, required_fields=required_fields, time_zone=time_zone)\n",
    "        if file_rules:\n",
    "            all_rules.extend(file_rules)\n",
    "            print(f\"[loader] {full_path}: rules={len(file_rules)}\")\n",
    "\n",
    "    if not all_rules:\n",
    "        print(\"No rules discovered; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[loader] total parsed rules (pre-dedupe): {len(all_rules)}\")\n",
    "\n",
    "    # in-batch dedupe on check_id only\n",
    "    all_rules = dedupe_rules_in_batch_by_check_id(all_rules, mode=batch_dedupe_mode)\n",
    "\n",
    "    # assemble DataFrame and DISPLAY diagnostics\n",
    "    df = spark.createDataFrame(all_rules, schema=TABLE_SCHEMA)\n",
    "    debug_display_batch(spark, df)\n",
    "\n",
    "    if dry_run:\n",
    "        display_section(\"DRY-RUN: FULL RULES PREVIEW\")\n",
    "        show_df(df.orderBy(\"table_name\", \"name\"), n=1000, truncate=False)\n",
    "        return\n",
    "\n",
    "    # ALWAYS OVERWRITE on each run\n",
    "    table_doc = table_doc or default_table_documentation(delta_table_name)\n",
    "    overwrite_rules_into_delta(spark, df, delta_table_name, table_doc=table_doc)\n",
    "    print(f\"Finished writing rules to '{delta_table_name}' (overwrite).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main(dry_run=True)\n",
    "    # main(validate_only=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
