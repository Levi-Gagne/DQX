{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load DQX Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff95a1d5-3010-440c-a60f-e8b4c7fcc84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2bbe9a89-330c-4868-9766-f17332b0f64a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "RUNBOOK"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Runbook — `01_load_dqx_checks` (DQX Rule Loader)\n",
    "\n",
    "**Purpose**  \n",
    "Load YAML-defined DQX rules into the **checks config** table (Unity Catalog), with strict validation, canonicalization, and a stable `check_id` derived from a canonical JSON payload.\n",
    "\n",
    "## Environment / Imports (exact sources)\n",
    "\n",
    "- **Databricks Runtime** with `pyspark`\n",
    "- **Package**: `databricks-labs-dqx==0.8.x`\n",
    "- **Engine**: `databricks.labs.dqx.engine.DQEngine` (used for `validate_checks`)\n",
    "- **Framework utils** (your repo, loaded via `add_src_to_sys_path()`):\n",
    "  - `framework_utils.display`: `show_df`, `display_section`\n",
    "  - `framework_utils.runtime`: `show_notebook_env`\n",
    "  - `framework_utils.color`: `Color`\n",
    "  - `framework_utils.console`: `Console`\n",
    "  - `framework_utils.config`: `ProjectConfig`, `ConfigError`\n",
    "  - `framework_utils.write`: `TableWriter`\n",
    "  - `framework_utils.table`: `table_exists`\n",
    "  - `framework_utils.path`: `dbfs_to_local`, `list_yaml_files`\n",
    "\n",
    "> Note: `CHECKS_CONFIG_COMMENTS` is referenced when creating the table. Ensure it’s defined/imported in the notebook scope, or it will evaluate to `None`.\n",
    "\n",
    "## How to Run\n",
    "\n",
    "```python\n",
    "from framework_utils.config import ProjectConfig\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "cfg = ProjectConfig(\"resources/dqx_config.yaml\", variables={})\n",
    "run_checks_loader(spark, cfg, notebook_idx=1, dry_run=False, validate_only=False)\n",
    "```\n",
    "\n",
    "- **Dry run**: `dry_run=True` → builds DF and previews, **no write**\n",
    "- **Validate only**: `validate_only=True` → validates YAMLs, **no write**\n",
    "\n",
    "## Configuration (read from `ProjectConfig`)\n",
    "\n",
    "- **Data source**: `notebooks.notebook_1.data_sources.data_source_1`\n",
    "  - `source_path` (required): root folder for YAMLs (supports `dbfs:/` via `/dbfs` bridge)\n",
    "  - `allowed_criticality` (required): allowed values for `criticality`\n",
    "  - `required_fields` (required): required YAML fields\n",
    "- **Target**: `notebooks.notebook_1.targets.target_table_1`\n",
    "  - `full_table_name()` (FQN): UC table to write\n",
    "  - `write`: `{format, mode, options}` (required)\n",
    "  - `primary_key` (required): PK column name (the code supplies it to `create_table`)\n",
    "  - `partition_by`, `table_description`, `table_tags` (optional)\n",
    "\n",
    "## Data Model (written schema)\n",
    "\n",
    "`CHECKS_CONFIG_STRUCT` (defined in this notebook) with fields:\n",
    "\n",
    "- `check_id` (PK semantics): `sha256` of canonical payload\n",
    "- `check_id_payload` (canonical JSON)\n",
    "- `table_name`, `name`, `criticality`\n",
    "- `check`: `{function, for_each_column?, arguments?}` (values stringified)\n",
    "- `filter`, `run_config_name`, `user_metadata`\n",
    "- `yaml_path`, `active`\n",
    "- `created_by`, `created_at`, `updated_by`, `updated_at`\n",
    "\n",
    "> Identity inputs for `check_id`: **`table_name` (lowercased), normalized `filter`, canonical `check`**.  \n",
    "> **Not** included: `name`, `criticality`, `run_config_name`.\n",
    "\n",
    "## Processing Logic\n",
    "\n",
    "1. **Discover YAMLs**: `list_yaml_files(source_path)` → show list (Spark DF).\n",
    "2. **Validation (file)**:\n",
    "   - Non-empty; no duplicate `name` within file.\n",
    "3. **Validation (rule)**:\n",
    "   - All `required_fields` present.\n",
    "   - `table_name` is fully qualified (`catalog.schema.table`).\n",
    "   - `criticality` ∈ `allowed_criticality`.\n",
    "   - `check.function` present (non-empty).\n",
    "4. **DQX validation**: `DQEngine.validate_checks(rules)` per file (raises on errors).\n",
    "5. **Canonicalization**:\n",
    "   - `filter` normalized whitespace.\n",
    "   - `check.for_each_column` sorted (or `None`).\n",
    "   - `check.arguments` stringified (JSON/bool/null/scalars).\n",
    "   - Build payload JSON (`sort_keys=True`, compact separators), then `sha256`.\n",
    "6. **Audit**: `created_by=\"AdminUser\"`, `created_at=UTC now`.\n",
    "7. **Batch de-dupe** (by `check_id`):\n",
    "   - Keep lexicographically first by `(yaml_path, name)`.\n",
    "   - Mode: **warn** (print), **error** (raise), **skip** (keep all).\n",
    "8. **Write**:\n",
    "   - If table missing → `TableWriter.create_table(...)` with `CHECKS_CONFIG_STRUCT`, PK, optional comments/tags.\n",
    "   - Write via `TableWriter.write_df` (format/mode/options from target block).  \n",
    "     DF columns are projected to target schema order.\n",
    "9. **Diagnostics**:\n",
    "   - Summary counts: total, distinct `check_id`, distinct `(check_id, run_config_name)`.\n",
    "   - Write result: rows written, table FQN, mode.\n",
    "\n",
    "## Failure Modes (and what the code does)\n",
    "\n",
    "- **Missing config keys** → `must(...)` raises `ConfigError`.\n",
    "- **Empty or invalid YAML file** → `validate_rules_file` raises.\n",
    "- **Bad rule fields** → `validate_rule_fields` raises with details.\n",
    "- **DQX schema/semantics error** → `validate_with_dqx` raises `ValueError` with `status.to_string()`.\n",
    "- **Duplicate `check_id`**:\n",
    "  - `mode=\"warn\"` prints dropped block(s), continues.\n",
    "  - `mode=\"error\"` raises.\n",
    "  - `mode=\"skip\"` keeps all.\n",
    "- **No rules after de-dupe** → short-circuit return (no write).\n",
    "\n",
    "## Post-Run sanity SQL (example)\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) AS rules,\n",
    "       COUNT(DISTINCT check_id) AS unique_rules\n",
    "FROM <your_fqn>;\n",
    "\n",
    "SELECT check_id, COUNT(*) c\n",
    "FROM <your_fqn>\n",
    "GROUP BY check_id\n",
    "HAVING COUNT(*) > 1;  -- should be 0 when dedupe_mode != \"skip\"\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Overwrite semantics: this is the **system of record** for rules derived from YAML; manual edits in the table will be lost on next run.  \n",
    "- `check_id` identity includes only `{table_name↓, filter, check.*}` — **not** `name`, `criticality`, or `run_config_name`.  \n",
    "- Argument values are persisted as strings for stability; cast/parse (e.g., `try_cast`, `from_json`) downstream.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5042f1e5-f5d4-4dda-a0b8-3e9130961893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FLOW"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START: run_checks_loader(spark, cfg, *, notebook_idx, dry_run=False, validate_only=False)\n",
    "|\n",
    "|-- 0. Bootstrap\n",
    "|     |-- add_src_to_sys_path(src_dir=\"src\", sentinel=\"framework_utils\")\n",
    "|     |     └─ (defined inline in this notebook)\n",
    "|     |-- show_notebook_env(spark)                     ── from framework_utils.runtime\n",
    "|     |-- NOTE (in __main__): spark.sql.session.timeZone = \"UTC\"\n",
    "|\n",
    "|-- 1. Imports (explicit sources)\n",
    "|     |-- pyspark.sql.{SparkSession, DataFrame, types as T, functions as F}\n",
    "|     |-- databricks.labs.dqx.engine.DQEngine\n",
    "|     |-- framework_utils.display.{show_df, display_section}\n",
    "|     |-- framework_utils.runtime.show_notebook_env\n",
    "|     |-- framework_utils.color.Color\n",
    "|     |-- framework_utils.console.Console\n",
    "|     |-- framework_utils.config.{ProjectConfig, ConfigError}\n",
    "|     |-- framework_utils.write.TableWriter\n",
    "|     |-- framework_utils.table.table_exists\n",
    "|     |-- framework_utils.path.{dbfs_to_local, list_yaml_files}\n",
    "|\n",
    "|-- 2. Constants / schema\n",
    "|     |-- CHECKS_CONFIG_STRUCT (StructType)            ── defined inline in this notebook\n",
    "|     |-- (Optional) CHECKS_CONFIG_COMMENTS            ── referenced in create_table; must exist in scope or be None\n",
    "|\n",
    "|-- 3. Utility functions (defined inline)\n",
    "|     |-- must(val, name) → raise ConfigError on missing\n",
    "|     |-- _canon_filter(s) → normalize whitespace\n",
    "|     |-- _stringify_map_values(map) → stringify JSON/bool/null/scalars\n",
    "|     |-- _canon_check(check) → {function, sorted for_each_column|None, arguments(sorted,stringified)}\n",
    "|     |-- compute_check_id_payload(table_name, check, filter) → canonical JSON (lowercased table_name)\n",
    "|     |-- compute_check_id_from_payload(payload) → sha256 hex\n",
    "|     |-- load_yaml_rules(path) → list[dict] using yaml.safe_load_all + dbfs_to_local (framework_utils.path)\n",
    "|     |-- validate_rules_file(rules, file_path) → nonempty + no duplicate rule names\n",
    "|     |-- validate_rule_fields(rule, file_path, required_fields, allowed_criticality)\n",
    "|     |-- validate_with_dqx(rules, file_path) → DQEngine.validate_checks(rules)\n",
    "|     |-- process_yaml_file(path, required_fields, created_by_value, allowed_criticality)\n",
    "|     |     ├─ load_yaml_rules + file/rule validation\n",
    "|     |     ├─ payload/check_id build\n",
    "|     |     └─ produce rows with audit fields (created_by=\"AdminUser\", created_at=UTC now)\n",
    "|     |-- dedupe_rules_in_batch_by_check_id(rules, mode) → group by check_id; keep first by (yaml_path, name);\n",
    "|     |     prints/warns/errors via Console.* depending on mode\n",
    "|     |-- discover_yaml(cfg, rules_dir)\n",
    "|     |     ├─ list_yaml_files(rules_dir)               ── from framework_utils.path\n",
    "|     |     └─ display_section + show_df of discovered files\n",
    "|     |-- build_df_from_rules(spark, rules) → createDataFrame(rules, CHECKS_CONFIG_STRUCT)\n",
    "|\n",
    "|-- 4. Resolve config (ProjectConfig)\n",
    "|     |-- nb = cfg.notebook(notebook_idx)\n",
    "|     |-- ds = nb.data_sources().data_source(1)\n",
    "|     |     ├─ rules_dir       = must(ds[\"source_path\"])\n",
    "|     |     ├─ allowed_crit    = must(ds[\"allowed_criticality\"])\n",
    "|     |     └─ required_fields = must(ds[\"required_fields\"])\n",
    "|     |-- t  = nb.targets().target_table(1)\n",
    "|           ├─ fqn           = t.full_table_name()\n",
    "|           ├─ write_block   = must(t[\"write\"])          # {format, mode, options}\n",
    "|           ├─ partition_by  = t.get(\"partition_by\") or []\n",
    "|           ├─ table_comment = t.get(\"table_description\")\n",
    "|           ├─ table_tags    = t.table_tags()\n",
    "|           └─ primary_key   = must(t[\"primary_key\"])\n",
    "|\n",
    "|-- 5. Ensure target table exists\n",
    "|     |-- if not table_exists(spark, fqn):                ── framework_utils.table\n",
    "|           └─ TableWriter.create_table(…)\n",
    "|                fqn=fqn,\n",
    "|                schema=CHECKS_CONFIG_STRUCT,\n",
    "|                format=write_block[\"format\"],\n",
    "|                options=write_block.get(\"options\") or {},\n",
    "|                partition_by=partition_by,\n",
    "|                table_comment=table_comment,\n",
    "|                column_comments=(CHECKS_CONFIG_COMMENTS or None),\n",
    "|                table_properties=None,\n",
    "|                table_tags=table_tags,\n",
    "|                column_tags=None,\n",
    "|                primary_key_cols=[primary_key]\n",
    "|\n",
    "|-- 6. Discover YAMLs\n",
    "|     |-- yaml_files = discover_yaml(cfg, rules_dir)      ── prints and shows files\n",
    "|\n",
    "|-- 7. validate_only short-circuit (if True)\n",
    "|     |-- for p in yaml_files: validate_rules_file(load_yaml_rules(p), p)\n",
    "|     |-- RETURN {config_path=cfg.path, rules_files=len(yaml_files), errors=[…]}\n",
    "|\n",
    "|-- 8. Load + validate + canonicalize + ID\n",
    "|     |-- all_rules = []\n",
    "|     |-- for p in yaml_files:\n",
    "|           ├─ file_rules = process_yaml_file(p, required_fields, created_by_value=\"AdminUser\", allowed_criticality=allowed_crit)\n",
    "|           ├─ if file_rules: extend all_rules; print Console.LOADER summary\n",
    "|     |-- pre_dedupe  = len(all_rules)\n",
    "|     |-- rules       = dedupe_rules_in_batch_by_check_id(all_rules, mode=cfg.variables.batch_dedupe_mode)\n",
    "|     |-- post_dedupe = len(rules)\n",
    "|     |-- if not rules: RETURN {config_path, rules_files, wrote_rows=0, target_table=fqn}\n",
    "|\n",
    "|-- 9. Build DataFrame + quick diagnostics\n",
    "|     |-- df = build_df_from_rules(spark, rules)\n",
    "|     |-- display_section(\"SUMMARY OF RULES LOADED FROM YAML\")\n",
    "|     |-- show_df(totals_df: [count, distinct check_id, distinct (check_id, run_config_name)])\n",
    "|\n",
    "|-- 10. dry_run short-circuit (if True)\n",
    "|     |-- display_section(\"DRY-RUN: FULL RULES PREVIEW\"); show_df(df.orderBy(\"table_name\",\"name\"))\n",
    "|     |-- RETURN {metrics…, target_table=fqn, wrote_rows=0, write_mode=write_block[\"mode\"]}\n",
    "|\n",
    "|-- 11. Write to target table (order aligned to target schema)\n",
    "|     |-- tw.write_df(\n",
    "|            df=df.select(*[f.name for f in spark.table(fqn).schema.fields]),\n",
    "|            fqn=fqn,\n",
    "|            mode=write_block[\"mode\"],\n",
    "|            format=write_block[\"format\"],\n",
    "|            options=write_block.get(\"options\") or {},\n",
    "|         )\n",
    "|     |-- wrote_rows = df.count()\n",
    "|     |-- display_section(\"WRITE RESULT\"); show_df(summary rows, table, mode)\n",
    "|     |-- print colored success line via framework_utils.color.Color\n",
    "|\n",
    "|-- 12. RETURN result dict\n",
    "|     |-- {config_path, rules_files, rules_pre_dedupe, rules_post_dedupe,\n",
    "|         unique_check_ids, distinct_rule_run_pairs, target_table=fqn,\n",
    "|         wrote_rows, write_mode=write_block[\"mode\"]}\n",
    "|\n",
    "END: run_checks_loader\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c31bb004-e051-42b7-b798-c014f394828b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TARGET TABLE"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# dq_{env}.dqx.checks_config\n",
    "CHECKS_CONFIG_STRUCT = T.StructType([\n",
    "    T.StructField(\"check_id\",         T.StringType(),   False),\n",
    "    T.StructField(\"check_id_payload\", T.StringType(),   False),\n",
    "    T.StructField(\"table_name\",       T.StringType(),   False),\n",
    "    T.StructField(\"name\",             T.StringType(),   False),\n",
    "    T.StructField(\"criticality\",      T.StringType(),   False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",           T.StringType(),   True),\n",
    "    T.StructField(\"run_config_name\",  T.StringType(),   False),\n",
    "    T.StructField(\"user_metadata\",    T.MapType(T.StringType(), T.StringType()), True),\n",
    "    T.StructField(\"yaml_path\",        T.StringType(),   False),\n",
    "    T.StructField(\"active\",           T.BooleanType(),  False),\n",
    "    T.StructField(\"created_by\",       T.StringType(),   False),\n",
    "    T.StructField(\"created_at\",       T.TimestampType(),False),\n",
    "    T.StructField(\"updated_by\",       T.StringType(),   True),\n",
    "    T.StructField(\"updated_at\",       T.TimestampType(),True),\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74936d7c-4414-42fe-a143-cb6c7d364d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1e26e-12cc-44e8-ae5a-e7b1fa4d5049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1154f87-29b1-4cb5-8a2f-ff0737aa9cda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40761988-2989-4a37-8b9e-e4b441e97bce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 01_load_dqx_checks\n",
    "# Purpose: Load YAML rules into dq_{env}.dqx.checks_config\n",
    "# Requires: databricks-labs-dqx==0.8.x\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, types as T, functions as F\n",
    "\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "def add_src_to_sys_path(src_dir=\"src\", sentinel=\"framework_utils\", max_levels=12):\n",
    "    start = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd().resolve()\n",
    "    p = start\n",
    "    for _ in range(max_levels):\n",
    "        cand = p / src_dir\n",
    "        if (cand / sentinel).exists():\n",
    "            s = str(cand.resolve())\n",
    "            if s not in sys.path:\n",
    "                sys.path.insert(0, s)\n",
    "                print(f\"[bootstrap] sys.path[0] = {s}\")\n",
    "            return\n",
    "        if p == p.parent: break\n",
    "        p = p.parent\n",
    "    raise ImportError(f\"Couldn't find {src_dir}/{sentinel} above {start}\")\n",
    "\n",
    "add_src_to_sys_path()\n",
    "\n",
    "from framework_utils.display import show_df, display_section\n",
    "from framework_utils.runtime import show_notebook_env\n",
    "from framework_utils.color import Color\n",
    "from framework_utils.console import Console\n",
    "from framework_utils.config import ProjectConfig, ConfigError\n",
    "from framework_utils.write import TableWriter\n",
    "from framework_utils.table import table_exists\n",
    "from framework_utils.path import dbfs_to_local, list_yaml_files\n",
    "\n",
    "# =========================\n",
    "# SPARK STRUCTURED SCHEMA (source of truth)\n",
    "# =========================\n",
    "CHECKS_CONFIG_STRUCT = T.StructType([\n",
    "    T.StructField(\"check_id\",         T.StringType(),   False, {\"comment\": \"PRIMARY KEY. Stable sha256 over canonical {table_name↓, filter, check.*}.\"}),\n",
    "    T.StructField(\"check_id_payload\", T.StringType(),   False, {\"comment\": \"Canonical JSON used to derive `check_id` (sorted keys, normalized values).\"}),\n",
    "    T.StructField(\"table_name\",       T.StringType(),   False, {\"comment\": \"Target table FQN (`catalog.schema.table`). Lowercased in payload for stability.\"}),\n",
    "    T.StructField(\"name\",             T.StringType(),   False, {\"comment\": \"Human-readable rule name. Used in UI/diagnostics and joins.\"}),\n",
    "    T.StructField(\"criticality\",      T.StringType(),   False, {\"comment\": \"Rule severity: `error|warn`.\"}),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False, {\"comment\": \"DQX function to run\"}),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True,  {\"comment\": \"Optional list of columns\"}),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True, {\"comment\": \"Key/value args\"}),\n",
    "    ]), False, {\"comment\": \"Structured rule `{function, for_each_column?, arguments?}`; values stringified.\"}),\n",
    "    T.StructField(\"filter\",           T.StringType(),   True,  {\"comment\": \"Optional SQL predicate applied before evaluation (row-level).\"}),\n",
    "    T.StructField(\"run_config_name\",  T.StringType(),   False, {\"comment\": \"Execution group/tag. Not part of identity.\"}),\n",
    "    T.StructField(\"user_metadata\",    T.MapType(T.StringType(), T.StringType()), True, {\"comment\": \"Free-form map<string,string>.\"}),\n",
    "    T.StructField(\"yaml_path\",        T.StringType(),   False, {\"comment\": \"Absolute/volume path to the defining YAML doc (lineage).\"}),\n",
    "    T.StructField(\"active\",           T.BooleanType(),  False, {\"comment\": \"If `false`, rule is ignored by runners.\"}),\n",
    "    T.StructField(\"created_by\",       T.StringType(),   False, {\"comment\": \"Audit: creator/principal that materialized the row.\"}),\n",
    "    T.StructField(\"created_at\",       T.TimestampType(),False, {\"comment\": \"Audit: creation timestamp (UTC).\"}),\n",
    "    T.StructField(\"updated_by\",       T.StringType(),   True,  {\"comment\": \"Audit: last updater (nullable).\"}),\n",
    "    T.StructField(\"updated_at\",       T.TimestampType(),True,  {\"comment\": \"Audit: last update timestamp (UTC, nullable).\"}),\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Small local helpers (no dependency on must() inside utils.config)\n",
    "# =========================\n",
    "def must(val: Any, name: str) -> Any:\n",
    "    if val is None or (isinstance(val, str) and not val.strip()):\n",
    "        raise ConfigError(f\"Missing required config: {name}\")\n",
    "    return val\n",
    "\n",
    "# =========================\n",
    "# Canonicalization & IDs\n",
    "# =========================\n",
    "def _canon_filter(s: Optional[str]) -> str:\n",
    "    return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "    fec = chk.get(\"for_each_column\")\n",
    "    if isinstance(fec, list):\n",
    "        out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "    args = chk.get(\"arguments\") or {}\n",
    "    canon_args: Dict[str, str] = {}\n",
    "    for k, v in args.items():\n",
    "        sv = \"\" if v is None else str(v).strip()\n",
    "        if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "            try:\n",
    "                sv = json.dumps(json.loads(sv), sort_keys=True, separators=(\",\", \":\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        canon_args[str(k)] = sv\n",
    "    out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "    return out\n",
    "\n",
    "def compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    payload_obj = {\"table_name\": (table_name or \"\").lower(), \"filter\": _canon_filter(filter_str), \"check\": _canon_check(check_dict or {})}\n",
    "    return json.dumps(payload_obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def compute_check_id_from_payload(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode()).hexdigest()\n",
    "\n",
    "# =========================\n",
    "# Rule YAML load/validate\n",
    "# =========================\n",
    "def load_yaml_rules(path: str) -> List[dict]:\n",
    "    p = Path(dbfs_to_local(path))\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Rules YAML not found: {p}\")\n",
    "    with open(p, \"r\") as fh:\n",
    "        docs = list(yaml.safe_load_all(fh)) or []\n",
    "    out: List[dict] = []\n",
    "    for d in docs:\n",
    "        if not d:\n",
    "            continue\n",
    "        if isinstance(d, dict):\n",
    "            out.append(d)\n",
    "        elif isinstance(d, list):\n",
    "            out.extend([x for x in d if isinstance(x, dict)])\n",
    "    return out\n",
    "\n",
    "def validate_rules_file(rules: List[dict], file_path: str):\n",
    "    if not rules:\n",
    "        raise ValueError(f\"No rules found in {file_path} (empty or invalid YAML).\")\n",
    "    probs, seen = [], set()\n",
    "    for r in rules:\n",
    "        nm = r.get(\"name\")\n",
    "        if not nm: probs.append(f\"Missing rule name in {file_path}\")\n",
    "        if nm in seen: probs.append(f\"Duplicate rule name '{nm}' in {file_path}\")\n",
    "        seen.add(nm)\n",
    "    if probs: raise ValueError(f\"File-level validation failed in {file_path}: {probs}\")\n",
    "\n",
    "def validate_rule_fields(rule: dict, file_path: str, required_fields: List[str], allowed_criticality: List[str]):\n",
    "    probs = []\n",
    "    for f in required_fields:\n",
    "        if not rule.get(f): probs.append(f\"Missing required field '{f}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        probs.append(f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"criticality\") not in set(allowed_criticality):\n",
    "        probs.append(f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        probs.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if probs: raise ValueError(\"Rule-level validation failed: \" + \"; \".join(probs))\n",
    "\n",
    "def validate_with_dqx(rules: List[dict], file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if getattr(status, \"has_errors\", False):\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "# =========================\n",
    "# Build rows from YAML docs (UTC timestamps)\n",
    "# =========================\n",
    "def process_yaml_file(\n",
    "    path: str,\n",
    "    required_fields: List[str],\n",
    "    created_by_value: str,\n",
    "    allowed_criticality: List[str],\n",
    ") -> List[dict]:\n",
    "    docs = load_yaml_rules(path)\n",
    "    if not docs:\n",
    "        print(f\"{Console.SKIP} {path} has no rules.\")\n",
    "        return []\n",
    "\n",
    "    validate_rules_file(docs, path)\n",
    "    flat: List[dict] = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path, required_fields, allowed_criticality)\n",
    "        raw_check = rule[\"check\"] or {}\n",
    "        payload   = compute_check_id_payload(rule[\"table_name\"], raw_check, rule.get(\"filter\"))\n",
    "        check_id  = compute_check_id_from_payload(payload)\n",
    "\n",
    "        function = raw_check.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\")\n",
    "\n",
    "        for_each = raw_check.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\")\n",
    "        for_each = [str(x) for x in (for_each or [])] or None\n",
    "\n",
    "        arguments = raw_check.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(f\"{path}: user_metadata must be a map (rule '{rule.get('name')}').\")\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        created_at_ts = datetime.utcnow()\n",
    "\n",
    "        flat.append({\n",
    "            \"check_id\": check_id,\n",
    "            \"check_id_payload\": payload,\n",
    "            \"table_name\": rule[\"table_name\"],\n",
    "            \"name\": rule[\"name\"],\n",
    "            \"criticality\": rule[\"criticality\"],\n",
    "            \"check\": {\"function\": function, \"for_each_column\": for_each, \"arguments\": arguments or None},\n",
    "            \"filter\": rule.get(\"filter\"),\n",
    "            \"run_config_name\": rule[\"run_config_name\"],\n",
    "            \"user_metadata\": user_metadata or None,\n",
    "            \"yaml_path\": path,\n",
    "            \"active\": bool(rule.get(\"active\", True)),\n",
    "            \"created_by\": \"AdminUser\",\n",
    "            \"created_at\": created_at_ts,\n",
    "            \"updated_by\": None,\n",
    "            \"updated_at\": None,\n",
    "        })\n",
    "\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat\n",
    "\n",
    "# =========================\n",
    "# Dedupe (on check_id)\n",
    "# =========================\n",
    "def _fmt_rule_for_dup(r: dict) -> str:\n",
    "    return f\"name={r.get('name')} | file={r.get('yaml_path')} | criticality={r.get('criticality')} | run_config={r.get('run_config_name')} | filter={r.get('filter')}\"\n",
    "\n",
    "def dedupe_rules_in_batch_by_check_id(rules: List[dict], mode: str) -> List[dict]:\n",
    "    groups: Dict[str, List[dict]] = {}\n",
    "    for r in rules: groups.setdefault(r[\"check_id\"], []).append(r)\n",
    "    out: List[dict] = []; dropped = 0; blocks: List[str] = []\n",
    "    for cid, lst in groups.items():\n",
    "        if len(lst) == 1:\n",
    "            out.append(lst[0]); continue\n",
    "        lst = sorted(lst, key=lambda x: (x.get(\"yaml_path\",\"\"), x.get(\"name\",\"\")))\n",
    "        keep, dups = lst[0], lst[1:]; dropped += len(dups)\n",
    "        head = f\"{Console.DEDUPE} {len(dups)} duplicate(s) for check_id={cid[:12]}…\"\n",
    "        lines = [\"    \" + _fmt_rule_for_dup(x) for x in lst]\n",
    "        tail = f\"    -> keeping: name={keep.get('name')} | file={keep.get('yaml_path')}\"\n",
    "        blocks.append(\"\\n\".join([head, *lines, tail])); out.append(keep)\n",
    "    if dropped:\n",
    "        msg = \"\\n\\n\".join(blocks) + f\"\\n{Console.DEDUPE} total dropped={dropped}\"\n",
    "        if mode == \"error\": raise ValueError(msg)\n",
    "        if mode == \"warn\": print(msg)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Discover rule YAMLs\n",
    "# =========================\n",
    "def discover_yaml(cfg: ProjectConfig, rules_dir: str) -> List[str]:\n",
    "    print(f\"{Console.DEBUG} rules_dir (raw from YAML): {rules_dir}\")\n",
    "    files = list_yaml_files(rules_dir)\n",
    "    display_section(\"YAML FILES DISCOVERED (recursive)\")\n",
    "    df = SparkSession.builder.getOrCreate().createDataFrame([(p,) for p in files], \"yaml_path string\")\n",
    "    show_df(df, n=500, truncate=False)\n",
    "    return files\n",
    "\n",
    "# =========================\n",
    "# Build DF\n",
    "# =========================\n",
    "def build_df_from_rules(spark: SparkSession, rules: List[dict]) -> DataFrame:\n",
    "    return spark.createDataFrame(rules, schema=CHECKS_CONFIG_STRUCT)\n",
    "\n",
    "# =========================\n",
    "# Runner\n",
    "# =========================\n",
    "def run_checks_loader(\n",
    "    spark: SparkSession,\n",
    "    cfg: ProjectConfig,\n",
    "    *,\n",
    "    notebook_idx: int,\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    apply_meta  = bool(must(cfg.get(\"variables.apply_table_metadata\"), \"variables.apply_table_metadata\"))\n",
    "    dedupe_mode = must(cfg.get(\"variables.batch_dedupe_mode\"), \"variables.batch_dedupe_mode\")\n",
    "\n",
    "    nb = cfg.notebook(notebook_idx)\n",
    "\n",
    "    # Use data_sources.data_source_1\n",
    "    ds = nb.data_sources().data_source(1)\n",
    "    rules_dir       = must(ds.get(\"source_path\"),         f\"notebooks.notebook_{notebook_idx}.data_sources.data_source_1.source_path\")\n",
    "    allowed_crit    = must(ds.get(\"allowed_criticality\"), f\"notebooks.notebook_{notebook_idx}.data_sources.data_source_1.allowed_criticality\")\n",
    "    required_fields = must(ds.get(\"required_fields\"),     f\"notebooks.notebook_{notebook_idx}.data_sources.data_source_1.required_fields\")\n",
    "\n",
    "    # Target (checks_config)\n",
    "    t = nb.targets().target_table(1)\n",
    "    fqn           = t.full_table_name()\n",
    "    partition_by  = t.get(\"partition_by\") or []\n",
    "    write_block   = must(t.get(\"write\"), f\"{fqn}.write\")\n",
    "    table_comment = t.get(\"table_description\")\n",
    "    primary_key   = must(t.get(\"primary_key\"), f\"{fqn}.primary_key\")   # from YAML\n",
    "    table_tags    = t.table_tags()                                     # normalized in config.py\n",
    "\n",
    "    tw = TableWriter(spark)\n",
    "\n",
    "    # Create if needed (no unsupported create modes)\n",
    "    if not table_exists(spark, fqn):\n",
    "        tw.create_table(\n",
    "            fqn=fqn,\n",
    "            schema=CHECKS_CONFIG_STRUCT,\n",
    "            format=must(write_block.get(\"format\"), f\"{fqn}.write.format\"),\n",
    "            options=write_block.get(\"options\") or {},\n",
    "            partition_by=partition_by,\n",
    "            table_comment=table_comment,\n",
    "            column_comments=(CHECKS_CONFIG_COMMENTS or None),\n",
    "            table_properties=None,          # add if you map something to TBLPROPERTIES\n",
    "            table_tags=table_tags,          # tags from YAML (flat dict)\n",
    "            column_tags=None,               # per-column tags if you maintain them\n",
    "            primary_key_cols=[primary_key], # PK columns only; name auto-generated\n",
    "        )\n",
    "\n",
    "    yaml_files = discover_yaml(cfg, rules_dir)\n",
    "\n",
    "    if validate_only:\n",
    "        print(f\"{Console.VALIDATION} Validation only: not writing any rules.\")\n",
    "        errs: List[str] = []\n",
    "        for p in yaml_files:\n",
    "            try:\n",
    "                validate_rules_file(load_yaml_rules(p), p)\n",
    "            except Exception as e:\n",
    "                errs.append(f\"{p}: {e}\")\n",
    "        return {\"config_path\": cfg.path, \"rules_files\": len(yaml_files), \"errors\": errs}\n",
    "\n",
    "    # Build rows (UTC created_at), dedupe by check_id\n",
    "    all_rules: List[dict] = []\n",
    "    for full_path in yaml_files:\n",
    "        file_rules = process_yaml_file(\n",
    "            full_path,\n",
    "            required_fields=required_fields,\n",
    "            created_by_value=\"AdminUser\",\n",
    "            allowed_criticality=allowed_crit,\n",
    "        )\n",
    "        if file_rules:\n",
    "            all_rules.extend(file_rules)\n",
    "            print(f\"{Console.LOADER} {full_path}: rules={len(file_rules)}\")\n",
    "\n",
    "    pre_dedupe = len(all_rules)\n",
    "    rules = dedupe_rules_in_batch_by_check_id(all_rules, mode=dedupe_mode)\n",
    "    post_dedupe = len(rules)\n",
    "\n",
    "    if not rules:\n",
    "        print(f\"{Console.SKIP} No rules discovered; nothing to do.\")\n",
    "        return {\"config_path\": cfg.path, \"rules_files\": len(yaml_files), \"wrote_rows\": 0, \"target_table\": fqn}\n",
    "\n",
    "    print(f\"{Console.DEDUPE} total parsed rules (pre-dedupe): {pre_dedupe}\")\n",
    "    df = build_df_from_rules(spark, rules)\n",
    "\n",
    "    display_section(\"SUMMARY OF RULES LOADED FROM YAML\")\n",
    "    totals = [(df.count(), df.select(\"check_id\").distinct().count(), df.select(\"check_id\", \"run_config_name\").distinct().count())]\n",
    "    tdf = df.sparkSession.createDataFrame(\n",
    "        totals,\n",
    "        schema=\"`total number of rules found` long, `unique rules found` long, `distinct pair of rules` long\",\n",
    "    )\n",
    "    show_df(tdf, n=1)\n",
    "\n",
    "    if dry_run:\n",
    "        display_section(\"DRY-RUN: FULL RULES PREVIEW\")\n",
    "        show_df(df.orderBy(\"table_name\", \"name\"), n=1000, truncate=False)\n",
    "        return {\n",
    "            \"config_path\": cfg.path,\n",
    "            \"rules_files\": len(yaml_files),\n",
    "            \"rules_pre_dedupe\": pre_dedupe,\n",
    "            \"rules_post_dedupe\": post_dedupe,\n",
    "            \"unique_check_ids\": df.select(\"check_id\").distinct().count(),\n",
    "            \"distinct_rule_run_pairs\": df.select(\"check_id\",\"run_config_name\").distinct().count(),\n",
    "            \"target_table\": fqn,\n",
    "            \"wrote_rows\": 0,\n",
    "            \"write_mode\": must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"),\n",
    "        }\n",
    "\n",
    "    # Write (project keeps column order via target table schema)\n",
    "    tw.write_df(\n",
    "        df=df.select(*[f.name for f in spark.table(fqn).schema.fields]),\n",
    "        fqn=fqn,\n",
    "        mode=must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"),\n",
    "        format=must(write_block.get(\"format\"), f\"{fqn}.write.format\"),\n",
    "        options=write_block.get(\"options\") or {},\n",
    "    )\n",
    "\n",
    "    wrote_rows = df.count()\n",
    "    display_section(\"WRITE RESULT\")\n",
    "    summary = spark.createDataFrame([(wrote_rows, fqn, must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"))],\n",
    "                                    schema=\"`rules written` long, `target table` string, `mode` string\")\n",
    "    show_df(summary, n=1)\n",
    "    print(f\"{Color.b}{Color.ivory}Finished writing rules to '{Color.r}{Color.b}{Color.i}{Color.sea_green}{fqn}{Color.r}{Color.b}{Color.ivory}'{Color.r}.\")\n",
    "\n",
    "    return {\n",
    "        \"config_path\": cfg.path,\n",
    "        \"rules_files\": len(yaml_files),\n",
    "        \"rules_pre_dedupe\": pre_dedupe,\n",
    "        \"rules_post_dedupe\": post_dedupe,\n",
    "        \"unique_check_ids\": df.select(\"check_id\").distinct().count(),\n",
    "        \"distinct_rule_run_pairs\": df.select(\"check_id\",\"run_config_name\").distinct().count(),\n",
    "        \"target_table\": fqn,\n",
    "        \"wrote_rows\": wrote_rows,\n",
    "        \"write_mode\": must(write_block.get(\"mode\"), f\"{fqn}.write.mode\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Entrypoint (local/dev)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    show_notebook_env(spark)\n",
    "    cfg = ProjectConfig(\"resources/dqx_config.yaml\", variables={})\n",
    "    result = run_checks_loader(spark, cfg, notebook_idx=1, dry_run=False, validate_only=False)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
