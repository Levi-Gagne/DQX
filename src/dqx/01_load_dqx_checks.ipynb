{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a52743c-29bb-4c1a-a63c-d5b210763342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load DQX Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff95a1d5-3010-440c-a60f-e8b4c7fcc84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5042f1e5-f5d4-4dda-a0b8-3e9130961893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FLOW"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "START: main (01_load_dqx_checks)\n",
    "|\n",
    "|-- 0. Environment banner\n",
    "|     |-- print_notebook_env(spark, local_timezone)\n",
    "|\n",
    "|-- 1. Load output config\n",
    "|     |-- output_config = yaml(resources/dqx_config.yaml)\n",
    "|     |-- rules_dir = output_config[\"dqx_yaml_checks\"]\n",
    "|     |-- delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "|     |-- required_fields = [\"table_name\",\"name\",\"criticality\",\"run_config_name\",\"check\"]\n",
    "|\n",
    "|-- 2. Recursively discover YAMLs\n",
    "|     |-- files = discover_yaml_files_recursive(rules_dir)  # supports dbfs:/ via /dbfs bridge\n",
    "|     |-- DISPLAY: list of YAML paths\n",
    "|\n",
    "|-- 3. Validation-only short-circuit (if validate_only=True)\n",
    "|     |-- For each file:\n",
    "|     |     |-- load_yaml_rules(file)  # supports multi-doc YAML, flattens lists\n",
    "|     |     |-- File-level checks: non-empty; no duplicate rule names\n",
    "|     |     |-- For each rule:\n",
    "|     |     |     |-- Rule-level checks:\n",
    "|     |     |     |     |-- table_name fully qualified (catalog.schema.table)\n",
    "|     |     |     |     |-- criticality ∈ {warn, warning, error}\n",
    "|     |     |     |     |-- check.function exists\n",
    "|     |     |     |-- DQEngine.validate_checks(docs) must pass\n",
    "|     |-- PRINT summary; END\n",
    "|\n",
    "|-- 4. Load + flatten rules (normal path)\n",
    "|     |-- all_rules = []\n",
    "|     |-- For each file in files:\n",
    "|           |-- docs = load_yaml_rules(file)\n",
    "|           |-- File-level checks (as above)\n",
    "|           |-- now = current_time_iso(time_zone)\n",
    "|           |-- For each rule in docs:\n",
    "|                 |-- Rule-level checks (as above)\n",
    "|                 |-- Canonicalize:\n",
    "|                 |     |-- filter := normalized whitespace (or \"\")\n",
    "|                 |     |-- check := {\n",
    "|                 |            function,\n",
    "|                 |            for_each_column := sorted list or None (validated as list[str]),\n",
    "|                 |            arguments := map with all values stringified\n",
    "|                 |        }\n",
    "|                 |     |-- user_metadata := map with stringified values (or None)\n",
    "|                 |-- payload := JSON over {table_name_lower, filter, canonical check} (sorted keys, tight separators)\n",
    "|                 |-- check_id := sha256(payload)\n",
    "|                 |-- Append flattened dict:\n",
    "|                 |     |-- keys: [check_id, check_id_payload, table_name, name, criticality,\n",
    "|                 |                check{function, for_each_column, arguments}, filter, run_config_name,\n",
    "|                 |                user_metadata, yaml_path=file, active (default True),\n",
    "|                 |                created_by=\"AdminUser\", created_at=now, updated_by=None, updated_at=None]\n",
    "|           |-- DQEngine.validate_checks(docs) must pass (file-level)\n",
    "|           |-- PRINT \"[loader] {file}: rules={n}\"\n",
    "|\n",
    "|-- 5. Batch de-duplication (by check_id only)\n",
    "|     |-- Group all_rules by check_id\n",
    "|     |-- Keep the lexicographically first (yaml_path, name); drop others\n",
    "|     |-- Mode:\n",
    "|           |-- \"warn\" (default): print detailed duplicate blocks\n",
    "|           |-- \"error\": raise\n",
    "|           |-- \"skip\": keep all (no drops)\n",
    "|\n",
    "|-- 6. Assemble DataFrame + display-first diagnostics\n",
    "|     |-- df := spark.createDataFrame(all_rules, schema=TABLE_SCHEMA)\n",
    "|     |-- DISPLAY: totals (count, distinct check_id, distinct (check_id, run_config_name))\n",
    "|     |-- DISPLAY: sample(check_id, name, run_config_name, yaml_path) ordered by yaml_path desc\n",
    "|     |-- DISPLAY: rules per table_name\n",
    "|     |-- (If very small) DISPLAY: first 3 payloads\n",
    "|\n",
    "|-- 7. Dry-run short-circuit (if dry_run=True)\n",
    "|     |-- DISPLAY: full rules preview ordered by (table_name, name)\n",
    "|     |-- END\n",
    "|\n",
    "|-- 8. Overwrite Delta target table (ALWAYS overwrite)\n",
    "|     |-- existed_before := spark.catalog.tableExists(delta_table_name)\n",
    "|     |-- ensure_schema_exists(catalog.schema)\n",
    "|     |-- Cast timestamps: created_at/updated_at := to_timestamp\n",
    "|     |-- df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(delta_table_name)\n",
    "|\n",
    "|-- 9. Apply documentation metadata (table + columns)\n",
    "|     |-- doc := materialize DQX_CHECKS_CONFIG_METADATA with {TABLE_FQN}=delta_table_name\n",
    "|     |-- Apply table comment (COMMENT ON TABLE; fallback to TBLPROPERTIES)\n",
    "|     |-- If created_now (i.e., !existed_before):\n",
    "|           |-- For each existing column: ALTER TABLE ... ALTER COLUMN ... COMMENT\n",
    "|     |-- DISPLAY: preview of table comment + column comments\n",
    "|\n",
    "|-- 10. Write result summary\n",
    "|     |-- DISPLAY: single-row summary (rules written, target table)\n",
    "|     |-- PRINT: confirmation line with target table\n",
    "|\n",
    "END: main\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74936d7c-4414-42fe-a143-cb6c7d364d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc1e26e-12cc-44e8-ae5a-e7b1fa4d5049",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install DQX Package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1154f87-29b1-4cb5-8a2f-ff0737aa9cda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1f4310-624f-4575-b404-6d6416d7d9f9",
     "showTitle": true,
     "tableResultSettingsMap": {
      "6": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"target table\":297},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755184491016}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 6
      }
     },
     "title": "Load DQX Checks --> 'checks_config'"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 01_load_dqx_checks  (DISPLAY-FIRST + METADATA, SAFE COMMENTS, RECURSIVE YAML DISCOVERY)\n",
    "# - Uses display() for clean tables\n",
    "# - Clearer section titles/column names\n",
    "# - Applies TABLE comment always (markdown), COLUMN comments only on first create\n",
    "# - Uses ALTER TABLE ... ALTER COLUMN COMMENT to avoid parser issues\n",
    "# - Shows a markdown-ish preview table of table/column docs\n",
    "# - Recursively loads *all* .yaml/.yml files under the configured folder (folders inside folders)\n",
    "# - NEW: Table/column comments come from a DICTIONARY (no function), with {TABLE_FQN} placeholder\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import yaml\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, types as T\n",
    "from pyspark.sql.functions import to_timestamp, col, desc\n",
    "\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "\n",
    "from utils.print import print_notebook_env\n",
    "from utils.timezone import current_time_iso\n",
    "from utils.color import Color\n",
    "\n",
    "# ======================================================\n",
    "# Small helpers for Databricks-friendly tabular display\n",
    "# ======================================================\n",
    "\n",
    "def _can_display() -> bool:\n",
    "    return \"display\" in globals()\n",
    "\n",
    "def show_df(df: DataFrame, n: int = 100, truncate: bool = False) -> None:\n",
    "    if _can_display():\n",
    "        display(df.limit(n))\n",
    "    else:\n",
    "        df.show(n, truncate=truncate)\n",
    "\n",
    "def display_section(title: str) -> None:\n",
    "    print(\"\\n\" + f\"{Color.b}{Color.deep_magenta}═{Color.r}\" * 80)\n",
    "    print(f\"{Color.b}{Color.deep_magenta}║{Color.r} {Color.b}{Color.ghost_white}{title}{Color.r}\")\n",
    "    print(f\"{Color.b}{Color.deep_magenta}═{Color.r}\" * 80)\n",
    "\n",
    "# =========================\n",
    "# Target schema (Delta sink)\n",
    "# =========================\n",
    "TABLE_SCHEMA = T.StructType([\n",
    "    T.StructField(\"check_id\",            T.StringType(), False),  # sha256 over canonical payload\n",
    "    T.StructField(\"check_id_payload\",    T.StringType(), False),  # canonical JSON used to compute check_id\n",
    "    T.StructField(\"table_name\",          T.StringType(), False),\n",
    "\n",
    "    # DQX fields\n",
    "    T.StructField(\"name\",                T.StringType(), False),\n",
    "    T.StructField(\"criticality\",         T.StringType(), False),\n",
    "    T.StructField(\"check\", T.StructType([\n",
    "        T.StructField(\"function\",        T.StringType(), False),\n",
    "        T.StructField(\"for_each_column\", T.ArrayType(T.StringType()), True),\n",
    "        T.StructField(\"arguments\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "    ]), False),\n",
    "    T.StructField(\"filter\",              T.StringType(), True),\n",
    "    T.StructField(\"run_config_name\",     T.StringType(), False),\n",
    "    T.StructField(\"user_metadata\",       T.MapType(T.StringType(), T.StringType()), True),\n",
    "\n",
    "    # Ops fields\n",
    "    T.StructField(\"yaml_path\",           T.StringType(), False),\n",
    "    T.StructField(\"active\",              T.BooleanType(), False),\n",
    "    T.StructField(\"created_by\",          T.StringType(), False),\n",
    "    T.StructField(\"created_at\",          T.StringType(), False),  # ISO string; cast on write\n",
    "    T.StructField(\"updated_by\",          T.StringType(), True),\n",
    "    T.StructField(\"updated_at\",          T.StringType(), True),\n",
    "])\n",
    "\n",
    "# ======================================================\n",
    "# NEW: Documentation dictionary (no function)\n",
    "# Use {TABLE_FQN} token in table_comment; we’ll inject the actual table name.\n",
    "# ======================================================\n",
    "DQX_CHECKS_CONFIG_METADATA: Dict[str, Any] = {\n",
    "    \"table\": \"<override at create time>\",\n",
    "    \"table_comment\": (\n",
    "        \"## DQX Checks Configuration\\n\"\n",
    "        \"- **Target**: `{TABLE_FQN}`\\n\"\n",
    "        \"- Stores flattened Data Quality checks generated from YAML files.\\n\"\n",
    "        \"- `check_id` is a stable hash over (table_name, filter, check.*).\\n\"\n",
    "    ),\n",
    "    \"columns\": {\n",
    "        \"check_id\": \"**Hash** of canonical rule payload. Used for identity/dedupe.\",\n",
    "        \"check_id_payload\": \"Canonical **JSON** used to compute `check_id`.\",\n",
    "        \"table_name\": \"Fully qualified **target table** (`catalog.schema.table`).\",\n",
    "        \"name\": \"Human-readable **rule name**.\",\n",
    "        \"criticality\": \"Rule severity: `warn|warning|error`.\",\n",
    "        \"check\": \"Structured **check** object `{function, for_each_column, arguments}`.\",\n",
    "        \"filter\": \"Optional row-level **filter** expression.\",\n",
    "        \"run_config_name\": \"**Execution group/tag** to route this rule.\",\n",
    "        \"user_metadata\": \"User-provided **metadata** `map<string,string>`.\",\n",
    "        \"yaml_path\": \"Source **YAML** file path that defined this rule.\",\n",
    "        \"active\": \"If **false**, the rule is ignored by runners.\",\n",
    "        \"created_by\": \"Audit: **creator** of the row.\",\n",
    "        \"created_at\": \"Audit: **creation timestamp**.\",\n",
    "        \"updated_by\": \"Audit: **last updater**.\",\n",
    "        \"updated_at\": \"Audit: **last update timestamp**.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def _materialize_table_doc(doc_template: Dict[str, Any], table_fqn: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Make a deep copy of the doc dict and inject the concrete table name.\n",
    "    Only replaces the {TABLE_FQN} token inside table_comment and sets the 'table' key.\n",
    "    \"\"\"\n",
    "    copy = json.loads(json.dumps(doc_template))  # deep copy\n",
    "    copy[\"table\"] = table_fqn\n",
    "    if \"table_comment\" in copy and isinstance(copy[\"table_comment\"], str):\n",
    "        copy[\"table_comment\"] = copy[\"table_comment\"].replace(\"{TABLE_FQN}\", table_fqn)\n",
    "    return copy\n",
    "\n",
    "# =========================\n",
    "# YAML loading (robust)\n",
    "# =========================\n",
    "def load_yaml_rules(path: str) -> List[dict]:\n",
    "    with open(path, \"r\") as fh:\n",
    "        docs = list(yaml.safe_load_all(fh))\n",
    "    out: List[dict] = []\n",
    "    for d in docs:\n",
    "        if d is None:\n",
    "            continue\n",
    "        if isinstance(d, dict):\n",
    "            out.append(d)\n",
    "        elif isinstance(d, list):\n",
    "            out.extend([x for x in d if isinstance(x, dict)])\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Canonicalization & IDs\n",
    "# =========================\n",
    "def _canon_filter(s: Optional[str]) -> str:\n",
    "    return \"\" if not s else \" \".join(str(s).split())\n",
    "\n",
    "def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {\"function\": chk.get(\"function\"), \"for_each_column\": None, \"arguments\": {}}\n",
    "    fec = chk.get(\"for_each_column\")\n",
    "    if isinstance(fec, list):\n",
    "        out[\"for_each_column\"] = sorted([str(x) for x in fec]) or None\n",
    "    args = chk.get(\"arguments\") or {}\n",
    "    canon_args: Dict[str, str] = {}\n",
    "    for k, v in args.items():\n",
    "        sv = \"\" if v is None else str(v).strip()\n",
    "        if (sv.startswith(\"{\") and sv.endswith(\"}\")) or (sv.startswith(\"[\") and sv.endswith(\"]\")):\n",
    "            try:\n",
    "                sv = json.dumps(json.loads(sv), sort_keys=True, separators=(\",\", \":\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        canon_args[str(k)] = sv\n",
    "    out[\"arguments\"] = {k: canon_args[k] for k in sorted(canon_args)}\n",
    "    return out\n",
    "\n",
    "def compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:\n",
    "    payload_obj = {\n",
    "        \"table_name\": (table_name or \"\").lower(),\n",
    "        \"filter\": _canon_filter(filter_str),\n",
    "        \"check\": _canon_check(check_dict or {}),\n",
    "    }\n",
    "    return json.dumps(payload_obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "def compute_check_id_from_payload(payload: str) -> str:\n",
    "    return hashlib.sha256(payload.encode()).hexdigest()\n",
    "\n",
    "# =========================\n",
    "# Conversions / validation\n",
    "# =========================\n",
    "def _stringify_map_values(d: Dict[str, Any]) -> Dict[str, str]:\n",
    "    out: Dict[str, str] = {}\n",
    "    for k, v in (d or {}).items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            out[k] = json.dumps(v)\n",
    "        elif isinstance(v, bool):\n",
    "            out[k] = \"true\" if v else \"false\"\n",
    "        elif v is None:\n",
    "            out[k] = \"null\"\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def validate_rules_file(rules: List[dict], file_path: str):\n",
    "    if not rules:\n",
    "        raise ValueError(f\"No rules found in {file_path} (empty or invalid YAML).\")\n",
    "    probs, seen = [], set()\n",
    "    for r in rules:\n",
    "        nm = r.get(\"name\")\n",
    "        if not nm:\n",
    "            probs.append(f\"Missing rule name in {file_path}\")\n",
    "        if nm in seen:\n",
    "            probs.append(f\"Duplicate rule name '{nm}' in {file_path}\")\n",
    "        seen.add(nm)\n",
    "    if probs:\n",
    "        raise ValueError(f\"File-level validation failed in {file_path}: {probs}\")\n",
    "\n",
    "def validate_rule_fields(\n",
    "    rule: dict,\n",
    "    file_path: str,\n",
    "    required_fields: List[str],\n",
    "    allowed_criticality={\"error\", \"warn\", \"warning\"},\n",
    "):\n",
    "    probs = []\n",
    "    for f in required_fields:\n",
    "        if not rule.get(f):\n",
    "            probs.append(f\"Missing required field '{f}' in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if rule.get(\"table_name\", \"\").count(\".\") != 2:\n",
    "        probs.append(\n",
    "            f\"table_name '{rule.get('table_name')}' not fully qualified in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if rule.get(\"criticality\") not in allowed_criticality:\n",
    "        probs.append(\n",
    "            f\"Invalid criticality '{rule.get('criticality')}' in rule '{rule.get('name')}' ({file_path})\"\n",
    "        )\n",
    "    if not rule.get(\"check\", {}).get(\"function\"):\n",
    "        probs.append(f\"Missing check.function in rule '{rule.get('name')}' ({file_path})\")\n",
    "    if probs:\n",
    "        raise ValueError(\"Rule-level validation failed: \" + \"; \".join(probs))\n",
    "\n",
    "def validate_with_dqx(rules: List[dict], file_path: str):\n",
    "    status = DQEngine.validate_checks(rules)\n",
    "    if getattr(status, \"has_errors\", False):\n",
    "        raise ValueError(f\"DQX validation failed in {file_path}:\\n{status.to_string()}\")\n",
    "\n",
    "# =========================\n",
    "# Build rows\n",
    "# =========================\n",
    "def process_yaml_file(path: str, required_fields: List[str], time_zone: str = \"UTC\") -> List[dict]:\n",
    "    docs = load_yaml_rules(path)\n",
    "    if not docs:\n",
    "        print(f\"[skip] {path} has no rules (empty/comment-only).\")\n",
    "        return []\n",
    "\n",
    "    validate_rules_file(docs, path)\n",
    "\n",
    "    now = current_time_iso(time_zone)\n",
    "    flat: List[dict] = []\n",
    "\n",
    "    for rule in docs:\n",
    "        validate_rule_fields(rule, path, required_fields=required_fields)\n",
    "\n",
    "        raw_check = rule[\"check\"] or {}\n",
    "        payload = compute_check_id_payload(rule[\"table_name\"], raw_check, rule.get(\"filter\"))\n",
    "        check_id = compute_check_id_from_payload(payload)\n",
    "\n",
    "        function = raw_check.get(\"function\")\n",
    "        if not isinstance(function, str) or not function:\n",
    "            raise ValueError(\n",
    "                f\"{path}: check.function must be a non-empty string (rule '{rule.get('name')}').\"\n",
    "            )\n",
    "\n",
    "        for_each = raw_check.get(\"for_each_column\")\n",
    "        if for_each is not None and not isinstance(for_each, list):\n",
    "            raise ValueError(\n",
    "                f\"{path}: check.for_each_column must be an array of strings (rule '{rule.get('name')}').\"\n",
    "            )\n",
    "        if isinstance(for_each, list):\n",
    "            try:\n",
    "                for_each = [str(x) for x in for_each]\n",
    "            except Exception:\n",
    "                raise ValueError(\n",
    "                    f\"{path}: unable to cast for_each_column items to strings (rule '{rule.get('name')}').\"\n",
    "                )\n",
    "\n",
    "        arguments = raw_check.get(\"arguments\", {}) or {}\n",
    "        if not isinstance(arguments, dict):\n",
    "            raise ValueError(f\"{path}: check.arguments must be a map (rule '{rule.get('name')}').\")\n",
    "        arguments = _stringify_map_values(arguments)\n",
    "\n",
    "        user_metadata = rule.get(\"user_metadata\")\n",
    "        if user_metadata is not None:\n",
    "            if not isinstance(user_metadata, dict):\n",
    "                raise ValueError(\n",
    "                    f\"{path}: user_metadata must be a map (rule '{rule.get('name')}').\"\n",
    "                )\n",
    "            user_metadata = _stringify_map_values(user_metadata)\n",
    "\n",
    "        flat.append(\n",
    "            {\n",
    "                \"check_id\": check_id,\n",
    "                \"check_id_payload\": payload,\n",
    "                \"table_name\": rule[\"table_name\"],\n",
    "                \"name\": rule[\"name\"],\n",
    "                \"criticality\": rule[\"criticality\"],\n",
    "                \"check\": {\n",
    "                    \"function\": function,\n",
    "                    \"for_each_column\": for_each if for_each else None,\n",
    "                    \"arguments\": arguments if arguments else None,\n",
    "                },\n",
    "                \"filter\": rule.get(\"filter\"),\n",
    "                \"run_config_name\": rule[\"run_config_name\"],\n",
    "                \"user_metadata\": user_metadata if user_metadata else None,\n",
    "                \"yaml_path\": path,\n",
    "                \"active\": rule.get(\"active\", True),\n",
    "                \"created_by\": \"AdminUser\",\n",
    "                \"created_at\": now,\n",
    "                \"updated_by\": None,\n",
    "                \"updated_at\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    validate_with_dqx(docs, path)\n",
    "    return flat\n",
    "\n",
    "# =========================\n",
    "# Batch dedupe (on check_id ONLY)\n",
    "# =========================\n",
    "def _fmt_rule_for_dup(r: dict) -> str:\n",
    "    return (\n",
    "        f\"name={r.get('name')} | file={r.get('yaml_path')} | \"\n",
    "        f\"criticality={r.get('criticality')} | run_config={r.get('run_config_name')} | \"\n",
    "        f\"filter={r.get('filter')}\"\n",
    "    )\n",
    "\n",
    "def dedupe_rules_in_batch_by_check_id(rules: List[dict], mode: str = \"warn\") -> List[dict]:\n",
    "    groups: Dict[str, List[dict]] = {}\n",
    "    for r in rules:\n",
    "        groups.setdefault(r[\"check_id\"], []).append(r)\n",
    "\n",
    "    out: List[dict] = []\n",
    "    dropped = 0\n",
    "    blocks: List[str] = []\n",
    "\n",
    "    for cid, lst in groups.items():\n",
    "        if len(lst) == 1:\n",
    "            out.append(lst[0])\n",
    "            continue\n",
    "        lst = sorted(lst, key=lambda x: (x.get(\"yaml_path\", \"\"), x.get(\"name\", \"\")))\n",
    "        keep, dups = lst[0], lst[1:]\n",
    "        dropped += len(dups)\n",
    "        head = f\"[dup/batch/check_id] {len(dups)} duplicate(s) for check_id={cid[:12]}…\"\n",
    "        lines = [\"    \" + _fmt_rule_for_dup(x) for x in lst]\n",
    "        tail = f\"    -> keeping: name={keep.get('name')} | file={keep.get('yaml_path')}\"\n",
    "        blocks.append(\"\\n\".join([head, *lines, tail]))\n",
    "        out.append(keep)\n",
    "\n",
    "    if dropped:\n",
    "        msg = \"\\n\\n\".join(blocks) + f\"\\n[dedupe/batch] total dropped={dropped}\"\n",
    "        if mode == \"error\":\n",
    "            raise ValueError(msg)\n",
    "        if mode == \"warn\":\n",
    "            print(msg)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Table & column comments (safe, creation-only for columns)\n",
    "# =========================\n",
    "def _esc_comment(s: str) -> str:\n",
    "    return (s or \"\").replace(\"'\", \"''\")\n",
    "\n",
    "def _q_fqn(fqn: str) -> str:\n",
    "    # quote a.b.c -> `a`.`b`.`c`\n",
    "    return \".\".join(f\"`{p}`\" for p in fqn.split(\".\"))\n",
    "\n",
    "def preview_table_documentation(spark: SparkSession, table_fqn: str, doc: Dict[str, Any]) -> None:\n",
    "    display_section(\"TABLE METADATA PREVIEW (markdown text stored in comments)\")\n",
    "    doc_df = spark.createDataFrame(\n",
    "        [(table_fqn, doc.get(\"table_comment\", \"\"))],\n",
    "        schema=\"table string, table_comment_markdown string\",\n",
    "    )\n",
    "    show_df(doc_df, n=1, truncate=False)\n",
    "\n",
    "    cols = doc.get(\"columns\", {}) or {}\n",
    "    cols_df = spark.createDataFrame(\n",
    "        [(k, v) for k, v in cols.items()],\n",
    "        schema=\"column string, column_comment_markdown string\",\n",
    "    )\n",
    "    show_df(cols_df, n=200, truncate=False)\n",
    "\n",
    "def apply_table_documentation(\n",
    "    spark: SparkSession,\n",
    "    table_fqn: str,\n",
    "    doc: Optional[Dict[str, Any]],\n",
    "    created_now: bool,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Apply table & column comments.\n",
    "    - TABLE comment: always applied (markdown string).\n",
    "    - COLUMN comments: only when created_now=True (creation-time).\n",
    "    Uses robust syntax to avoid parser issues.\n",
    "    \"\"\"\n",
    "    if not doc:\n",
    "        return\n",
    "\n",
    "    qtable = _q_fqn(table_fqn)\n",
    "\n",
    "    # Prefer COMMENT ON TABLE; fallback to TBLPROPERTIES\n",
    "    table_comment = _esc_comment(doc.get(\"table_comment\", \"\"))\n",
    "    if table_comment:\n",
    "        try:\n",
    "            spark.sql(f\"COMMENT ON TABLE {qtable} IS '{table_comment}'\")\n",
    "        except Exception:\n",
    "            spark.sql(f\"ALTER TABLE {qtable} SET TBLPROPERTIES ('comment' = '{table_comment}')\")\n",
    "\n",
    "    if not created_now:\n",
    "        return  # Only set column comments on first create\n",
    "\n",
    "    cols: Dict[str, str] = doc.get(\"columns\", {}) or {}\n",
    "    # Only try columns that exist\n",
    "    existing_cols = {f.name.lower() for f in spark.table(table_fqn).schema.fields}\n",
    "    for col_name, comment in cols.items():\n",
    "        if col_name.lower() not in existing_cols:\n",
    "            continue\n",
    "        qcol = f\"`{col_name}`\"\n",
    "        comment_sql = f\"ALTER TABLE {qtable} ALTER COLUMN {qcol} COMMENT '{_esc_comment(comment)}'\"\n",
    "        try:\n",
    "            spark.sql(comment_sql)\n",
    "        except Exception as e:\n",
    "            print(f\"[meta] Skipped column comment for {table_fqn}.{col_name}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# Recursive YAML discovery\n",
    "# =========================\n",
    "def _normalize_base(path: str) -> str:\n",
    "    \"\"\"Allow dbfs:/… by translating to /dbfs/… so open()/os.walk() work.\"\"\"\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        return \"/dbfs/\" + path[len(\"dbfs:/\") :].lstrip(\"/\")\n",
    "    return path\n",
    "\n",
    "def discover_yaml_files_recursive(base_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively find all *.yaml / *.yml under base_dir (skips hidden dirs/files).\n",
    "    Supports both workspace filesystem and dbfs:/ (via /dbfs/ bridge).\n",
    "    \"\"\"\n",
    "    base = _normalize_base(base_dir)\n",
    "    if not os.path.isdir(base):\n",
    "        raise FileNotFoundError(f\"Rules folder not found or not a directory: {base_dir} (resolved: {base})\")\n",
    "\n",
    "    out: List[str] = []\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        # skip hidden directories for noise reduction\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "        for f in files:\n",
    "            if f.startswith(\".\"):\n",
    "                continue\n",
    "            fl = f.lower()\n",
    "            if fl.endswith(\".yaml\") or fl.endswith(\".yml\"):\n",
    "                out.append(os.path.join(root, f))\n",
    "    return sorted(out)\n",
    "\n",
    "# =========================\n",
    "# Delta I/O (ALWAYS OVERWRITE)\n",
    "# =========================\n",
    "def ensure_schema_exists(spark: SparkSession, full_table_name: str):\n",
    "    parts = full_table_name.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Expected a 3-part name (catalog.schema.table), got '{full_table_name}'\")\n",
    "    cat, sch, _ = parts\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}`\")\n",
    "\n",
    "def overwrite_rules_into_delta(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    delta_table_name: str,\n",
    "    table_doc: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    # Track existence BEFORE write\n",
    "    existed_before = spark.catalog.tableExists(delta_table_name)\n",
    "\n",
    "    # Ensure schema\n",
    "    ensure_schema_exists(spark, delta_table_name)\n",
    "\n",
    "    # Cast audit timestamps\n",
    "    df = df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"))) \\\n",
    "           .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "\n",
    "    # Overwrite (content + schema)\n",
    "    df.write.format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .option(\"overwriteSchema\", \"true\") \\\n",
    "      .saveAsTable(delta_table_name)\n",
    "\n",
    "    # Materialize the table doc dictionary with the actual table name\n",
    "    doc_to_apply = _materialize_table_doc(table_doc or DQX_CHECKS_CONFIG_METADATA, delta_table_name)\n",
    "\n",
    "    # Apply comments (table: always; columns: only on first create)\n",
    "    apply_table_documentation(spark, delta_table_name, doc_to_apply, created_now=not existed_before)\n",
    "\n",
    "    # Preview metadata\n",
    "    preview_table_documentation(spark, delta_table_name, doc_to_apply)\n",
    "\n",
    "    # Display a clean confirmation table\n",
    "    display_section(\"WRITE RESULT (Delta)\")\n",
    "    summary = spark.createDataFrame(\n",
    "        [(df.count(), delta_table_name)],\n",
    "        schema=\"`rules written` long, `target table` string\",\n",
    "    )\n",
    "    show_df(summary, n=1)\n",
    "    print(f\"\\n{Color.b}{Color.ivory}Rules written to target table: '{Color.r}{Color.b}{Color.chartreuse}{delta_table_name}{Color.r}{Color.b}{Color.ivory}'\")\n",
    "\n",
    "# =========================\n",
    "# Display-first debug helpers\n",
    "# =========================\n",
    "def debug_display_batch(spark: SparkSession, df_rules: DataFrame) -> None:\n",
    "    display_section(\"SUMMARY OF RULES LOADED FROM YAML\")\n",
    "    totals = [\n",
    "        (\n",
    "            df_rules.count(),\n",
    "            df_rules.select(\"check_id\").distinct().count(),\n",
    "            df_rules.select(\"check_id\", \"run_config_name\").distinct().count(),\n",
    "        )\n",
    "    ]\n",
    "    totals_df = spark.createDataFrame(\n",
    "        totals,\n",
    "        schema=\"`total number of rules found` long, `unique rules found` long, `distinct pair of rules` long\",\n",
    "    )\n",
    "    show_df(totals_df, n=1)\n",
    "\n",
    "    display_section(\"SAMPLE OF RULES LOADED FROM YAML (check_id, name, run_config_name, yaml_path)\")\n",
    "    sample_cols = df_rules.select(\"check_id\", \"name\", \"run_config_name\", \"yaml_path\").orderBy(\n",
    "        desc(\"yaml_path\")\n",
    "    )\n",
    "    show_df(sample_cols, n=50, truncate=False)\n",
    "\n",
    "    display_section(\"RULES LOADED PER TABLE\")\n",
    "    by_table = df_rules.groupBy(\"table_name\").count().orderBy(desc(\"count\"))\n",
    "    show_df(by_table, n=200)\n",
    "\n",
    "    # Only show payload snippet when small\n",
    "    distinct_cid = totals[0][1]\n",
    "    if distinct_cid <= 3:\n",
    "        display_section(\"PAYLOAD PREVIEW (first 3)\")\n",
    "        show_df(df_rules.select(\"check_id\", \"check_id_payload\"), n=3, truncate=False)\n",
    "\n",
    "def print_rules_df(spark: SparkSession, rules: List[dict]) -> Optional[DataFrame]:\n",
    "    if not rules:\n",
    "        print(\"No rules to show.\")\n",
    "        return None\n",
    "    df = (\n",
    "        spark.createDataFrame(rules, schema=TABLE_SCHEMA)\n",
    "        .withColumn(\"created_at\", to_timestamp(col(\"created_at\")))\n",
    "        .withColumn(\"updated_at\", to_timestamp(col(\"updated_at\")))\n",
    "    )\n",
    "    debug_display_batch(spark, df)\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Validation (now works off a file list)\n",
    "# =========================\n",
    "def validate_rule_files(file_paths: List[str], required_fields: List[str], fail_fast: bool = True) -> List[str]:\n",
    "    errors = []\n",
    "    for full_path in file_paths:\n",
    "        print(f\"\\nValidating file: {full_path}\")\n",
    "        try:\n",
    "            docs = load_yaml_rules(full_path)\n",
    "            if not docs:\n",
    "                print(f\"  (empty) skipped: {full_path}\")\n",
    "                continue\n",
    "            validate_rules_file(docs, full_path)\n",
    "            print(f\"  File-level validation passed for {full_path}\")\n",
    "            for rule in docs:\n",
    "                validate_rule_fields(rule, full_path, required_fields=required_fields)\n",
    "                print(f\"    Rule-level validation passed for rule '{rule.get('name')}'\")\n",
    "            validate_with_dqx(docs, full_path)\n",
    "            print(f\"  DQX validation PASSED for {full_path}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"  Validation FAILED for file {full_path}\\n  Reason: {ex}\")\n",
    "            errors.append(str(ex))\n",
    "            if fail_fast:\n",
    "                break\n",
    "    if not errors:\n",
    "        print(\"\\nAll YAML rule files are valid!\")\n",
    "    else:\n",
    "        print(\"\\nRule validation errors found:\")\n",
    "        for e in errors:\n",
    "            print(e)\n",
    "    return errors\n",
    "\n",
    "# =========================\n",
    "# Recursive discovery + write\n",
    "# =========================\n",
    "def _load_output_config(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as fh:\n",
    "        return yaml.safe_load(fh) or {}\n",
    "\n",
    "def main(\n",
    "    output_config_path: str = \"resources/dqx_config.yaml\",\n",
    "    rules_dir: Optional[str] = None,\n",
    "    time_zone: str = \"America/Chicago\",\n",
    "    dry_run: bool = False,\n",
    "    validate_only: bool = False,\n",
    "    required_fields: Optional[List[str]] = None,\n",
    "    batch_dedupe_mode: str = \"warn\",  # warn | error | skip\n",
    "    table_doc: Optional[Dict[str, Any]] = None,   # pass your own dict to override defaults\n",
    "):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    print_notebook_env(spark, local_timezone=time_zone)\n",
    "\n",
    "    output_config = _load_output_config(output_config_path)\n",
    "    rules_dir = rules_dir or output_config[\"dqx_yaml_checks\"]\n",
    "    delta_table_name = output_config[\"dqx_checks_config_table_name\"]\n",
    "\n",
    "    required_fields = required_fields or [\"table_name\", \"name\", \"criticality\", \"run_config_name\", \"check\"]\n",
    "\n",
    "    # Recursively discover YAML files\n",
    "    yaml_files = discover_yaml_files_recursive(rules_dir)\n",
    "    display_section(\"YAML FILES DISCOVERED (recursive)\")\n",
    "    files_df = spark.createDataFrame([(p,) for p in yaml_files], \"yaml_path string\")\n",
    "    show_df(files_df, n=500, truncate=False)\n",
    "\n",
    "    if validate_only:\n",
    "        print(\"\\nValidation only: not writing any rules.\")\n",
    "        validate_rule_files(yaml_files, required_fields)\n",
    "        return\n",
    "\n",
    "    # Collect rules\n",
    "    all_rules: List[dict] = []\n",
    "    for full_path in yaml_files:\n",
    "        file_rules = process_yaml_file(full_path, required_fields=required_fields, time_zone=time_zone)\n",
    "        if file_rules:\n",
    "            all_rules.extend(file_rules)\n",
    "            print(f\"[loader] {full_path}: rules={len(file_rules)}\")\n",
    "\n",
    "    if not all_rules:\n",
    "        print(\"No rules discovered; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[loader] total parsed rules (pre-dedupe): {len(all_rules)}\")\n",
    "\n",
    "    # In-batch dedupe on check_id only\n",
    "    all_rules = dedupe_rules_in_batch_by_check_id(all_rules, mode=batch_dedupe_mode)\n",
    "\n",
    "    # Assemble DataFrame and DISPLAY diagnostics\n",
    "    df = spark.createDataFrame(all_rules, schema=TABLE_SCHEMA)\n",
    "    debug_display_batch(spark, df)\n",
    "\n",
    "    if dry_run:\n",
    "        display_section(\"DRY-RUN: FULL RULES PREVIEW\")\n",
    "        show_df(df.orderBy(\"table_name\", \"name\"), n=1000, truncate=False)\n",
    "        return\n",
    "\n",
    "    # ALWAYS OVERWRITE on each run\n",
    "    # If caller didn’t pass a doc dict, use our constant dictionary (materialized with the final table name)\n",
    "    doc = _materialize_table_doc(table_doc or DQX_CHECKS_CONFIG_METADATA, delta_table_name)\n",
    "    overwrite_rules_into_delta(spark, df, delta_table_name, table_doc=doc)\n",
    "    print(f\"{Color.b}{Color.ivory}Finished writing rules to '{Color.r}{Color.b}{Color.i}{Color.sea_green}{delta_table_name}{Color.r}{Color.b}{Color.ivory}' (overwrite){Color.r}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main(dry_run=True)\n",
    "    # main(validate_only=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_load_dqx_checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
