# src/dqx/resources/dqx_config.yaml
version: 1

variables:
  env: dev
  local_timezone: America/Chicago
  processing_timezone: UTC
  apply_table_metadata: true
  batch_dedupe_mode: warn

notebooks:
  notebook_1:
    name: 01_load_dqx_checks
    data_source:
      storage_type: repo         # repo | volume
      source_format: yaml        # yaml | json | python
      source_path: resources/dqx_checks_config_load
      allowed_criticality: [error, warn]
      required_fields: [table_name, name, criticality, run_config_name, check]
    targets:
      target_table_1:
        catalog: "dq_{env}"
        schema: "dqx"
        table: "checks_config"
        primary_key: "check_id"
        partition_by: []
        write:
          mode: overwrite
          format: delta
          options: {}
        table_tags:
          table_tag_1: { key: team,    value: data_quality }
          table_tag_2: { key: project, value: dqx }
        table_description: |
          ## DQX Checks Configuration
          - One row per **unique canonical rule** generated from YAML (source of truth).
          - **Primary key**: `check_id` (sha256 of canonical payload).
          - Rebuilt by the loader (**overwrite**). Manual edits will be lost.
          - `run_config_name` is a routing tag (not part of identity).
          - Only rows with `active=true` are executed.
        # schemas are defined in the notebook code

  notebook_2:
    name: 02_run_dqx_checks
    targets:
      target_table_1:
        catalog: "dq_{env}"
        schema: "dqx"
        table: "checks_log"
        primary_key: "log_id"
        partition_by: []
        write:
          mode: overwrite
          format: delta
          options: {}
        table_tags:
          table_tag_1: { key: team,    value: data_quality }
          table_tag_2: { key: project, value: dqx }
        table_description: |
          ## DQX Row-Level Check Results Log
          - One row per **flagged source row** (error or warn).
          - `log_id` = sha256(table_name, run_config_name, row_snapshot_fp, _errors_fp, _warnings_fp).
          - `_errors` / `_warnings` hold issue structs (with `check_id` when resolvable).
          - `row_snapshot` captures non-reserved columns (stringified).
      target_table_2:
        catalog: "dq_{env}"
        schema: "dqx"
        table: "checks_log_summary_by_rule"
        partition_by: []
        write:
          mode: overwrite
          format: delta
          options: {}
        table_tags:
          table_tag_1: { key: team,    value: data_quality }
          table_tag_2: { key: project, value: dqx }
        table_description: |
          ## DQX Row-Hit Summary by Rule
          - One row per (`table_name`, `rule_name`, `severity`) per run_config.
          - Includes rules with **zero hits**; `rows_flagged` may be 0.
          - `severity` normalized to `error` or `warning`.
      target_table_3:
        catalog: "dq_{env}"
        schema: "dqx"
        table: "checks_log_summary_by_table"
        partition_by: []
        write:
          mode: overwrite
          format: delta
          options: {}
        table_tags:
          table_tag_1: { key: team,    value: data_quality }
          table_tag_2: { key: project, value: dqx }
        table_description: |
          ## DQX Row-Hit Summary by Table
          - Per-table totals **per run_config**: `table_total_rows`, error/warn row counts,
            `total_flagged_rows`, `distinct_rules_fired`.

# your runners read this mapping:
run_config_name:
  default:
    output_config:
      location: dq_{env}.dqx.checks_log
      mode: overwrite
      format: delta
      options: {}
    quarantine_config:
      location: dq_{env}.dqx.checks_quarantine
      mode: append
      format: delta
      options: {}
  generated_check:
    output_config:
      location: dq_{env}.dqx.generated_checks_log
      mode: overwrite
      format: delta
      options: {}
    quarantine_config:
      location: dq_{env}.dqx.generated_checks_quarantine
      mode: append
      format: delta
      options: {}